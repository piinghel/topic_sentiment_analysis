{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"C:\\\\Users\\\\Pieter-Jan\\\\Documents\\\\Work\\\\Candriam\\\\nlp\\\\ESG\\\\topic_sentiment_analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "# data manipulations\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy import sparse\n",
    "\n",
    "# CMT\n",
    "from contextualized_topic_models.models.ctm import CTM\n",
    "from contextualized_topic_models.utils.data_preparation import bert_embeddings_from_file, TextHandler\n",
    "from contextualized_topic_models.datasets.dataset import CTMDataset\n",
    "from contextualized_topic_models.evaluation.measures import CoherenceNPMI, InvertedRBO, CoherenceWordEmbeddings\n",
    "from contextualized_topic_models.utils.preprocessing import SimplePreprocessing\n",
    "\n",
    "# lDA\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import LdaMulticore \n",
    "import pyLDAvis.gensim\n",
    "from gensim import corpora, matutils, models, similarities\n",
    "import pyLDAvis\n",
    "\n",
    "# BerTopic\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# other\n",
    "from tqdm import tqdm\n",
    "\n",
    "# own modules\n",
    "import modules.preprocessing as preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "source": [
    "# Goal\n",
    "\n",
    "We are going to compare the performance of three **unsupervised** models for topic modelling on ESG documents.\n",
    "\n",
    "1. Contextualized Topic Modelling (CTM): https://github.com/MilaNLProc/contextualized-topic-models\n",
    "2. Latent Dirichlet Allocation (LDA): https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "3. BERTopic: https://github.com/MaartenGr/BERTopic"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Evaluation measures \n",
    "\n",
    "1. **Normalized Point-wise Mutual Information (NPMI) (Lau et al.,\n",
    "2014)**\n",
    "\n",
    "It measures how much the top-10 words of a topic are related to each other, considering the empirical frequency of the words computed on the\n",
    "original corpus. τ is a symbolic metric and relies on co-occurrence.\n",
    "\n",
    "2. **External Word Embeddings Topic Coherence**\n",
    "\n",
    "As Ding et al. (2018) pointed out, though, topic\n",
    "coherence computed on the same data is inherently\n",
    "limited. Coherence computed on an external corpus, on the other hand, correlates much more to\n",
    "human judgment, but it may be expensive to estimate. Thus, our second metric is an external\n",
    "word embeddings topic coherence metric, which we compute by adopting a strategy similar to that\n",
    "described in Ding et al. (2018). First, we compute\n",
    "the average pairwise cosine similarity of the word\n",
    "embeddings of the top-10 words in a topic using (Mikolov et al., 2013) embeddings. Then, we\n",
    "compute the overall average of those values for all\n",
    "the topics (α).\n",
    "\n",
    "3. **rank-\n",
    "biased overlap (RBO) (Webber et al., 2010)**\n",
    "\n",
    "To evaluate how diverse the topics\n",
    "generated by a single model are, we use the rank-\n",
    "biased overlap (RBO) (Webber et al., 2010). RBO\n",
    "compares two topics of the same model. The key\n",
    "qualities of this measure are twofold: it allows\n",
    "disjointedness between the lists of topics (i.e., two\n",
    "topics can have different words in them) and it is\n",
    "weighted on the ranking (i.e., two lists that share\n",
    "some of the same words, albeit at different rankings,\n",
    "are penalized less than two lists that share the same\n",
    "words at the highest ranks). We deﬁne ρ as the rank-\n",
    "biased overlap diversity, that we interpret as the\n",
    "reciprocal of the standard RBO. ρ is 0 for identical\n",
    "topics and 1 for completely different topics. Both\n",
    "metrics are computed on the top-k ranked lists.\n",
    "Following the state-of-the-art, we consider k = 10."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_preprocess(data, dir_file, stop_words, nlp, update=False):\n",
    "    \"\"\"\n",
    "    reads in processed data if exists\n",
    "    \"\"\"\n",
    "\n",
    "    # read file if it exits\n",
    "    if os.path.isfile(dir_file) and not update:\n",
    "        df = pd.read_csv(dir_file, sep='\\t', header=None)\n",
    "        data[\"paragraph_cleaned\"] = df.values\n",
    "\n",
    "    else:\n",
    "        # progress bar\n",
    "        tqdm.pandas()\n",
    "        # perfrom some cleaning (stopwords, lemmatize)\n",
    "        data[\"paragraph_cleaned\"] = data['paragraph_raw'].progress_apply(preprocess.lemmatize, nlp=nlp, stop_words=stop_words, method=1)\n",
    "        # save raw and cleaned text\n",
    "        data[\"paragraph_raw\"].to_csv('output/newsgroup_raw.txt', sep='\\t', index=False, header=False)\n",
    "        data[\"paragraph_cleaned\"].to_csv(dir_file, sep='\\t', index=False, header=False)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variable used throughout te notebooextract_text_from_pdf_url\n",
    "UPDATE = True"
   ]
  },
  {
   "source": [
    "# Load data and cleaning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_subsamples = 10000\n",
    "dataset = fetch_20newsgroups(\n",
    "    shuffle=True,\n",
    "    random_state=32,\n",
    "    remove=('headers', 'footers', 'qutes')\n",
    "    )[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Emails\n",
    "data = [re.sub(r\"\\S*@\\S*\\s?\", '', sent) for sent in dataset]\n",
    "\n",
    "# remove special characters\n",
    "data = [re.sub(r'[^A-Za-z]+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(11314, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "data_df = pd.DataFrame(data, columns=['paragraph_raw'])\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our list contains all english stop words \n",
    "stop_words = text.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✔ Download and installation successful\nYou can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# load spacy model to lematize text\n",
    "nlp = preprocess.load_spacy_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11314/11314 [08:52<00:00, 21.26it/s]\n"
     ]
    }
   ],
   "source": [
    "data_df = load_preprocess(data=data_df, dir_file='output/newsgroup_cleaned.txt', stop_words=stop_words, nlp=nlp, update=UPDATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(11314, 2)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                       paragraph_raw  \\\n",
       "0  The real question here in my opinion is what M...   \n",
       "1  Please could someone in the US give me the cur...   \n",
       "2  Can somebody please help me with information a...   \n",
       "3  In article Pat Myrto writes I am sick dismayed...   \n",
       "4  From article by John R Daker Cup holders drivi...   \n",
       "\n",
       "                                   paragraph_cleaned  \n",
       "0  real question opinion motorola processor run c...  \n",
       "1  current street price following relevant taxis ...  \n",
       "2  somebody help information american magnetics c...  \n",
       "3  article myrto write sick dismay discouraged as...  \n",
       "4  article john daker holder drive importantant u...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>paragraph_raw</th>\n      <th>paragraph_cleaned</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The real question here in my opinion is what M...</td>\n      <td>real question opinion motorola processor run c...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Please could someone in the US give me the cur...</td>\n      <td>current street price following relevant taxis ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Can somebody please help me with information a...</td>\n      <td>somebody help information american magnetics c...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>In article Pat Myrto writes I am sick dismayed...</td>\n      <td>article myrto write sick dismay discouraged as...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>From article by John R Daker Cup holders drivi...</td>\n      <td>article john daker holder drive importantant u...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "print(data_df.shape)\n",
    "data_df.head()"
   ]
  },
  {
   "source": [
    "## 1. Contextualized Topic Modelling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1.1 Prepare data for model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cleaned embeddings (lemmatized + stopwords removed)\n",
    "# handler = TextHandler('output/newsgroup_cleaned.txt')\n",
    "# handler.prepare()\n",
    "# handler.bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2020-11-17 17:19:59.128 INFO    gensim.corpora.dictionary: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-17 17:20:01.224 INFO    gensim.corpora.dictionary: adding document #10000 to Dictionary(51580 unique tokens: ['benchmark', 'comparable', 'conversation', 'datum', 'david']...)\n",
      "2020-11-17 17:20:01.478 INFO    gensim.corpora.dictionary: built Dictionary(54503 unique tokens: ['benchmark', 'comparable', 'conversation', 'datum', 'david']...) from 11314 documents (total 1129305 corpus positions)\n",
      "2020-11-17 17:20:01.588 INFO    gensim.corpora.dictionary: discarding 48459 tokens: [('know', 3635), ('question', 1486), ('refund', 13), ('cannon', 13), ('carson', 13), ('charger', 14), ('frode', 2), ('help', 1315), ('like', 3392), ('magnetics', 1)]...\n",
      "2020-11-17 17:20:01.590 INFO    gensim.corpora.dictionary: keeping 6044 tokens which were in no less than 15 and no more than 1131 (=10.0%) documents\n",
      "2020-11-17 17:20:01.633 INFO    gensim.corpora.dictionary: resulting dictionary: Dictionary(6044 unique tokens: ['benchmark', 'comparable', 'conversation', 'datum', 'david']...)\n",
      "6044\n"
     ]
    }
   ],
   "source": [
    "with open('output/newsgroup_cleaned.txt',\"r\") as fr:\n",
    "    text_cleaned = [doc.split() for doc in fr.read().splitlines()] # load text for NPMI\n",
    "\n",
    "dictionary = Dictionary(text_cleaned)\n",
    "\n",
    "'''\n",
    "Remove very rare and very common words:\n",
    "\n",
    "- words appearing less than 15 times\n",
    "- words appearing in more than 10% of all documents\n",
    "'''\n",
    "\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.10, keep_n= 100000)\n",
    "print(len(dictionary.token2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in text_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(11314, 6044)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "tf_array = matutils.corpus2dense(bow_corpus, num_terms=len(dictionary.token2id)).T\n",
    "tf_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to sparse matrix\n",
    "tf_array_sparse = sparse.csr_matrix(tf_array)\n",
    "tf_array_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2020-11-17 17:20:05.625 INFO    root: Load pretrained SentenceTransformer: distiluse-base-multilingual-cased\n",
      "2020-11-17 17:20:05.628 INFO    root: Did not find folder distiluse-base-multilingual-cased. Assume to download model from server.\n",
      "2020-11-17 17:20:05.669 INFO    root: Load SentenceTransformer from folder: C:\\Users\\Pieter-Jan/.cache\\torch\\sentence_transformers\\sbert.net_models_distiluse-base-multilingual-cased\n",
      "2020-11-17 17:20:10.014 INFO    root: Use pytorch device: cpu\n",
      "Batches: 100%|██████████| 57/57 [29:55<00:00, 31.49s/it]\n"
     ]
    }
   ],
   "source": [
    "# create or load bert embeddings (either use raw text or clean text)\n",
    "# we can expirment with both\n",
    "embeddings_bert = preprocess.load_bert_embeddings(\n",
    "        text_dir=\"output/newsgroup_raw.txt\", \n",
    "        model=\"distiluse-base-multilingual-cased\",\n",
    "        dir_embeddings=\"output/newsgroup_bert_Embeddings_raw.npy\",\n",
    "        update=UPDATE\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(11314, 512)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "embeddings_bert.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ivert dictionary\n",
    "inv_token2id = {v: k for k, v in dictionary.token2id.items()}\n",
    "# create dataset\n",
    "training_dataset = CTMDataset(tf_array_sparse, embeddings_bert, inv_token2id)"
   ]
  },
  {
   "source": [
    "## 1.2 Train model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Settings: \n",
      "                   N Components: 20\n",
      "                   Topic Prior Mean: 0.0\n",
      "                   Topic Prior Variance: 0.95\n",
      "                   Model Type: prodLDA\n",
      "                   Hidden Sizes: (100, 100)\n",
      "                   Activation: softplus\n",
      "                   Dropout: 0.2\n",
      "                   Learn Priors: True\n",
      "                   Learning Rate: 0.002\n",
      "                   Momentum: 0.99\n",
      "                   Reduce On Plateau: True\n",
      "                   Save Dir: None\n",
      "Epoch: [1/10]\tSamples: [11314/113140]\tTrain Loss: 644.7722137327095\tTime: 0:00:38.026307\n",
      "Epoch: [2/10]\tSamples: [22628/113140]\tTrain Loss: 617.8858217694891\tTime: 0:00:39.461052\n",
      "Epoch: [3/10]\tSamples: [33942/113140]\tTrain Loss: 607.5998524366935\tTime: 0:00:38.177199\n",
      "Epoch: [4/10]\tSamples: [45256/113140]\tTrain Loss: 601.2149657987338\tTime: 0:00:39.296127\n",
      "Epoch: [5/10]\tSamples: [56570/113140]\tTrain Loss: 596.8946186002961\tTime: 0:00:40.052483\n",
      "Epoch: [6/10]\tSamples: [67884/113140]\tTrain Loss: 594.3946885150588\tTime: 0:00:45.307819\n",
      "Epoch: [7/10]\tSamples: [79198/113140]\tTrain Loss: 591.4509814651649\tTime: 0:00:44.235774\n",
      "Epoch: [8/10]\tSamples: [90512/113140]\tTrain Loss: 588.9536145759678\tTime: 0:00:38.432548\n",
      "Epoch: [9/10]\tSamples: [101826/113140]\tTrain Loss: 588.4462807763059\tTime: 0:00:38.741511\n",
      "Epoch: [10/10]\tSamples: [113140/113140]\tTrain Loss: 586.6529267182805\tTime: 0:00:39.290397\n"
     ]
    }
   ],
   "source": [
    "random.seed(69)\n",
    "ctm = CTM(\n",
    "    input_size=len(dictionary.token2id), \n",
    "    bert_input_size=512, \n",
    "    n_components=20, \n",
    "    inference_type=\"combined\", \n",
    "    num_epochs=10,\n",
    "    reduce_on_plateau=True\n",
    "    )\n",
    "ctm.fit(training_dataset) # run model"
   ]
  },
  {
   "source": [
    "## 1.3 Evaluate topics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmt_topics_l = ctm.get_topic_lists(10)\n",
    "cmt_topics_d = {}\n",
    "for i in range(len(cmt_topics_l)):\n",
    "    cmt_topics_d[i] = cmt_topics_l[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              0           1               2            3            4  \\\n",
       "0     christian        life        religion        bible        claim   \n",
       "1    government       state         turkish     armenian     genocide   \n",
       "2          file     program            line      include       number   \n",
       "3         space        chip         shuttle        datum     computer   \n",
       "4       rapidly     satisfy          divide       unique     commonly   \n",
       "5          game        team          player         play       hockey   \n",
       "6          bike         car            ride        wheel       engine   \n",
       "7   motherboard     speaker      appreciate      monitor          pin   \n",
       "8    censorship  internally  electronically        sheep  specialized   \n",
       "9       satisfy      lawful           swiss  participant      academy   \n",
       "10     baseball       leafs          hitter       talent      toronto   \n",
       "11  specialized      lawful      internally         yale        swiss   \n",
       "12         life        live           jesus         talk         kill   \n",
       "13      windows      dialog          screen   appreciate         icon   \n",
       "14         waco     clinton         witness        trial         kent   \n",
       "15  participant     rapidly       detection      satisfy    regularly   \n",
       "16       toyota        tune         tempest     cellular        drain   \n",
       "17         chip  government           study      clipper   encryption   \n",
       "18        drive        card           cable        power        apple   \n",
       "19       israel     israeli            jews         arab       jewish   \n",
       "\n",
       "             5           6             7           8           9  \n",
       "0        jesus       truth       science        true      church  \n",
       "1     homicide     ottoman         force      turkey     firearm  \n",
       "2       output      window          base      follow       entry  \n",
       "3     software       orbit          nasa        mail        disk  \n",
       "4      capable   transport       secrecy   virtually     initial  \n",
       "5       season       score        league      second        goal  \n",
       "6         mile        road          rear       honda      dealer  \n",
       "7        audio     gateway         brand       video       board  \n",
       "8   invalidate  accessible  subsequently    dramatic     satisfy  \n",
       "9      rapidly  accessible      sysadmin    strictly     closely  \n",
       "10    pitching       staff         trade       sport         fan  \n",
       "11      deploy     rapidly    sternlight  benevolent  accumulate  \n",
       "12       woman       leave          word        didn      happen  \n",
       "13  resolution      button        window      expose     greatly  \n",
       "14     freedom      commit          batf   surrender       civil  \n",
       "15    contrast  accomplish    invalidate    sysadmin      divide  \n",
       "16  sternlight        bike     collision       honda       wagon  \n",
       "17       phone        drug           key      escrow      secure  \n",
       "18      driver        scsi       monitor       speed        high  \n",
       "19       peace       human         arabs       state    israelis  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>christian</td>\n      <td>life</td>\n      <td>religion</td>\n      <td>bible</td>\n      <td>claim</td>\n      <td>jesus</td>\n      <td>truth</td>\n      <td>science</td>\n      <td>true</td>\n      <td>church</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>government</td>\n      <td>state</td>\n      <td>turkish</td>\n      <td>armenian</td>\n      <td>genocide</td>\n      <td>homicide</td>\n      <td>ottoman</td>\n      <td>force</td>\n      <td>turkey</td>\n      <td>firearm</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>file</td>\n      <td>program</td>\n      <td>line</td>\n      <td>include</td>\n      <td>number</td>\n      <td>output</td>\n      <td>window</td>\n      <td>base</td>\n      <td>follow</td>\n      <td>entry</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>space</td>\n      <td>chip</td>\n      <td>shuttle</td>\n      <td>datum</td>\n      <td>computer</td>\n      <td>software</td>\n      <td>orbit</td>\n      <td>nasa</td>\n      <td>mail</td>\n      <td>disk</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>rapidly</td>\n      <td>satisfy</td>\n      <td>divide</td>\n      <td>unique</td>\n      <td>commonly</td>\n      <td>capable</td>\n      <td>transport</td>\n      <td>secrecy</td>\n      <td>virtually</td>\n      <td>initial</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>game</td>\n      <td>team</td>\n      <td>player</td>\n      <td>play</td>\n      <td>hockey</td>\n      <td>season</td>\n      <td>score</td>\n      <td>league</td>\n      <td>second</td>\n      <td>goal</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>bike</td>\n      <td>car</td>\n      <td>ride</td>\n      <td>wheel</td>\n      <td>engine</td>\n      <td>mile</td>\n      <td>road</td>\n      <td>rear</td>\n      <td>honda</td>\n      <td>dealer</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>motherboard</td>\n      <td>speaker</td>\n      <td>appreciate</td>\n      <td>monitor</td>\n      <td>pin</td>\n      <td>audio</td>\n      <td>gateway</td>\n      <td>brand</td>\n      <td>video</td>\n      <td>board</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>censorship</td>\n      <td>internally</td>\n      <td>electronically</td>\n      <td>sheep</td>\n      <td>specialized</td>\n      <td>invalidate</td>\n      <td>accessible</td>\n      <td>subsequently</td>\n      <td>dramatic</td>\n      <td>satisfy</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>satisfy</td>\n      <td>lawful</td>\n      <td>swiss</td>\n      <td>participant</td>\n      <td>academy</td>\n      <td>rapidly</td>\n      <td>accessible</td>\n      <td>sysadmin</td>\n      <td>strictly</td>\n      <td>closely</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>baseball</td>\n      <td>leafs</td>\n      <td>hitter</td>\n      <td>talent</td>\n      <td>toronto</td>\n      <td>pitching</td>\n      <td>staff</td>\n      <td>trade</td>\n      <td>sport</td>\n      <td>fan</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>specialized</td>\n      <td>lawful</td>\n      <td>internally</td>\n      <td>yale</td>\n      <td>swiss</td>\n      <td>deploy</td>\n      <td>rapidly</td>\n      <td>sternlight</td>\n      <td>benevolent</td>\n      <td>accumulate</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>life</td>\n      <td>live</td>\n      <td>jesus</td>\n      <td>talk</td>\n      <td>kill</td>\n      <td>woman</td>\n      <td>leave</td>\n      <td>word</td>\n      <td>didn</td>\n      <td>happen</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>windows</td>\n      <td>dialog</td>\n      <td>screen</td>\n      <td>appreciate</td>\n      <td>icon</td>\n      <td>resolution</td>\n      <td>button</td>\n      <td>window</td>\n      <td>expose</td>\n      <td>greatly</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>waco</td>\n      <td>clinton</td>\n      <td>witness</td>\n      <td>trial</td>\n      <td>kent</td>\n      <td>freedom</td>\n      <td>commit</td>\n      <td>batf</td>\n      <td>surrender</td>\n      <td>civil</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>participant</td>\n      <td>rapidly</td>\n      <td>detection</td>\n      <td>satisfy</td>\n      <td>regularly</td>\n      <td>contrast</td>\n      <td>accomplish</td>\n      <td>invalidate</td>\n      <td>sysadmin</td>\n      <td>divide</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>toyota</td>\n      <td>tune</td>\n      <td>tempest</td>\n      <td>cellular</td>\n      <td>drain</td>\n      <td>sternlight</td>\n      <td>bike</td>\n      <td>collision</td>\n      <td>honda</td>\n      <td>wagon</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>chip</td>\n      <td>government</td>\n      <td>study</td>\n      <td>clipper</td>\n      <td>encryption</td>\n      <td>phone</td>\n      <td>drug</td>\n      <td>key</td>\n      <td>escrow</td>\n      <td>secure</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>drive</td>\n      <td>card</td>\n      <td>cable</td>\n      <td>power</td>\n      <td>apple</td>\n      <td>driver</td>\n      <td>scsi</td>\n      <td>monitor</td>\n      <td>speed</td>\n      <td>high</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>israel</td>\n      <td>israeli</td>\n      <td>jews</td>\n      <td>arab</td>\n      <td>jewish</td>\n      <td>peace</td>\n      <td>human</td>\n      <td>arabs</td>\n      <td>state</td>\n      <td>israelis</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(cmt_topics_d).T"
   ]
  },
  {
   "source": [
    "### 1.3.1 Normalized Point-wise Mutual Information"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "tual)\n",
      "2020-11-17 17:57:00.388 INFO    gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (165845 virtual)\n",
      "2020-11-17 17:57:00.475 INFO    gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (176791 virtual)\n",
      "2020-11-17 17:57:00.495 INFO    gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (182474 virtual)\n",
      "2020-11-17 17:57:00.544 INFO    gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (192673 virtual)\n",
      "2020-11-17 17:57:00.615 INFO    gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (197668 virtual)\n",
      "2020-11-17 17:57:00.659 INFO    gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (205573 virtual)\n",
      "2020-11-17 17:57:00.721 INFO    gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (209516 virtual)\n",
      "2020-11-17 17:57:00.835 INFO    gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (216164 virtual)\n",
      "2020-11-17 17:57:00.953 INFO    gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (223980 virtual)\n",
      "2020-11-17 17:57:00.974 INFO    gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (228409 virtual)\n",
      "2020-11-17 17:57:01.059 INFO    gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (233054 virtual)\n",
      "2020-11-17 17:57:01.114 INFO    gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (238439 virtual)\n",
      "2020-11-17 17:57:01.223 INFO    gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (247882 virtual)\n",
      "2020-11-17 17:57:01.241 INFO    gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (253192 virtual)\n",
      "2020-11-17 17:57:01.314 INFO    gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (260129 virtual)\n",
      "2020-11-17 17:57:01.340 INFO    gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (264131 virtual)\n",
      "2020-11-17 17:57:01.362 INFO    gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (268222 virtual)\n",
      "2020-11-17 17:57:01.449 INFO    gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (278216 virtual)\n",
      "2020-11-17 17:57:01.519 INFO    gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (283538 virtual)\n",
      "2020-11-17 17:57:01.603 INFO    gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (290919 virtual)\n",
      "2020-11-17 17:57:01.611 INFO    gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (296175 virtual)\n",
      "2020-11-17 17:57:01.658 INFO    gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (301899 virtual)\n",
      "2020-11-17 17:57:01.713 INFO    gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (307215 virtual)\n",
      "2020-11-17 17:57:01.792 INFO    gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (312534 virtual)\n",
      "2020-11-17 17:57:01.885 INFO    gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (318164 virtual)\n",
      "2020-11-17 17:57:01.909 INFO    gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (323778 virtual)\n",
      "2020-11-17 17:57:01.946 INFO    gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (328253 virtual)\n",
      "2020-11-17 17:57:02.042 INFO    gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (334232 virtual)\n",
      "2020-11-17 17:57:02.055 INFO    gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (340689 virtual)\n",
      "2020-11-17 17:57:02.083 INFO    gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (345340 virtual)\n",
      "2020-11-17 17:57:02.206 INFO    gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (353698 virtual)\n",
      "2020-11-17 17:57:02.236 INFO    gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (360439 virtual)\n",
      "2020-11-17 17:57:02.245 INFO    gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (368413 virtual)\n",
      "2020-11-17 17:57:02.342 INFO    gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (372585 virtual)\n",
      "2020-11-17 17:57:02.377 INFO    gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (380264 virtual)\n",
      "2020-11-17 17:57:02.402 INFO    gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (386552 virtual)\n",
      "2020-11-17 17:57:02.537 INFO    gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (393181 virtual)\n",
      "2020-11-17 17:57:02.561 INFO    gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (399552 virtual)\n",
      "2020-11-17 17:57:02.593 INFO    gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (403780 virtual)\n",
      "2020-11-17 17:57:02.664 INFO    gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (408932 virtual)\n",
      "2020-11-17 17:57:02.813 INFO    gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (415894 virtual)\n",
      "2020-11-17 17:57:02.822 INFO    gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (424528 virtual)\n",
      "2020-11-17 17:57:02.925 INFO    gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (431009 virtual)\n",
      "2020-11-17 17:57:02.950 INFO    gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (442392 virtual)\n",
      "2020-11-17 17:57:02.996 INFO    gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (447035 virtual)\n",
      "2020-11-17 17:57:03.087 INFO    gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (451873 virtual)\n",
      "2020-11-17 17:57:03.178 INFO    gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (457809 virtual)\n",
      "2020-11-17 17:57:03.275 INFO    gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (464147 virtual)\n",
      "2020-11-17 17:57:03.304 INFO    gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (470545 virtual)\n",
      "2020-11-17 17:57:03.425 INFO    gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (475887 virtual)\n",
      "2020-11-17 17:57:03.432 INFO    gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (482165 virtual)\n",
      "2020-11-17 17:57:03.494 INFO    gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (496748 virtual)\n",
      "2020-11-17 17:57:03.568 INFO    gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (501538 virtual)\n",
      "2020-11-17 17:57:03.591 INFO    gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (508118 virtual)\n",
      "2020-11-17 17:57:03.642 INFO    gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (516023 virtual)\n",
      "2020-11-17 17:57:03.707 INFO    gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (522389 virtual)\n",
      "2020-11-17 17:57:03.745 INFO    gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (534686 virtual)\n",
      "2020-11-17 17:57:03.834 INFO    gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (540964 virtual)\n",
      "2020-11-17 17:57:03.912 INFO    gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (546909 virtual)\n",
      "2020-11-17 17:57:03.992 INFO    gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (552009 virtual)\n",
      "2020-11-17 17:57:04.020 INFO    gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (557953 virtual)\n",
      "2020-11-17 17:57:04.060 INFO    gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (562933 virtual)\n",
      "2020-11-17 17:57:04.187 INFO    gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (567381 virtual)\n",
      "2020-11-17 17:57:04.199 INFO    gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (572928 virtual)\n",
      "2020-11-17 17:57:04.302 INFO    gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (578212 virtual)\n",
      "2020-11-17 17:57:04.336 INFO    gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (583626 virtual)\n",
      "2020-11-17 17:57:04.358 INFO    gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (589862 virtual)\n",
      "2020-11-17 17:57:04.459 INFO    gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (594456 virtual)\n",
      "2020-11-17 17:57:04.486 INFO    gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (603378 virtual)\n",
      "2020-11-17 17:57:04.517 INFO    gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (608403 virtual)\n",
      "2020-11-17 17:57:04.608 INFO    gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (613390 virtual)\n",
      "2020-11-17 17:57:04.624 INFO    gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (618726 virtual)\n",
      "2020-11-17 17:57:04.678 INFO    gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (625212 virtual)\n",
      "2020-11-17 17:57:04.736 INFO    gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (635169 virtual)\n",
      "2020-11-17 17:57:04.806 INFO    gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (639124 virtual)\n",
      "2020-11-17 17:57:04.843 INFO    gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (648490 virtual)\n",
      "2020-11-17 17:57:04.863 INFO    gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (653854 virtual)\n",
      "2020-11-17 17:57:04.944 INFO    gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (660037 virtual)\n",
      "2020-11-17 17:57:05.061 INFO    gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (665011 virtual)\n",
      "2020-11-17 17:57:05.097 INFO    gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (669900 virtual)\n",
      "2020-11-17 17:57:05.178 INFO    gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (678613 virtual)\n",
      "2020-11-17 17:57:05.257 INFO    gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (683997 virtual)\n",
      "2020-11-17 17:57:05.303 INFO    gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (689124 virtual)\n",
      "2020-11-17 17:57:05.364 INFO    gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (697925 virtual)\n",
      "2020-11-17 17:57:05.379 INFO    gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (706292 virtual)\n",
      "2020-11-17 17:57:05.426 INFO    gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (716307 virtual)\n",
      "2020-11-17 17:57:05.524 INFO    gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (722110 virtual)\n",
      "2020-11-17 17:57:05.627 INFO    gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (726735 virtual)\n",
      "2020-11-17 17:57:05.639 INFO    gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (733822 virtual)\n",
      "2020-11-17 17:57:05.751 INFO    gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (739706 virtual)\n",
      "2020-11-17 17:57:05.877 INFO    gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (744205 virtual)\n",
      "2020-11-17 17:57:05.903 INFO    gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (749674 virtual)\n",
      "2020-11-17 17:57:05.937 INFO    gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (759906 virtual)\n",
      "2020-11-17 17:57:06.013 INFO    gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (764578 virtual)\n",
      "2020-11-17 17:57:06.070 INFO    gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (769878 virtual)\n",
      "2020-11-17 17:57:06.078 INFO    gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (774214 virtual)\n",
      "2020-11-17 17:57:06.162 INFO    gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (779610 virtual)\n",
      "2020-11-17 17:57:06.231 INFO    gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (784515 virtual)\n",
      "2020-11-17 17:57:06.293 INFO    gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (791197 virtual)\n",
      "2020-11-17 17:57:06.342 INFO    gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (797170 virtual)\n",
      "2020-11-17 17:57:06.388 INFO    gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (801975 virtual)\n",
      "2020-11-17 17:57:06.409 INFO    gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (807232 virtual)\n",
      "2020-11-17 17:57:06.480 INFO    gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (814159 virtual)\n",
      "2020-11-17 17:57:06.510 INFO    gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (820004 virtual)\n",
      "2020-11-17 17:57:06.589 INFO    gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (825778 virtual)\n",
      "2020-11-17 17:57:06.655 INFO    gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (830259 virtual)\n",
      "2020-11-17 17:57:06.668 INFO    gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (835953 virtual)\n",
      "2020-11-17 17:57:06.760 INFO    gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (842541 virtual)\n",
      "2020-11-17 17:57:06.816 INFO    gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (849295 virtual)\n",
      "2020-11-17 17:57:06.829 INFO    gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (858108 virtual)\n",
      "2020-11-17 17:57:06.915 INFO    gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (864095 virtual)\n",
      "2020-11-17 17:57:06.948 INFO    gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (870967 virtual)\n",
      "2020-11-17 17:57:06.971 INFO    gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (876638 virtual)\n",
      "2020-11-17 17:57:07.069 INFO    gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (880946 virtual)\n",
      "2020-11-17 17:57:07.108 INFO    gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (890167 virtual)\n",
      "2020-11-17 17:57:07.177 INFO    gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (894451 virtual)\n",
      "2020-11-17 17:57:07.227 INFO    gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (903920 virtual)\n",
      "2020-11-17 17:57:07.288 INFO    gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (909441 virtual)\n",
      "2020-11-17 17:57:07.327 INFO    gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (914612 virtual)\n",
      "2020-11-17 17:57:07.339 INFO    gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (922345 virtual)\n",
      "2020-11-17 17:57:07.451 INFO    gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (930230 virtual)\n",
      "2020-11-17 17:57:07.490 INFO    gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (938465 virtual)\n",
      "2020-11-17 17:57:07.596 INFO    gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (943584 virtual)\n",
      "2020-11-17 17:57:07.602 INFO    gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (948641 virtual)\n",
      "2020-11-17 17:57:07.624 INFO    gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (954597 virtual)\n",
      "2020-11-17 17:57:07.805 INFO    gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (959039 virtual)\n",
      "2020-11-17 17:57:07.812 INFO    gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (963940 virtual)\n",
      "2020-11-17 17:57:07.832 INFO    gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (978828 virtual)\n",
      "2020-11-17 17:57:07.934 INFO    gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (983634 virtual)\n",
      "2020-11-17 17:57:07.949 INFO    gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (989005 virtual)\n",
      "2020-11-17 17:57:07.982 INFO    gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (996906 virtual)\n",
      "2020-11-17 17:57:08.066 INFO    gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (1003495 virtual)\n",
      "2020-11-17 17:57:08.094 INFO    gensim.topic_coherence.text_analysis: 158 batches submitted to accumulate stats from 10112 documents (1005185 virtual)\n",
      "2020-11-17 17:57:08.644 INFO    gensim.topic_coherence.text_analysis: 3 accumulators retrieved from output queue\n",
      "2020-11-17 17:57:08.676 INFO    gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 1005727 virtual documents\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-0.07338773817323598"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "npmi = CoherenceNPMI(texts=text_cleaned, topics=ctm.get_topic_lists(10))\n",
    "npmi.score()"
   ]
  },
  {
   "source": [
    "### 1.3.2 External Word Embeddings Topic Coherence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2020-11-17 17:57:09.736 INFO    gensim.models.utils_any2vec: loading projection weights from C:\\Users\\Pieter-Jan/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n",
      "2020-11-17 17:58:24.303 INFO    gensim.models.utils_any2vec: loaded (3000000, 300) matrix from C:\\Users\\Pieter-Jan/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.15084322"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "CoherenceWordEmbeddings(ctm.get_topic_lists(10)).score()"
   ]
  },
  {
   "source": [
    "### 1.3.3 Rank-Biased Overlap "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9859468748523684"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "InvertedRBO(ctm.get_topic_lists(10)).score()"
   ]
  },
  {
   "source": [
    "## 2. Latent Dirichlet Allocation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.1 Train model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "at\" + 0.009*\"line\" + 0.009*\"output\" + 0.009*\"send\" + 0.008*\"information\" + 0.008*\"available\"\n",
      "2020-11-17 17:59:46.400 INFO    gensim.models.ldamodel: topic #2 (0.050): 0.012*\"science\" + 0.012*\"exist\" + 0.009*\"evidence\" + 0.008*\"book\" + 0.007*\"theory\" + 0.007*\"atheist\" + 0.007*\"atheism\" + 0.007*\"existence\" + 0.006*\"study\" + 0.006*\"universe\"\n",
      "2020-11-17 17:59:46.402 INFO    gensim.models.ldamodel: topic #14 (0.050): 0.015*\"president\" + 0.010*\"water\" + 0.007*\"job\" + 0.005*\"homosexual\" + 0.005*\"talk\" + 0.005*\"number\" + 0.004*\"package\" + 0.004*\"consider\" + 0.004*\"group\" + 0.004*\"state\"\n",
      "2020-11-17 17:59:46.404 INFO    gensim.models.ldamodel: topic #9 (0.050): 0.007*\"weapon\" + 0.006*\"gun\" + 0.006*\"car\" + 0.006*\"drive\" + 0.005*\"engine\" + 0.005*\"firearm\" + 0.005*\"cost\" + 0.004*\"high\" + 0.004*\"light\" + 0.004*\"control\"\n",
      "2020-11-17 17:59:46.407 INFO    gensim.models.ldamodel: topic #7 (0.050): 0.014*\"printer\" + 0.014*\"book\" + 0.011*\"instruction\" + 0.009*\"power\" + 0.009*\"price\" + 0.008*\"battery\" + 0.008*\"father\" + 0.008*\"circuit\" + 0.007*\"risc\" + 0.006*\"spirit\"\n",
      "2020-11-17 17:59:46.410 INFO    gensim.models.ldamodel: topic diff=0.452637, rho=0.281083\n",
      "2020-11-17 17:59:49.182 INFO    gensim.models.ldamodel: -7.543 per-word bound, 186.6 perplexity estimate based on a held-out corpus of 1314 documents with 101726 words\n",
      "2020-11-17 17:59:49.195 INFO    gensim.models.ldamodel: merging changes from 3314 documents into a model of 11314 documents\n",
      "2020-11-17 17:59:49.210 INFO    gensim.models.ldamodel: topic #6 (0.050): 0.015*\"window\" + 0.010*\"application\" + 0.010*\"display\" + 0.010*\"widget\" + 0.010*\"server\" + 0.008*\"include\" + 0.007*\"version\" + 0.007*\"motif\" + 0.007*\"program\" + 0.007*\"available\"\n",
      "2020-11-17 17:59:49.212 INFO    gensim.models.ldamodel: topic #18 (0.050): 0.062*\"drive\" + 0.024*\"disk\" + 0.018*\"scsi\" + 0.017*\"hard\" + 0.015*\"controller\" + 0.010*\"floppy\" + 0.009*\"power\" + 0.007*\"jumper\" + 0.007*\"card\" + 0.006*\"slave\"\n",
      "2020-11-17 17:59:49.215 INFO    gensim.models.ldamodel: topic #17 (0.050): 0.017*\"mail\" + 0.010*\"price\" + 0.008*\"sell\" + 0.008*\"email\" + 0.008*\"computer\" + 0.008*\"modem\" + 0.008*\"send\" + 0.008*\"list\" + 0.008*\"offer\" + 0.008*\"sale\"\n",
      "2020-11-17 17:59:49.217 INFO    gensim.models.ldamodel: topic #11 (0.050): 0.036*\"window\" + 0.028*\"card\" + 0.021*\"driver\" + 0.012*\"memory\" + 0.011*\"monitor\" + 0.011*\"video\" + 0.009*\"mode\" + 0.009*\"machine\" + 0.009*\"color\" + 0.009*\"fast\"\n",
      "2020-11-17 17:59:49.219 INFO    gensim.models.ldamodel: topic #13 (0.050): 0.026*\"game\" + 0.022*\"team\" + 0.018*\"play\" + 0.015*\"player\" + 0.010*\"season\" + 0.008*\"hockey\" + 0.008*\"score\" + 0.007*\"league\" + 0.006*\"baseball\" + 0.006*\"goal\"\n",
      "2020-11-17 17:59:49.221 INFO    gensim.models.ldamodel: topic diff=0.421991, rho=0.281083\n",
      "2020-11-17 17:59:51.901 INFO    gensim.models.ldamodel: -7.487 per-word bound, 179.4 perplexity estimate based on a held-out corpus of 1314 documents with 101726 words\n",
      "2020-11-17 17:59:51.903 INFO    gensim.models.ldamulticore: PROGRESS: pass 7, dispatched chunk #0 = documents up to #2000/11314, outstanding queue size 1\n",
      "2020-11-17 17:59:51.964 INFO    gensim.models.ldamulticore: PROGRESS: pass 7, dispatched chunk #1 = documents up to #4000/11314, outstanding queue size 2\n",
      "2020-11-17 17:59:52.052 INFO    gensim.models.ldamulticore: PROGRESS: pass 7, dispatched chunk #2 = documents up to #6000/11314, outstanding queue size 3\n",
      "2020-11-17 17:59:52.130 INFO    gensim.models.ldamulticore: PROGRESS: pass 7, dispatched chunk #3 = documents up to #8000/11314, outstanding queue size 4\n",
      "2020-11-17 17:59:52.132 INFO    gensim.models.ldamulticore: PROGRESS: pass 7, dispatched chunk #4 = documents up to #10000/11314, outstanding queue size 5\n",
      "2020-11-17 17:59:52.134 INFO    gensim.models.ldamulticore: PROGRESS: pass 7, dispatched chunk #5 = documents up to #11314/11314, outstanding queue size 6\n",
      "2020-11-17 17:59:55.635 INFO    gensim.models.ldamodel: merging changes from 4000 documents into a model of 11314 documents\n",
      "2020-11-17 17:59:55.650 INFO    gensim.models.ldamodel: topic #19 (0.050): 0.017*\"israel\" + 0.013*\"jews\" + 0.012*\"israeli\" + 0.007*\"state\" + 0.007*\"arab\" + 0.007*\"religion\" + 0.007*\"country\" + 0.005*\"absolute\" + 0.005*\"attack\" + 0.005*\"jewish\"\n",
      "2020-11-17 17:59:55.652 INFO    gensim.models.ldamodel: topic #4 (0.050): 0.014*\"government\" + 0.008*\"public\" + 0.006*\"clinton\" + 0.006*\"police\" + 0.005*\"american\" + 0.005*\"secret\" + 0.005*\"state\" + 0.005*\"number\" + 0.005*\"attack\" + 0.005*\"information\"\n",
      "2020-11-17 17:59:55.653 INFO    gensim.models.ldamodel: topic #10 (0.050): 0.011*\"wire\" + 0.011*\"food\" + 0.011*\"ground\" + 0.006*\"level\" + 0.006*\"circuit\" + 0.005*\"software\" + 0.005*\"outlet\" + 0.005*\"program\" + 0.005*\"wiring\" + 0.004*\"power\"\n",
      "2020-11-17 17:59:55.655 INFO    gensim.models.ldamodel: topic #9 (0.050): 0.007*\"weapon\" + 0.007*\"gun\" + 0.006*\"car\" + 0.006*\"drive\" + 0.006*\"firearm\" + 0.005*\"engine\" + 0.005*\"cost\" + 0.004*\"high\" + 0.004*\"control\" + 0.004*\"auto\"\n",
      "2020-11-17 17:59:55.657 INFO    gensim.models.ldamodel: topic #0 (0.050): 0.023*\"space\" + 0.010*\"launch\" + 0.008*\"nasa\" + 0.007*\"program\" + 0.007*\"orbit\" + 0.007*\"satellite\" + 0.007*\"earth\" + 0.006*\"center\" + 0.006*\"mission\" + 0.006*\"health\"\n",
      "2020-11-17 17:59:55.659 INFO    gensim.models.ldamodel: topic diff=0.373817, rho=0.270597\n",
      "2020-11-17 17:59:58.193 INFO    gensim.models.ldamodel: -7.502 per-word bound, 181.3 perplexity estimate based on a held-out corpus of 1314 documents with 101726 words\n",
      "2020-11-17 17:59:58.463 INFO    gensim.models.ldamodel: merging changes from 4000 documents into a model of 11314 documents\n",
      "2020-11-17 17:59:58.476 INFO    gensim.models.ldamodel: topic #6 (0.050): 0.016*\"window\" + 0.011*\"server\" + 0.011*\"display\" + 0.010*\"application\" + 0.009*\"widget\" + 0.008*\"motif\" + 0.008*\"version\" + 0.008*\"include\" + 0.008*\"program\" + 0.007*\"available\"\n",
      "2020-11-17 17:59:58.478 INFO    gensim.models.ldamodel: topic #7 (0.050): 0.014*\"book\" + 0.014*\"printer\" + 0.011*\"instruction\" + 0.010*\"power\" + 0.010*\"battery\" + 0.009*\"price\" + 0.008*\"circuit\" + 0.007*\"risc\" + 0.007*\"father\" + 0.006*\"print\"\n",
      "2020-11-17 17:59:58.480 INFO    gensim.models.ldamodel: topic #13 (0.050): 0.026*\"game\" + 0.022*\"team\" + 0.019*\"play\" + 0.015*\"player\" + 0.011*\"season\" + 0.008*\"hockey\" + 0.008*\"score\" + 0.007*\"league\" + 0.006*\"baseball\" + 0.005*\"goal\"\n",
      "2020-11-17 17:59:58.482 INFO    gensim.models.ldamodel: topic #1 (0.050): 0.022*\"church\" + 0.011*\"catholic\" + 0.011*\"ripem\" + 0.010*\"pope\" + 0.010*\"code\" + 0.008*\"authority\" + 0.008*\"group\" + 0.007*\"change\" + 0.007*\"patent\" + 0.007*\"message\"\n",
      "2020-11-17 17:59:58.484 INFO    gensim.models.ldamodel: topic #14 (0.050): 0.016*\"president\" + 0.010*\"water\" + 0.007*\"job\" + 0.006*\"homosexual\" + 0.006*\"talk\" + 0.005*\"number\" + 0.005*\"package\" + 0.004*\"group\" + 0.004*\"consider\" + 0.004*\"state\"\n",
      "2020-11-17 17:59:58.487 INFO    gensim.models.ldamodel: topic diff=0.355837, rho=0.270597\n",
      "2020-11-17 18:00:00.680 INFO    gensim.models.ldamodel: -7.520 per-word bound, 183.5 perplexity estimate based on a held-out corpus of 1314 documents with 101726 words\n",
      "2020-11-17 18:00:00.693 INFO    gensim.models.ldamodel: merging changes from 3314 documents into a model of 11314 documents\n",
      "2020-11-17 18:00:00.708 INFO    gensim.models.ldamodel: topic #19 (0.050): 0.019*\"israel\" + 0.013*\"israeli\" + 0.013*\"jews\" + 0.008*\"state\" + 0.007*\"arab\" + 0.007*\"country\" + 0.006*\"religion\" + 0.006*\"attack\" + 0.005*\"jewish\" + 0.005*\"absolute\"\n",
      "2020-11-17 18:00:00.710 INFO    gensim.models.ldamodel: topic #11 (0.050): 0.035*\"window\" + 0.028*\"card\" + 0.021*\"driver\" + 0.013*\"memory\" + 0.011*\"monitor\" + 0.011*\"video\" + 0.010*\"mode\" + 0.010*\"machine\" + 0.009*\"color\" + 0.009*\"fast\"\n",
      "2020-11-17 18:00:00.712 INFO    gensim.models.ldamodel: topic #8 (0.050): 0.012*\"armenian\" + 0.010*\"turkish\" + 0.009*\"armenians\" + 0.007*\"child\" + 0.007*\"kill\" + 0.005*\"woman\" + 0.005*\"government\" + 0.005*\"turkey\" + 0.005*\"armenia\" + 0.004*\"happen\"\n",
      "2020-11-17 18:00:00.714 INFO    gensim.models.ldamodel: topic #0 (0.050): 0.024*\"space\" + 0.011*\"launch\" + 0.008*\"nasa\" + 0.008*\"satellite\" + 0.007*\"orbit\" + 0.007*\"program\" + 0.007*\"earth\" + 0.006*\"mission\" + 0.006*\"center\" + 0.006*\"health\"\n",
      "2020-11-17 18:00:00.716 INFO    gensim.models.ldamodel: topic #17 (0.050): 0.018*\"mail\" + 0.010*\"price\" + 0.009*\"sell\" + 0.009*\"email\" + 0.008*\"offer\" + 0.008*\"computer\" + 0.008*\"list\" + 0.008*\"sale\" + 0.008*\"send\" + 0.008*\"modem\"\n",
      "2020-11-17 18:00:00.719 INFO    gensim.models.ldamodel: topic diff=0.331443, rho=0.270597\n",
      "2020-11-17 18:00:02.294 INFO    gensim.models.ldamodel: -7.468 per-word bound, 177.1 perplexity estimate based on a held-out corpus of 1314 documents with 101726 words\n",
      "2020-11-17 18:00:02.297 INFO    gensim.models.ldamulticore: PROGRESS: pass 8, dispatched chunk #0 = documents up to #2000/11314, outstanding queue size 1\n",
      "2020-11-17 18:00:02.357 INFO    gensim.models.ldamulticore: PROGRESS: pass 8, dispatched chunk #1 = documents up to #4000/11314, outstanding queue size 2\n",
      "2020-11-17 18:00:02.414 INFO    gensim.models.ldamulticore: PROGRESS: pass 8, dispatched chunk #2 = documents up to #6000/11314, outstanding queue size 3\n",
      "2020-11-17 18:00:02.481 INFO    gensim.models.ldamulticore: PROGRESS: pass 8, dispatched chunk #3 = documents up to #8000/11314, outstanding queue size 4\n",
      "2020-11-17 18:00:02.483 INFO    gensim.models.ldamulticore: PROGRESS: pass 8, dispatched chunk #4 = documents up to #10000/11314, outstanding queue size 5\n",
      "2020-11-17 18:00:02.486 INFO    gensim.models.ldamulticore: PROGRESS: pass 8, dispatched chunk #5 = documents up to #11314/11314, outstanding queue size 6\n",
      "2020-11-17 18:00:05.187 INFO    gensim.models.ldamodel: merging changes from 4000 documents into a model of 11314 documents\n",
      "2020-11-17 18:00:05.201 INFO    gensim.models.ldamodel: topic #18 (0.050): 0.065*\"drive\" + 0.026*\"disk\" + 0.026*\"scsi\" + 0.019*\"hard\" + 0.016*\"controller\" + 0.011*\"floppy\" + 0.008*\"power\" + 0.007*\"card\" + 0.007*\"jumper\" + 0.006*\"cable\"\n",
      "2020-11-17 18:00:05.203 INFO    gensim.models.ldamodel: topic #15 (0.050): 0.017*\"chip\" + 0.015*\"encryption\" + 0.014*\"government\" + 0.011*\"clipper\" + 0.009*\"security\" + 0.009*\"privacy\" + 0.008*\"key\" + 0.007*\"technology\" + 0.007*\"protect\" + 0.007*\"escrow\"\n",
      "2020-11-17 18:00:05.210 INFO    gensim.models.ldamodel: topic #13 (0.050): 0.026*\"game\" + 0.022*\"team\" + 0.018*\"play\" + 0.015*\"player\" + 0.011*\"season\" + 0.009*\"hockey\" + 0.008*\"score\" + 0.007*\"league\" + 0.006*\"baseball\" + 0.005*\"goal\"\n",
      "2020-11-17 18:00:05.212 INFO    gensim.models.ldamodel: topic #17 (0.050): 0.018*\"mail\" + 0.010*\"price\" + 0.009*\"email\" + 0.009*\"sell\" + 0.009*\"computer\" + 0.009*\"sale\" + 0.008*\"offer\" + 0.008*\"list\" + 0.008*\"send\" + 0.008*\"modem\"\n",
      "2020-11-17 18:00:05.215 INFO    gensim.models.ldamodel: topic #19 (0.050): 0.017*\"israel\" + 0.014*\"jews\" + 0.012*\"israeli\" + 0.008*\"state\" + 0.007*\"arab\" + 0.007*\"country\" + 0.006*\"religion\" + 0.006*\"jewish\" + 0.006*\"attack\" + 0.005*\"absolute\"\n",
      "2020-11-17 18:00:05.218 INFO    gensim.models.ldamodel: topic diff=0.294192, rho=0.261203\n",
      "2020-11-17 18:00:07.457 INFO    gensim.models.ldamodel: -7.484 per-word bound, 179.0 perplexity estimate based on a held-out corpus of 1314 documents with 101726 words\n",
      "2020-11-17 18:00:07.704 INFO    gensim.models.ldamodel: merging changes from 4000 documents into a model of 11314 documents\n",
      "2020-11-17 18:00:07.716 INFO    gensim.models.ldamodel: topic #0 (0.050): 0.024*\"space\" + 0.009*\"launch\" + 0.009*\"nasa\" + 0.007*\"program\" + 0.007*\"center\" + 0.007*\"satellite\" + 0.006*\"orbit\" + 0.006*\"earth\" + 0.006*\"health\" + 0.006*\"research\"\n",
      "2020-11-17 18:00:07.718 INFO    gensim.models.ldamodel: topic #3 (0.050): 0.015*\"moral\" + 0.013*\"morality\" + 0.013*\"objective\" + 0.009*\"book\" + 0.009*\"animal\" + 0.007*\"natural\" + 0.007*\"speed\" + 0.007*\"human\" + 0.006*\"claim\" + 0.006*\"language\"\n",
      "2020-11-17 18:00:07.720 INFO    gensim.models.ldamodel: topic #1 (0.050): 0.023*\"church\" + 0.011*\"catholic\" + 0.011*\"ripem\" + 0.010*\"pope\" + 0.010*\"code\" + 0.009*\"authority\" + 0.008*\"group\" + 0.007*\"change\" + 0.007*\"patent\" + 0.007*\"message\"\n",
      "2020-11-17 18:00:07.722 INFO    gensim.models.ldamodel: topic #11 (0.050): 0.035*\"window\" + 0.028*\"card\" + 0.020*\"driver\" + 0.012*\"memory\" + 0.012*\"monitor\" + 0.011*\"video\" + 0.010*\"color\" + 0.010*\"machine\" + 0.010*\"mode\" + 0.010*\"fast\"\n",
      "2020-11-17 18:00:07.723 INFO    gensim.models.ldamodel: topic #14 (0.050): 0.017*\"president\" + 0.010*\"water\" + 0.008*\"job\" + 0.006*\"homosexual\" + 0.006*\"talk\" + 0.005*\"number\" + 0.005*\"package\" + 0.005*\"group\" + 0.005*\"consider\" + 0.004*\"support\"\n",
      "2020-11-17 18:00:07.726 INFO    gensim.models.ldamodel: topic diff=0.283743, rho=0.261203\n",
      "2020-11-17 18:00:09.811 INFO    gensim.models.ldamodel: -7.502 per-word bound, 181.3 perplexity estimate based on a held-out corpus of 1314 documents with 101726 words\n",
      "2020-11-17 18:00:09.854 INFO    gensim.models.ldamodel: merging changes from 3314 documents into a model of 11314 documents\n",
      "2020-11-17 18:00:09.866 INFO    gensim.models.ldamodel: topic #13 (0.050): 0.027*\"game\" + 0.022*\"team\" + 0.018*\"play\" + 0.015*\"player\" + 0.010*\"season\" + 0.008*\"hockey\" + 0.007*\"score\" + 0.007*\"league\" + 0.006*\"baseball\" + 0.005*\"goal\"\n",
      "2020-11-17 18:00:09.868 INFO    gensim.models.ldamodel: topic #8 (0.050): 0.012*\"armenian\" + 0.010*\"turkish\" + 0.010*\"armenians\" + 0.008*\"child\" + 0.007*\"kill\" + 0.006*\"government\" + 0.005*\"turkey\" + 0.005*\"woman\" + 0.005*\"armenia\" + 0.005*\"greek\"\n",
      "2020-11-17 18:00:09.872 INFO    gensim.models.ldamodel: topic #16 (0.050): 0.012*\"bike\" + 0.008*\"didn\" + 0.007*\"ride\" + 0.007*\"doctor\" + 0.006*\"hear\" + 0.005*\"remember\" + 0.005*\"disease\" + 0.005*\"patient\" + 0.004*\"home\" + 0.004*\"night\"\n",
      "2020-11-17 18:00:09.875 INFO    gensim.models.ldamodel: topic #7 (0.050): 0.016*\"book\" + 0.014*\"printer\" + 0.012*\"power\" + 0.011*\"instruction\" + 0.011*\"battery\" + 0.009*\"price\" + 0.008*\"circuit\" + 0.007*\"design\" + 0.007*\"input\" + 0.007*\"print\"\n",
      "2020-11-17 18:00:09.877 INFO    gensim.models.ldamodel: topic #10 (0.050): 0.013*\"wire\" + 0.013*\"ground\" + 0.011*\"food\" + 0.006*\"circuit\" + 0.006*\"outlet\" + 0.006*\"level\" + 0.006*\"wiring\" + 0.005*\"neutral\" + 0.005*\"software\" + 0.005*\"power\"\n",
      "2020-11-17 18:00:09.882 INFO    gensim.models.ldamodel: topic diff=0.265724, rho=0.261203\n",
      "2020-11-17 18:00:11.456 INFO    gensim.models.ldamodel: -7.455 per-word bound, 175.4 perplexity estimate based on a held-out corpus of 1314 documents with 101726 words\n",
      "2020-11-17 18:00:11.459 INFO    gensim.models.ldamulticore: PROGRESS: pass 9, dispatched chunk #0 = documents up to #2000/11314, outstanding queue size 1\n",
      "2020-11-17 18:00:11.512 INFO    gensim.models.ldamulticore: PROGRESS: pass 9, dispatched chunk #1 = documents up to #4000/11314, outstanding queue size 2\n",
      "2020-11-17 18:00:11.574 INFO    gensim.models.ldamulticore: PROGRESS: pass 9, dispatched chunk #2 = documents up to #6000/11314, outstanding queue size 3\n",
      "2020-11-17 18:00:11.636 INFO    gensim.models.ldamulticore: PROGRESS: pass 9, dispatched chunk #3 = documents up to #8000/11314, outstanding queue size 4\n",
      "2020-11-17 18:00:11.637 INFO    gensim.models.ldamulticore: PROGRESS: pass 9, dispatched chunk #4 = documents up to #10000/11314, outstanding queue size 5\n",
      "2020-11-17 18:00:11.639 INFO    gensim.models.ldamulticore: PROGRESS: pass 9, dispatched chunk #5 = documents up to #11314/11314, outstanding queue size 6\n",
      "2020-11-17 18:00:14.166 INFO    gensim.models.ldamodel: merging changes from 4000 documents into a model of 11314 documents\n",
      "2020-11-17 18:00:14.189 INFO    gensim.models.ldamodel: topic #14 (0.050): 0.018*\"president\" + 0.009*\"water\" + 0.007*\"job\" + 0.006*\"homosexual\" + 0.006*\"talk\" + 0.005*\"package\" + 0.005*\"consider\" + 0.005*\"number\" + 0.005*\"group\" + 0.005*\"myers\"\n",
      "2020-11-17 18:00:14.191 INFO    gensim.models.ldamodel: topic #11 (0.050): 0.034*\"window\" + 0.028*\"card\" + 0.020*\"driver\" + 0.012*\"memory\" + 0.012*\"monitor\" + 0.011*\"video\" + 0.010*\"machine\" + 0.010*\"color\" + 0.010*\"mode\" + 0.009*\"fast\"\n",
      "2020-11-17 18:00:14.194 INFO    gensim.models.ldamodel: topic #2 (0.050): 0.013*\"exist\" + 0.012*\"science\" + 0.010*\"evidence\" + 0.008*\"atheist\" + 0.007*\"atheism\" + 0.007*\"theory\" + 0.007*\"argument\" + 0.007*\"book\" + 0.006*\"existence\" + 0.006*\"reason\"\n",
      "2020-11-17 18:00:14.197 INFO    gensim.models.ldamodel: topic #4 (0.050): 0.016*\"government\" + 0.009*\"public\" + 0.006*\"clinton\" + 0.006*\"state\" + 0.006*\"police\" + 0.006*\"american\" + 0.005*\"secret\" + 0.005*\"attack\" + 0.005*\"number\" + 0.005*\"information\"\n",
      "2020-11-17 18:00:14.200 INFO    gensim.models.ldamodel: topic #17 (0.050): 0.019*\"mail\" + 0.011*\"price\" + 0.010*\"email\" + 0.010*\"sell\" + 0.009*\"sale\" + 0.009*\"offer\" + 0.009*\"list\" + 0.009*\"computer\" + 0.009*\"send\" + 0.008*\"internet\"\n",
      "2020-11-17 18:00:14.203 INFO    gensim.models.ldamodel: topic diff=0.236626, rho=0.252724\n",
      "2020-11-17 18:00:16.411 INFO    gensim.models.ldamodel: -7.471 per-word bound, 177.4 perplexity estimate based on a held-out corpus of 1314 documents with 101726 words\n",
      "2020-11-17 18:00:16.533 INFO    gensim.models.ldamodel: merging changes from 4000 documents into a model of 11314 documents\n",
      "2020-11-17 18:00:16.547 INFO    gensim.models.ldamodel: topic #14 (0.050): 0.018*\"president\" + 0.010*\"water\" + 0.008*\"job\" + 0.006*\"homosexual\" + 0.006*\"talk\" + 0.005*\"package\" + 0.005*\"number\" + 0.005*\"group\" + 0.005*\"consider\" + 0.005*\"support\"\n",
      "2020-11-17 18:00:16.549 INFO    gensim.models.ldamodel: topic #17 (0.050): 0.019*\"mail\" + 0.010*\"price\" + 0.010*\"email\" + 0.010*\"sell\" + 0.009*\"offer\" + 0.009*\"sale\" + 0.009*\"list\" + 0.009*\"send\" + 0.008*\"computer\" + 0.008*\"internet\"\n",
      "2020-11-17 18:00:16.551 INFO    gensim.models.ldamodel: topic #2 (0.050): 0.013*\"exist\" + 0.012*\"science\" + 0.010*\"evidence\" + 0.009*\"atheist\" + 0.007*\"theory\" + 0.007*\"atheism\" + 0.007*\"argument\" + 0.006*\"book\" + 0.006*\"existence\" + 0.006*\"reason\"\n",
      "2020-11-17 18:00:16.553 INFO    gensim.models.ldamodel: topic #3 (0.050): 0.016*\"moral\" + 0.014*\"objective\" + 0.014*\"morality\" + 0.009*\"animal\" + 0.009*\"book\" + 0.007*\"natural\" + 0.007*\"human\" + 0.006*\"speed\" + 0.006*\"claim\" + 0.006*\"language\"\n",
      "2020-11-17 18:00:16.555 INFO    gensim.models.ldamodel: topic #5 (0.050): 0.014*\"jesus\" + 0.011*\"christian\" + 0.008*\"bible\" + 0.008*\"life\" + 0.006*\"christ\" + 0.006*\"word\" + 0.005*\"faith\" + 0.005*\"death\" + 0.005*\"love\" + 0.005*\"world\"\n",
      "2020-11-17 18:00:16.575 INFO    gensim.models.ldamodel: topic diff=0.231840, rho=0.252724\n",
      "2020-11-17 18:00:18.641 INFO    gensim.models.ldamodel: -7.489 per-word bound, 179.6 perplexity estimate based on a held-out corpus of 1314 documents with 101726 words\n",
      "2020-11-17 18:00:18.654 INFO    gensim.models.ldamodel: merging changes from 3314 documents into a model of 11314 documents\n",
      "2020-11-17 18:00:18.667 INFO    gensim.models.ldamodel: topic #1 (0.050): 0.022*\"church\" + 0.012*\"ripem\" + 0.011*\"catholic\" + 0.011*\"pope\" + 0.009*\"authority\" + 0.009*\"code\" + 0.008*\"group\" + 0.007*\"patent\" + 0.007*\"message\" + 0.007*\"public\"\n",
      "2020-11-17 18:00:18.669 INFO    gensim.models.ldamodel: topic #0 (0.050): 0.025*\"space\" + 0.011*\"launch\" + 0.008*\"nasa\" + 0.008*\"satellite\" + 0.007*\"orbit\" + 0.007*\"program\" + 0.007*\"center\" + 0.007*\"earth\" + 0.006*\"mission\" + 0.006*\"health\"\n",
      "2020-11-17 18:00:18.671 INFO    gensim.models.ldamodel: topic #18 (0.050): 0.069*\"drive\" + 0.029*\"scsi\" + 0.029*\"disk\" + 0.019*\"hard\" + 0.017*\"controller\" + 0.012*\"floppy\" + 0.009*\"power\" + 0.008*\"tape\" + 0.008*\"jumper\" + 0.008*\"card\"\n",
      "2020-11-17 18:00:18.674 INFO    gensim.models.ldamodel: topic #4 (0.050): 0.016*\"government\" + 0.009*\"public\" + 0.007*\"state\" + 0.007*\"clinton\" + 0.006*\"secret\" + 0.006*\"police\" + 0.005*\"american\" + 0.005*\"attack\" + 0.005*\"number\" + 0.004*\"court\"\n",
      "2020-11-17 18:00:18.676 INFO    gensim.models.ldamodel: topic #12 (0.050): 0.048*\"file\" + 0.017*\"program\" + 0.016*\"image\" + 0.011*\"entry\" + 0.011*\"output\" + 0.010*\"send\" + 0.010*\"line\" + 0.010*\"format\" + 0.008*\"available\" + 0.008*\"information\"\n",
      "2020-11-17 18:00:18.678 INFO    gensim.models.ldamodel: topic diff=0.218864, rho=0.252724\n",
      "2020-11-17 18:00:19.981 INFO    gensim.models.ldamodel: -7.445 per-word bound, 174.2 perplexity estimate based on a held-out corpus of 1314 documents with 101726 words\n"
     ]
    }
   ],
   "source": [
    "num_topics = 20\n",
    "lda_model =  LdaMulticore(\n",
    "    corpus=bow_corpus, \n",
    "    num_topics = num_topics, \n",
    "    id2word = dictionary,                                    \n",
    "    passes = 10,\n",
    "    workers = 2\n",
    "    )"
   ]
  },
  {
   "source": [
    "## 2.2 Evaluate topics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topics_d = {}\n",
    "lda_topics_l = []\n",
    "for i in range(num_topics):\n",
    "    t = [w[0] for w in lda_model.show_topic(i)[0:10]]\n",
    "    lda_topics_d[i+1] = t\n",
    "    lda_topics_l.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             0           1            2           3            4           5  \\\n",
       "1        space      launch         nasa   satellite        orbit     program   \n",
       "2       church       ripem     catholic        pope    authority        code   \n",
       "3        exist     science     evidence     atheist       theory     atheism   \n",
       "4        moral   objective     morality      animal         book     natural   \n",
       "5   government      public        state     clinton       secret      police   \n",
       "6        jesus   christian        bible        life       christ        word   \n",
       "7       window     display  application      widget       server       motif   \n",
       "8         book     printer        power     battery  instruction       price   \n",
       "9     armenian     turkish    armenians       child         kill  government   \n",
       "10      weapon         car        drive         gun      firearm      engine   \n",
       "11        wire      ground         food     circuit       outlet       level   \n",
       "12      window        card       driver      memory      monitor       video   \n",
       "13        file     program        image       entry       output        send   \n",
       "14        game        team         play      player       season      hockey   \n",
       "15   president       water          job  homosexual         talk       myers   \n",
       "16        chip  encryption   government     clipper     security     privacy   \n",
       "17        bike        didn         ride      doctor         hear    remember   \n",
       "18        mail       price         sell       offer        email        list   \n",
       "19       drive        scsi         disk        hard   controller      floppy   \n",
       "20      israel     israeli         jews       state      country        arab   \n",
       "\n",
       "           6           7          8            9  \n",
       "1     center       earth    mission       health  \n",
       "2      group      patent    message       public  \n",
       "3   argument     example     reason         book  \n",
       "4      human       claim      keith        speed  \n",
       "5   american      attack     number        court  \n",
       "6      faith        love      death        world  \n",
       "7    program     include       code      version  \n",
       "8    circuit       input     design        print  \n",
       "9     turkey       woman    armenia        greek  \n",
       "10      high        rate      speed         auto  \n",
       "11    wiring     neutral      power     software  \n",
       "12      mode     machine      color         fast  \n",
       "13      line      format  available  information  \n",
       "14     score      league   baseball         goal  \n",
       "15   package      number   consider      support  \n",
       "16       key  technology     escrow      protect  \n",
       "17   disease     patient       week         pain  \n",
       "18      sale        send   computer        modem  \n",
       "19     power        tape     jumper         card  \n",
       "20    jewish      attack      peace         land  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>space</td>\n      <td>launch</td>\n      <td>nasa</td>\n      <td>satellite</td>\n      <td>orbit</td>\n      <td>program</td>\n      <td>center</td>\n      <td>earth</td>\n      <td>mission</td>\n      <td>health</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>church</td>\n      <td>ripem</td>\n      <td>catholic</td>\n      <td>pope</td>\n      <td>authority</td>\n      <td>code</td>\n      <td>group</td>\n      <td>patent</td>\n      <td>message</td>\n      <td>public</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>exist</td>\n      <td>science</td>\n      <td>evidence</td>\n      <td>atheist</td>\n      <td>theory</td>\n      <td>atheism</td>\n      <td>argument</td>\n      <td>example</td>\n      <td>reason</td>\n      <td>book</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>moral</td>\n      <td>objective</td>\n      <td>morality</td>\n      <td>animal</td>\n      <td>book</td>\n      <td>natural</td>\n      <td>human</td>\n      <td>claim</td>\n      <td>keith</td>\n      <td>speed</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>government</td>\n      <td>public</td>\n      <td>state</td>\n      <td>clinton</td>\n      <td>secret</td>\n      <td>police</td>\n      <td>american</td>\n      <td>attack</td>\n      <td>number</td>\n      <td>court</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>jesus</td>\n      <td>christian</td>\n      <td>bible</td>\n      <td>life</td>\n      <td>christ</td>\n      <td>word</td>\n      <td>faith</td>\n      <td>love</td>\n      <td>death</td>\n      <td>world</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>window</td>\n      <td>display</td>\n      <td>application</td>\n      <td>widget</td>\n      <td>server</td>\n      <td>motif</td>\n      <td>program</td>\n      <td>include</td>\n      <td>code</td>\n      <td>version</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>book</td>\n      <td>printer</td>\n      <td>power</td>\n      <td>battery</td>\n      <td>instruction</td>\n      <td>price</td>\n      <td>circuit</td>\n      <td>input</td>\n      <td>design</td>\n      <td>print</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>armenian</td>\n      <td>turkish</td>\n      <td>armenians</td>\n      <td>child</td>\n      <td>kill</td>\n      <td>government</td>\n      <td>turkey</td>\n      <td>woman</td>\n      <td>armenia</td>\n      <td>greek</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>weapon</td>\n      <td>car</td>\n      <td>drive</td>\n      <td>gun</td>\n      <td>firearm</td>\n      <td>engine</td>\n      <td>high</td>\n      <td>rate</td>\n      <td>speed</td>\n      <td>auto</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>wire</td>\n      <td>ground</td>\n      <td>food</td>\n      <td>circuit</td>\n      <td>outlet</td>\n      <td>level</td>\n      <td>wiring</td>\n      <td>neutral</td>\n      <td>power</td>\n      <td>software</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>window</td>\n      <td>card</td>\n      <td>driver</td>\n      <td>memory</td>\n      <td>monitor</td>\n      <td>video</td>\n      <td>mode</td>\n      <td>machine</td>\n      <td>color</td>\n      <td>fast</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>file</td>\n      <td>program</td>\n      <td>image</td>\n      <td>entry</td>\n      <td>output</td>\n      <td>send</td>\n      <td>line</td>\n      <td>format</td>\n      <td>available</td>\n      <td>information</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>game</td>\n      <td>team</td>\n      <td>play</td>\n      <td>player</td>\n      <td>season</td>\n      <td>hockey</td>\n      <td>score</td>\n      <td>league</td>\n      <td>baseball</td>\n      <td>goal</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>president</td>\n      <td>water</td>\n      <td>job</td>\n      <td>homosexual</td>\n      <td>talk</td>\n      <td>myers</td>\n      <td>package</td>\n      <td>number</td>\n      <td>consider</td>\n      <td>support</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>chip</td>\n      <td>encryption</td>\n      <td>government</td>\n      <td>clipper</td>\n      <td>security</td>\n      <td>privacy</td>\n      <td>key</td>\n      <td>technology</td>\n      <td>escrow</td>\n      <td>protect</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>bike</td>\n      <td>didn</td>\n      <td>ride</td>\n      <td>doctor</td>\n      <td>hear</td>\n      <td>remember</td>\n      <td>disease</td>\n      <td>patient</td>\n      <td>week</td>\n      <td>pain</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>mail</td>\n      <td>price</td>\n      <td>sell</td>\n      <td>offer</td>\n      <td>email</td>\n      <td>list</td>\n      <td>sale</td>\n      <td>send</td>\n      <td>computer</td>\n      <td>modem</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>drive</td>\n      <td>scsi</td>\n      <td>disk</td>\n      <td>hard</td>\n      <td>controller</td>\n      <td>floppy</td>\n      <td>power</td>\n      <td>tape</td>\n      <td>jumper</td>\n      <td>card</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>israel</td>\n      <td>israeli</td>\n      <td>jews</td>\n      <td>state</td>\n      <td>country</td>\n      <td>arab</td>\n      <td>jewish</td>\n      <td>attack</td>\n      <td>peace</td>\n      <td>land</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "# show topics\n",
    "# evert row is a topic\n",
    "pd.DataFrame.from_dict(lda_topics_d).T"
   ]
  },
  {
   "source": [
    "### 2.2.1 Normalized Point-wise Mutual Information"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "18:00:31.208 INFO    gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (213827 virtual)\n",
      "2020-11-17 18:00:31.229 INFO    gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (220191 virtual)\n",
      "2020-11-17 18:00:31.282 INFO    gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (227051 virtual)\n",
      "2020-11-17 18:00:31.407 INFO    gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (231646 virtual)\n",
      "2020-11-17 18:00:31.509 INFO    gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (236039 virtual)\n",
      "2020-11-17 18:00:31.557 INFO    gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (241718 virtual)\n",
      "2020-11-17 18:00:31.570 INFO    gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (251723 virtual)\n",
      "2020-11-17 18:00:31.706 INFO    gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (255540 virtual)\n",
      "2020-11-17 18:00:31.721 INFO    gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (262383 virtual)\n",
      "2020-11-17 18:00:31.768 INFO    gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (266411 virtual)\n",
      "2020-11-17 18:00:31.843 INFO    gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (270499 virtual)\n",
      "2020-11-17 18:00:31.925 INFO    gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (280422 virtual)\n",
      "2020-11-17 18:00:31.990 INFO    gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (284119 virtual)\n",
      "2020-11-17 18:00:32.101 INFO    gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (290451 virtual)\n",
      "2020-11-17 18:00:32.129 INFO    gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (296986 virtual)\n",
      "2020-11-17 18:00:32.145 INFO    gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (302335 virtual)\n",
      "2020-11-17 18:00:32.237 INFO    gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (308121 virtual)\n",
      "2020-11-17 18:00:32.269 INFO    gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (312574 virtual)\n",
      "2020-11-17 18:00:32.436 INFO    gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (318714 virtual)\n",
      "2020-11-17 18:00:32.451 INFO    gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (323923 virtual)\n",
      "2020-11-17 18:00:32.514 INFO    gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (329153 virtual)\n",
      "2020-11-17 18:00:32.612 INFO    gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (334811 virtual)\n",
      "2020-11-17 18:00:32.656 INFO    gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (340177 virtual)\n",
      "2020-11-17 18:00:32.666 INFO    gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (345085 virtual)\n",
      "2020-11-17 18:00:32.772 INFO    gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (353739 virtual)\n",
      "2020-11-17 18:00:32.792 INFO    gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (358174 virtual)\n",
      "2020-11-17 18:00:32.825 INFO    gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (364729 virtual)\n",
      "2020-11-17 18:00:32.916 INFO    gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (372447 virtual)\n",
      "2020-11-17 18:00:32.940 INFO    gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (376527 virtual)\n",
      "2020-11-17 18:00:33.002 INFO    gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (384111 virtual)\n",
      "2020-11-17 18:00:33.104 INFO    gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (390228 virtual)\n",
      "2020-11-17 18:00:33.174 INFO    gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (396321 virtual)\n",
      "2020-11-17 18:00:33.197 INFO    gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (402730 virtual)\n",
      "2020-11-17 18:00:33.280 INFO    gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (406858 virtual)\n",
      "2020-11-17 18:00:33.333 INFO    gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (411848 virtual)\n",
      "2020-11-17 18:00:33.402 INFO    gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (418690 virtual)\n",
      "2020-11-17 18:00:33.464 INFO    gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (427738 virtual)\n",
      "2020-11-17 18:00:33.516 INFO    gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (433988 virtual)\n",
      "2020-11-17 18:00:33.586 INFO    gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (443369 virtual)\n",
      "2020-11-17 18:00:33.594 INFO    gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (449511 virtual)\n",
      "2020-11-17 18:00:33.668 INFO    gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (453163 virtual)\n",
      "2020-11-17 18:00:33.776 INFO    gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (458249 virtual)\n",
      "2020-11-17 18:00:33.883 INFO    gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (464746 virtual)\n",
      "2020-11-17 18:00:33.929 INFO    gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (470823 virtual)\n",
      "2020-11-17 18:00:34.074 INFO    gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (476330 virtual)\n",
      "2020-11-17 18:00:34.123 INFO    gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (482554 virtual)\n",
      "2020-11-17 18:00:34.200 INFO    gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (490554 virtual)\n",
      "2020-11-17 18:00:34.269 INFO    gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (501946 virtual)\n",
      "2020-11-17 18:00:34.319 INFO    gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (507085 virtual)\n",
      "2020-11-17 18:00:34.364 INFO    gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (513218 virtual)\n",
      "2020-11-17 18:00:34.434 INFO    gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (520575 virtual)\n",
      "2020-11-17 18:00:34.486 INFO    gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (526822 virtual)\n",
      "2020-11-17 18:00:34.606 INFO    gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (531353 virtual)\n",
      "2020-11-17 18:00:34.637 INFO    gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (545361 virtual)\n",
      "2020-11-17 18:00:34.750 INFO    gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (550060 virtual)\n",
      "2020-11-17 18:00:34.796 INFO    gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (555816 virtual)\n",
      "2020-11-17 18:00:34.843 INFO    gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (561235 virtual)\n",
      "2020-11-17 18:00:34.925 INFO    gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (566442 virtual)\n",
      "2020-11-17 18:00:34.935 INFO    gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (570227 virtual)\n",
      "2020-11-17 18:00:35.071 INFO    gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (576102 virtual)\n",
      "2020-11-17 18:00:35.093 INFO    gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (581208 virtual)\n",
      "2020-11-17 18:00:35.248 INFO    gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (585534 virtual)\n",
      "2020-11-17 18:00:35.262 INFO    gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (591078 virtual)\n",
      "2020-11-17 18:00:35.274 INFO    gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (597345 virtual)\n",
      "2020-11-17 18:00:35.385 INFO    gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (602637 virtual)\n",
      "2020-11-17 18:00:35.442 INFO    gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (610390 virtual)\n",
      "2020-11-17 18:00:35.459 INFO    gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (615270 virtual)\n",
      "2020-11-17 18:00:35.514 INFO    gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (620186 virtual)\n",
      "2020-11-17 18:00:35.617 INFO    gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (625427 virtual)\n",
      "2020-11-17 18:00:35.653 INFO    gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (631939 virtual)\n",
      "2020-11-17 18:00:35.686 INFO    gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (641512 virtual)\n",
      "2020-11-17 18:00:35.827 INFO    gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (645380 virtual)\n",
      "2020-11-17 18:00:35.845 INFO    gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (654568 virtual)\n",
      "2020-11-17 18:00:35.868 INFO    gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (659843 virtual)\n",
      "2020-11-17 18:00:36.012 INFO    gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (666350 virtual)\n",
      "2020-11-17 18:00:36.047 INFO    gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (669788 virtual)\n",
      "2020-11-17 18:00:36.144 INFO    gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (675444 virtual)\n",
      "2020-11-17 18:00:36.200 INFO    gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (683504 virtual)\n",
      "2020-11-17 18:00:36.303 INFO    gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (688597 virtual)\n",
      "2020-11-17 18:00:36.319 INFO    gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (694341 virtual)\n",
      "2020-11-17 18:00:36.427 INFO    gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (702500 virtual)\n",
      "2020-11-17 18:00:36.435 INFO    gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (711288 virtual)\n",
      "2020-11-17 18:00:36.513 INFO    gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (716496 virtual)\n",
      "2020-11-17 18:00:36.606 INFO    gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (726964 virtual)\n",
      "2020-11-17 18:00:36.662 INFO    gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (730831 virtual)\n",
      "2020-11-17 18:00:36.713 INFO    gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (736442 virtual)\n",
      "2020-11-17 18:00:36.834 INFO    gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (742740 virtual)\n",
      "2020-11-17 18:00:36.882 INFO    gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (748538 virtual)\n",
      "2020-11-17 18:00:36.947 INFO    gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (752697 virtual)\n",
      "2020-11-17 18:00:37.009 INFO    gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (757932 virtual)\n",
      "2020-11-17 18:00:37.114 INFO    gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (768013 virtual)\n",
      "2020-11-17 18:00:37.178 INFO    gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (772543 virtual)\n",
      "2020-11-17 18:00:37.199 INFO    gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (777302 virtual)\n",
      "2020-11-17 18:00:37.294 INFO    gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (781622 virtual)\n",
      "2020-11-17 18:00:37.300 INFO    gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (786707 virtual)\n",
      "2020-11-17 18:00:37.348 INFO    gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (792086 virtual)\n",
      "2020-11-17 18:00:37.451 INFO    gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (797639 virtual)\n",
      "2020-11-17 18:00:37.535 INFO    gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (803798 virtual)\n",
      "2020-11-17 18:00:37.595 INFO    gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (808528 virtual)\n",
      "2020-11-17 18:00:37.646 INFO    gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (814031 virtual)\n",
      "2020-11-17 18:00:37.696 INFO    gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (821367 virtual)\n",
      "2020-11-17 18:00:37.754 INFO    gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (825642 virtual)\n",
      "2020-11-17 18:00:37.841 INFO    gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (832414 virtual)\n",
      "2020-11-17 18:00:37.885 INFO    gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (836810 virtual)\n",
      "2020-11-17 18:00:37.896 INFO    gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (840597 virtual)\n",
      "2020-11-17 18:00:38.022 INFO    gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (846000 virtual)\n",
      "2020-11-17 18:00:38.075 INFO    gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (854380 virtual)\n",
      "2020-11-17 18:00:38.119 INFO    gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (858655 virtual)\n",
      "2020-11-17 18:00:38.275 INFO    gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (866518 virtual)\n",
      "2020-11-17 18:00:38.283 INFO    gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (873194 virtual)\n",
      "2020-11-17 18:00:38.304 INFO    gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (880206 virtual)\n",
      "2020-11-17 18:00:38.439 INFO    gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (884454 virtual)\n",
      "2020-11-17 18:00:38.444 INFO    gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (890099 virtual)\n",
      "2020-11-17 18:00:38.565 INFO    gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (898990 virtual)\n",
      "2020-11-17 18:00:38.679 INFO    gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (902792 virtual)\n",
      "2020-11-17 18:00:38.715 INFO    gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (907900 virtual)\n",
      "2020-11-17 18:00:38.804 INFO    gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (917249 virtual)\n",
      "2020-11-17 18:00:38.840 INFO    gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (921776 virtual)\n",
      "2020-11-17 18:00:38.897 INFO    gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (930527 virtual)\n",
      "2020-11-17 18:00:38.957 INFO    gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (935036 virtual)\n",
      "2020-11-17 18:00:39.050 INFO    gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (943546 virtual)\n",
      "2020-11-17 18:00:39.071 INFO    gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (950553 virtual)\n",
      "2020-11-17 18:00:39.184 INFO    gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (955449 virtual)\n",
      "2020-11-17 18:00:39.294 INFO    gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (960594 virtual)\n",
      "2020-11-17 18:00:39.312 INFO    gensim.topic_coherence.text_analysis: 158 batches submitted to accumulate stats from 10112 documents (965973 virtual)\n",
      "2020-11-17 18:00:39.318 INFO    gensim.topic_coherence.text_analysis: 159 batches submitted to accumulate stats from 10176 documents (970262 virtual)\n",
      "2020-11-17 18:00:39.440 INFO    gensim.topic_coherence.text_analysis: 160 batches submitted to accumulate stats from 10240 documents (975000 virtual)\n",
      "2020-11-17 18:00:39.514 INFO    gensim.topic_coherence.text_analysis: 161 batches submitted to accumulate stats from 10304 documents (989575 virtual)\n",
      "2020-11-17 18:00:39.552 INFO    gensim.topic_coherence.text_analysis: 162 batches submitted to accumulate stats from 10368 documents (994290 virtual)\n",
      "2020-11-17 18:00:39.617 INFO    gensim.topic_coherence.text_analysis: 163 batches submitted to accumulate stats from 10432 documents (999449 virtual)\n",
      "2020-11-17 18:00:39.709 INFO    gensim.topic_coherence.text_analysis: 164 batches submitted to accumulate stats from 10496 documents (1007230 virtual)\n",
      "2020-11-17 18:00:39.715 INFO    gensim.topic_coherence.text_analysis: 165 batches submitted to accumulate stats from 10560 documents (1013387 virtual)\n",
      "2020-11-17 18:00:39.773 INFO    gensim.topic_coherence.text_analysis: 166 batches submitted to accumulate stats from 10624 documents (1016192 virtual)\n",
      "2020-11-17 18:00:40.342 INFO    gensim.topic_coherence.text_analysis: 3 accumulators retrieved from output queue\n",
      "2020-11-17 18:00:40.382 INFO    gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 1017002 virtual documents\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.09898559684649942"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "npmi = CoherenceNPMI(texts=text_cleaned, topics=lda_topics_l)\n",
    "npmi.score()"
   ]
  },
  {
   "source": [
    "### 2.2.2 External Word Embeddings Topic Coherence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2020-11-17 18:00:41.526 INFO    gensim.models.utils_any2vec: loading projection weights from C:\\Users\\Pieter-Jan/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n",
      "2020-11-17 18:01:55.661 INFO    gensim.models.utils_any2vec: loaded (3000000, 300) matrix from C:\\Users\\Pieter-Jan/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.1608593"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "CoherenceWordEmbeddings(lda_topics_l).score()"
   ]
  },
  {
   "source": [
    "### 2.2.3 Rank-Biased Overlap "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.991251863478609"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "InvertedRBO(lda_topics_l).score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "# pyLDAvis.enable_notebook()\n",
    "# LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "# LDAvis_prepared"
   ]
  },
  {
   "source": [
    "## 3. Bert Topic"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.1 Train model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2020-11-17 18:02:35,173 - BERTopic - Reduced dimensionality with UMAP\n",
      "2020-11-17 18:02:35.173 INFO    BERTopic: Reduced dimensionality with UMAP\n",
      "2020-11-17 18:02:38,756 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2020-11-17 18:02:38.756 INFO    BERTopic: Clustered UMAP embeddings with HDBSCAN\n"
     ]
    }
   ],
   "source": [
    "random.seed(69)\n",
    "\n",
    "with open('output/newsgroup_cleaned.txt',\"r\") as fr:\n",
    "    docs = [doc for doc in fr.read().splitlines()] \n",
    "\n",
    "model = BERTopic(verbose=True)\n",
    "topics = model.fit_transform(docs, embeddings_bert)"
   ]
  },
  {
   "source": [
    "## 3.2 Evaluate topics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "topcis_b = model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract words for each topic\n",
    "topics_k = {}\n",
    "for k,v in topcis_b.items():\n",
    "    t_words = []\n",
    "    for w in v:\n",
    "        t_words.append(w[0])\n",
    "    # append the first 10 words\n",
    "    topics_k[k] = t_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              0               1              2           3             4  \\\n",
       "-1    president  stephanopoulos            say     believe        people   \n",
       " 0         team            game           play      player        season   \n",
       " 1       jewish      lowenstein         koufax  stankowitz         sandy   \n",
       " 2         test         inguiry           woof   subscribe         hello   \n",
       " 3         bike            ride            car  motorcycle        engine   \n",
       " 4         nrhj            gizw           wwiz        bxom          bhjn   \n",
       " 5       window            file        program       space         drive   \n",
       " 6         drug         cocaine        alcohol   marijuana  legalization   \n",
       " 7         food         patient        disease      doctor       medical   \n",
       " 8     armenian         turkish      armenians      turkey       armenia   \n",
       " 9   homosexual          sexual  homosexuality       child       clayton   \n",
       " 10    morality       objective          moral       value        animal   \n",
       " 11     firearm             gun         weapon     handgun         crime   \n",
       " 12  government     libertarian   constitution    limbaugh         state   \n",
       " 13     clinton         deficit           bush       taxis        income   \n",
       " 14        chip      encryption        clipper         key        escrow   \n",
       " 15        batf        compound         koresh        waco          tear   \n",
       " 16  punishment           cruel          death    innocent       penalty   \n",
       " 17      israel         israeli           jews        arab         arabs   \n",
       " 18       islam         islamic       religion     rushdie        muslim   \n",
       " 19     context           quote        respond      meritt          post   \n",
       " 20       jesus       christian          bible      church       believe   \n",
       "\n",
       "              5            6               7               8            9  \n",
       "-1          job           go          health           state        think  \n",
       " 0       hockey        score          league            year     baseball  \n",
       " 1         hank         berg            lame        baseball         rack  \n",
       " 2       critus         hdfd            thud        bilinsky     melittin  \n",
       " 3         road        brake            tire          helmet        drive  \n",
       " 4      maxbyte         tbxn            nriz            byte         pnei  \n",
       " 5         card          use           image        software        thank  \n",
       " 6       heroin    cigarette             kid        legalize        legal  \n",
       " 7    treatment       health            pain           cause         diet  \n",
       " 8         turk        greek        genocide      azerbaijan          say  \n",
       " 9         male          gay          cramer     promiscuous  orientation  \n",
       " 10     natural      immoral           dwyer          hudson         goal  \n",
       " 11     militia    amendment        criminal             arm      control  \n",
       " 12  regulation        power       hendricks  libertarianism        right  \n",
       " 13      reagan    president  administration      encryption     investor  \n",
       " 14       phone   government          secure         encrypt       crypto  \n",
       " 15       child         burn        building           start      assault  \n",
       " 16      murder         kill           crime          commit       guilty  \n",
       " 17    lebanese  palestinian          jewish         lebanon     israelis  \n",
       " 18     muslims        gregg        khomeini          jaeger        fatwa  \n",
       " 19       right    telepathy           weiss           human         take  \n",
       " 20     atheist        faith          belief        religion        exist  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>-1</th>\n      <td>president</td>\n      <td>stephanopoulos</td>\n      <td>say</td>\n      <td>believe</td>\n      <td>people</td>\n      <td>job</td>\n      <td>go</td>\n      <td>health</td>\n      <td>state</td>\n      <td>think</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>team</td>\n      <td>game</td>\n      <td>play</td>\n      <td>player</td>\n      <td>season</td>\n      <td>hockey</td>\n      <td>score</td>\n      <td>league</td>\n      <td>year</td>\n      <td>baseball</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>jewish</td>\n      <td>lowenstein</td>\n      <td>koufax</td>\n      <td>stankowitz</td>\n      <td>sandy</td>\n      <td>hank</td>\n      <td>berg</td>\n      <td>lame</td>\n      <td>baseball</td>\n      <td>rack</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>test</td>\n      <td>inguiry</td>\n      <td>woof</td>\n      <td>subscribe</td>\n      <td>hello</td>\n      <td>critus</td>\n      <td>hdfd</td>\n      <td>thud</td>\n      <td>bilinsky</td>\n      <td>melittin</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>bike</td>\n      <td>ride</td>\n      <td>car</td>\n      <td>motorcycle</td>\n      <td>engine</td>\n      <td>road</td>\n      <td>brake</td>\n      <td>tire</td>\n      <td>helmet</td>\n      <td>drive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>nrhj</td>\n      <td>gizw</td>\n      <td>wwiz</td>\n      <td>bxom</td>\n      <td>bhjn</td>\n      <td>maxbyte</td>\n      <td>tbxn</td>\n      <td>nriz</td>\n      <td>byte</td>\n      <td>pnei</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>window</td>\n      <td>file</td>\n      <td>program</td>\n      <td>space</td>\n      <td>drive</td>\n      <td>card</td>\n      <td>use</td>\n      <td>image</td>\n      <td>software</td>\n      <td>thank</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>drug</td>\n      <td>cocaine</td>\n      <td>alcohol</td>\n      <td>marijuana</td>\n      <td>legalization</td>\n      <td>heroin</td>\n      <td>cigarette</td>\n      <td>kid</td>\n      <td>legalize</td>\n      <td>legal</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>food</td>\n      <td>patient</td>\n      <td>disease</td>\n      <td>doctor</td>\n      <td>medical</td>\n      <td>treatment</td>\n      <td>health</td>\n      <td>pain</td>\n      <td>cause</td>\n      <td>diet</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>armenian</td>\n      <td>turkish</td>\n      <td>armenians</td>\n      <td>turkey</td>\n      <td>armenia</td>\n      <td>turk</td>\n      <td>greek</td>\n      <td>genocide</td>\n      <td>azerbaijan</td>\n      <td>say</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>homosexual</td>\n      <td>sexual</td>\n      <td>homosexuality</td>\n      <td>child</td>\n      <td>clayton</td>\n      <td>male</td>\n      <td>gay</td>\n      <td>cramer</td>\n      <td>promiscuous</td>\n      <td>orientation</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>morality</td>\n      <td>objective</td>\n      <td>moral</td>\n      <td>value</td>\n      <td>animal</td>\n      <td>natural</td>\n      <td>immoral</td>\n      <td>dwyer</td>\n      <td>hudson</td>\n      <td>goal</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>firearm</td>\n      <td>gun</td>\n      <td>weapon</td>\n      <td>handgun</td>\n      <td>crime</td>\n      <td>militia</td>\n      <td>amendment</td>\n      <td>criminal</td>\n      <td>arm</td>\n      <td>control</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>government</td>\n      <td>libertarian</td>\n      <td>constitution</td>\n      <td>limbaugh</td>\n      <td>state</td>\n      <td>regulation</td>\n      <td>power</td>\n      <td>hendricks</td>\n      <td>libertarianism</td>\n      <td>right</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>clinton</td>\n      <td>deficit</td>\n      <td>bush</td>\n      <td>taxis</td>\n      <td>income</td>\n      <td>reagan</td>\n      <td>president</td>\n      <td>administration</td>\n      <td>encryption</td>\n      <td>investor</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>chip</td>\n      <td>encryption</td>\n      <td>clipper</td>\n      <td>key</td>\n      <td>escrow</td>\n      <td>phone</td>\n      <td>government</td>\n      <td>secure</td>\n      <td>encrypt</td>\n      <td>crypto</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>batf</td>\n      <td>compound</td>\n      <td>koresh</td>\n      <td>waco</td>\n      <td>tear</td>\n      <td>child</td>\n      <td>burn</td>\n      <td>building</td>\n      <td>start</td>\n      <td>assault</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>punishment</td>\n      <td>cruel</td>\n      <td>death</td>\n      <td>innocent</td>\n      <td>penalty</td>\n      <td>murder</td>\n      <td>kill</td>\n      <td>crime</td>\n      <td>commit</td>\n      <td>guilty</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>israel</td>\n      <td>israeli</td>\n      <td>jews</td>\n      <td>arab</td>\n      <td>arabs</td>\n      <td>lebanese</td>\n      <td>palestinian</td>\n      <td>jewish</td>\n      <td>lebanon</td>\n      <td>israelis</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>islam</td>\n      <td>islamic</td>\n      <td>religion</td>\n      <td>rushdie</td>\n      <td>muslim</td>\n      <td>muslims</td>\n      <td>gregg</td>\n      <td>khomeini</td>\n      <td>jaeger</td>\n      <td>fatwa</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>context</td>\n      <td>quote</td>\n      <td>respond</td>\n      <td>meritt</td>\n      <td>post</td>\n      <td>right</td>\n      <td>telepathy</td>\n      <td>weiss</td>\n      <td>human</td>\n      <td>take</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>jesus</td>\n      <td>christian</td>\n      <td>bible</td>\n      <td>church</td>\n      <td>believe</td>\n      <td>atheist</td>\n      <td>faith</td>\n      <td>belief</td>\n      <td>religion</td>\n      <td>exist</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "topics_bert = list(topics_k.values())\n",
    "pd.DataFrame.from_dict(topics_k).T"
   ]
  },
  {
   "source": [
    "### 3.2.1 Normalized Point-wise Mutual Information"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "17 18:02:53.759 INFO    gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (205250 virtual)\n",
      "2020-11-17 18:02:53.810 INFO    gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (209999 virtual)\n",
      "2020-11-17 18:02:53.921 INFO    gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (214986 virtual)\n",
      "2020-11-17 18:02:53.931 INFO    gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (224481 virtual)\n",
      "2020-11-17 18:02:53.949 INFO    gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (228234 virtual)\n",
      "2020-11-17 18:02:54.077 INFO    gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (232614 virtual)\n",
      "2020-11-17 18:02:54.130 INFO    gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (237965 virtual)\n",
      "2020-11-17 18:02:54.179 INFO    gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (247105 virtual)\n",
      "2020-11-17 18:02:54.212 INFO    gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (253127 virtual)\n",
      "2020-11-17 18:02:54.283 INFO    gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (259984 virtual)\n",
      "2020-11-17 18:02:54.331 INFO    gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (264042 virtual)\n",
      "2020-11-17 18:02:54.362 INFO    gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (268316 virtual)\n",
      "2020-11-17 18:02:54.427 INFO    gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (273324 virtual)\n",
      "2020-11-17 18:02:54.507 INFO    gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (282431 virtual)\n",
      "2020-11-17 18:02:54.544 INFO    gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (288443 virtual)\n",
      "2020-11-17 18:02:54.624 INFO    gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (294877 virtual)\n",
      "2020-11-17 18:02:54.631 INFO    gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (300477 virtual)\n",
      "2020-11-17 18:02:54.675 INFO    gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (306007 virtual)\n",
      "2020-11-17 18:02:54.802 INFO    gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (311614 virtual)\n",
      "2020-11-17 18:02:54.820 INFO    gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (316822 virtual)\n",
      "2020-11-17 18:02:54.888 INFO    gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (322637 virtual)\n",
      "2020-11-17 18:02:54.963 INFO    gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (327638 virtual)\n",
      "2020-11-17 18:02:54.994 INFO    gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (332082 virtual)\n",
      "2020-11-17 18:02:55.045 INFO    gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (338934 virtual)\n",
      "2020-11-17 18:02:55.107 INFO    gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (344351 virtual)\n",
      "2020-11-17 18:02:55.142 INFO    gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (349864 virtual)\n",
      "2020-11-17 18:02:55.208 INFO    gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (357410 virtual)\n",
      "2020-11-17 18:02:55.249 INFO    gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (364324 virtual)\n",
      "2020-11-17 18:02:55.286 INFO    gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (372194 virtual)\n",
      "2020-11-17 18:02:55.390 INFO    gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (376210 virtual)\n",
      "2020-11-17 18:02:55.396 INFO    gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (383275 virtual)\n",
      "2020-11-17 18:02:55.436 INFO    gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (390056 virtual)\n",
      "2020-11-17 18:02:55.576 INFO    gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (395928 virtual)\n",
      "2020-11-17 18:02:55.585 INFO    gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (402629 virtual)\n",
      "2020-11-17 18:02:55.612 INFO    gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (406944 virtual)\n",
      "2020-11-17 18:02:55.719 INFO    gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (412155 virtual)\n",
      "2020-11-17 18:02:55.844 INFO    gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (419010 virtual)\n",
      "2020-11-17 18:02:55.855 INFO    gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (428107 virtual)\n",
      "2020-11-17 18:02:55.919 INFO    gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (434562 virtual)\n",
      "2020-11-17 18:02:55.990 INFO    gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (445492 virtual)\n",
      "2020-11-17 18:02:56.103 INFO    gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (450289 virtual)\n",
      "2020-11-17 18:02:56.112 INFO    gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (454405 virtual)\n",
      "2020-11-17 18:02:56.189 INFO    gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (459917 virtual)\n",
      "2020-11-17 18:02:56.302 INFO    gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (466992 virtual)\n",
      "2020-11-17 18:02:56.369 INFO    gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (471601 virtual)\n",
      "2020-11-17 18:02:56.443 INFO    gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (478310 virtual)\n",
      "2020-11-17 18:02:56.498 INFO    gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (483935 virtual)\n",
      "2020-11-17 18:02:56.509 INFO    gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (494020 virtual)\n",
      "2020-11-17 18:02:56.605 INFO    gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (504007 virtual)\n",
      "2020-11-17 18:02:56.654 INFO    gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (509024 virtual)\n",
      "2020-11-17 18:02:56.710 INFO    gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (515498 virtual)\n",
      "2020-11-17 18:02:56.789 INFO    gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (522604 virtual)\n",
      "2020-11-17 18:02:56.853 INFO    gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (528500 virtual)\n",
      "2020-11-17 18:02:57.077 INFO    gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (542494 virtual)\n",
      "2020-11-17 18:02:57.101 INFO    gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (547557 virtual)\n",
      "2020-11-17 18:02:57.128 INFO    gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (553036 virtual)\n",
      "2020-11-17 18:02:57.290 INFO    gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (559460 virtual)\n",
      "2020-11-17 18:02:57.307 INFO    gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (564398 virtual)\n",
      "2020-11-17 18:02:57.316 INFO    gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (568741 virtual)\n",
      "2020-11-17 18:02:57.470 INFO    gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (573251 virtual)\n",
      "2020-11-17 18:02:57.476 INFO    gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (578926 virtual)\n",
      "2020-11-17 18:02:57.607 INFO    gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (583693 virtual)\n",
      "2020-11-17 18:02:57.657 INFO    gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (589278 virtual)\n",
      "2020-11-17 18:02:57.700 INFO    gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (595613 virtual)\n",
      "2020-11-17 18:02:57.742 INFO    gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (600988 virtual)\n",
      "2020-11-17 18:02:57.800 INFO    gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (608586 virtual)\n",
      "2020-11-17 18:02:57.870 INFO    gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (613446 virtual)\n",
      "2020-11-17 18:02:57.901 INFO    gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (618611 virtual)\n",
      "2020-11-17 18:02:57.977 INFO    gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (623057 virtual)\n",
      "2020-11-17 18:02:58.073 INFO    gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (630128 virtual)\n",
      "2020-11-17 18:02:58.089 INFO    gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (640037 virtual)\n",
      "2020-11-17 18:02:58.227 INFO    gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (643962 virtual)\n",
      "2020-11-17 18:02:58.252 INFO    gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (652821 virtual)\n",
      "2020-11-17 18:02:58.261 INFO    gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (658339 virtual)\n",
      "2020-11-17 18:02:58.375 INFO    gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (664698 virtual)\n",
      "2020-11-17 18:02:58.447 INFO    gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (668053 virtual)\n",
      "2020-11-17 18:02:58.487 INFO    gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (673725 virtual)\n",
      "2020-11-17 18:02:58.509 INFO    gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (681683 virtual)\n",
      "2020-11-17 18:02:58.631 INFO    gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (686897 virtual)\n",
      "2020-11-17 18:02:58.656 INFO    gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (692617 virtual)\n",
      "2020-11-17 18:02:58.668 INFO    gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (697284 virtual)\n",
      "2020-11-17 18:02:58.751 INFO    gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (709685 virtual)\n",
      "2020-11-17 18:02:58.830 INFO    gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (714728 virtual)\n",
      "2020-11-17 18:02:58.883 INFO    gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (725079 virtual)\n",
      "2020-11-17 18:02:58.891 INFO    gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (729393 virtual)\n",
      "2020-11-17 18:02:59.007 INFO    gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (734213 virtual)\n",
      "2020-11-17 18:02:59.014 INFO    gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (741100 virtual)\n",
      "2020-11-17 18:02:59.170 INFO    gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (747148 virtual)\n",
      "2020-11-17 18:02:59.193 INFO    gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (751670 virtual)\n",
      "2020-11-17 18:02:59.287 INFO    gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (756736 virtual)\n",
      "2020-11-17 18:02:59.306 INFO    gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (766760 virtual)\n",
      "2020-11-17 18:02:59.342 INFO    gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (771373 virtual)\n",
      "2020-11-17 18:02:59.441 INFO    gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (776176 virtual)\n",
      "2020-11-17 18:02:59.468 INFO    gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (780453 virtual)\n",
      "2020-11-17 18:02:59.483 INFO    gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (785571 virtual)\n",
      "2020-11-17 18:02:59.571 INFO    gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (790928 virtual)\n",
      "2020-11-17 18:02:59.607 INFO    gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (796628 virtual)\n",
      "2020-11-17 18:02:59.709 INFO    gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (802373 virtual)\n",
      "2020-11-17 18:02:59.715 INFO    gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (807139 virtual)\n",
      "2020-11-17 18:02:59.744 INFO    gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (812512 virtual)\n",
      "2020-11-17 18:02:59.846 INFO    gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (817374 virtual)\n",
      "2020-11-17 18:02:59.853 INFO    gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (823758 virtual)\n",
      "2020-11-17 18:02:59.885 INFO    gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (830785 virtual)\n",
      "2020-11-17 18:02:59.969 INFO    gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (835564 virtual)\n",
      "2020-11-17 18:03:00.017 INFO    gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (839465 virtual)\n",
      "2020-11-17 18:03:00.073 INFO    gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (844984 virtual)\n",
      "2020-11-17 18:03:00.127 INFO    gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (853262 virtual)\n",
      "2020-11-17 18:03:00.201 INFO    gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (859454 virtual)\n",
      "2020-11-17 18:03:00.282 INFO    gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (866408 virtual)\n",
      "2020-11-17 18:03:00.306 INFO    gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (872469 virtual)\n",
      "2020-11-17 18:03:00.349 INFO    gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (879487 virtual)\n",
      "2020-11-17 18:03:00.431 INFO    gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (885082 virtual)\n",
      "2020-11-17 18:03:00.508 INFO    gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (889242 virtual)\n",
      "2020-11-17 18:03:00.519 INFO    gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (898179 virtual)\n",
      "2020-11-17 18:03:00.665 INFO    gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (902725 virtual)\n",
      "2020-11-17 18:03:00.774 INFO    gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (909429 virtual)\n",
      "2020-11-17 18:03:00.832 INFO    gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (917198 virtual)\n",
      "2020-11-17 18:03:00.904 INFO    gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (922266 virtual)\n",
      "2020-11-17 18:03:00.984 INFO    gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (930087 virtual)\n",
      "2020-11-17 18:03:01.107 INFO    gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (934840 virtual)\n",
      "2020-11-17 18:03:01.148 INFO    gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (946240 virtual)\n",
      "2020-11-17 18:03:01.235 INFO    gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (950495 virtual)\n",
      "2020-11-17 18:03:01.333 INFO    gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (956079 virtual)\n",
      "2020-11-17 18:03:01.394 INFO    gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (962441 virtual)\n",
      "2020-11-17 18:03:01.480 INFO    gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (966870 virtual)\n",
      "2020-11-17 18:03:01.502 INFO    gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (971264 virtual)\n",
      "2020-11-17 18:03:01.613 INFO    gensim.topic_coherence.text_analysis: 158 batches submitted to accumulate stats from 10112 documents (985145 virtual)\n",
      "2020-11-17 18:03:01.680 INFO    gensim.topic_coherence.text_analysis: 159 batches submitted to accumulate stats from 10176 documents (990735 virtual)\n",
      "2020-11-17 18:03:01.726 INFO    gensim.topic_coherence.text_analysis: 160 batches submitted to accumulate stats from 10240 documents (995995 virtual)\n",
      "2020-11-17 18:03:01.781 INFO    gensim.topic_coherence.text_analysis: 161 batches submitted to accumulate stats from 10304 documents (1000989 virtual)\n",
      "2020-11-17 18:03:01.821 INFO    gensim.topic_coherence.text_analysis: 162 batches submitted to accumulate stats from 10368 documents (1007619 virtual)\n",
      "2020-11-17 18:03:01.847 INFO    gensim.topic_coherence.text_analysis: 163 batches submitted to accumulate stats from 10432 documents (1014729 virtual)\n",
      "2020-11-17 18:03:01.985 INFO    gensim.topic_coherence.text_analysis: 164 batches submitted to accumulate stats from 10496 documents (1014859 virtual)\n",
      "2020-11-17 18:03:02.509 INFO    gensim.topic_coherence.text_analysis: 3 accumulators retrieved from output queue\n",
      "2020-11-17 18:03:02.565 INFO    gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 1015732 virtual documents\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.1365956953251772"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "npmi = CoherenceNPMI(texts=text_cleaned, topics=topics_bert)\n",
    "npmi.score()"
   ]
  },
  {
   "source": [
    "### 3.2.2 External Word Embeddings Topic Coherence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2020-11-17 18:19:56.796 INFO    gensim.models.utils_any2vec: loading projection weights from C:\\Users\\Pieter-Jan/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n",
      "2020-11-17 18:20:44.772 INFO    gensim.models.utils_any2vec: loaded (3000000, 300) matrix from C:\\Users\\Pieter-Jan/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "CoherenceWordEmbeddings(topics_bert).score()"
   ]
  },
  {
   "source": [
    "### 3.2.3 Rank-Biased Overlap "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9969902925952691"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "InvertedRBO(topics_bert).score()"
   ]
  }
 ]
}