{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"C:\\\\Users\\\\Pieter-Jan\\\\Documents\\\\Work\\\\Candriam\\\\nlp\\\\ESG\\\\topic_sentiment_analysis\")"
   ]
  },
  {
   "source": [
    "# Libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "sorflow\\core\\framework\\types_pb2.py:177: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.EnumValueDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:181: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.EnumValueDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:185: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.EnumValueDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:189: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.EnumValueDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:193: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.EnumValueDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:197: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.EnumValueDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:201: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.EnumValueDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:205: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.EnumValueDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:209: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.EnumValueDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:213: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.EnumValueDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:217: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.EnumValueDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _DATATYPE = _descriptor.EnumDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  DESCRIPTOR = _descriptor.FileDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:46: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:76: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:83: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:90: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:97: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:104: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:111: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:69: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _RESOURCEHANDLEPROTO = _descriptor.Descriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  DESCRIPTOR = _descriptor.FileDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:47: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:54: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:61: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:68: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:75: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:82: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:89: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:96: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:103: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:110: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:117: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:124: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:131: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:138: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:145: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:152: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _TENSORPROTO = _descriptor.Descriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:183: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:190: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:197: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:176: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _VARIANTTENSORDATAPROTO = _descriptor.Descriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  DESCRIPTOR = _descriptor.FileDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:47: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:54: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:61: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:68: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:75: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:82: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:89: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _ATTRVALUE_LISTVALUE = _descriptor.Descriptor(\n"
     ]
    }
   ],
   "source": [
    "# data manipulations\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "# data manipulations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy import sparse\n",
    "\n",
    "# CMT\n",
    "from contextualized_topic_models.models.ctm import CTM\n",
    "from contextualized_topic_models.datasets.dataset import CTMDataset\n",
    "from contextualized_topic_models.evaluation.measures import CoherenceNPMI, InvertedRBO, CoherenceWordEmbeddings\n",
    "from contextualized_topic_models.utils.preprocessing import SimplePreprocessing\n",
    "\n",
    "# lDA\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import LdaMulticore \n",
    "import pyLDAvis.gensim\n",
    "from gensim import corpora, matutils, models, similarities\n",
    "import pyLDAvis\n",
    "\n",
    "# BerTopic\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# other\n",
    "import random\n",
    "\n",
    "# own modules\n",
    "import modules.preprocessing as preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "source": [
    "# Goal\n",
    "\n",
    "We are going to compare the performance of three **unsupervised** models for topic modelling on ESG documents.\n",
    "\n",
    "1. Contextualized Topic Modelling (CTM): https://github.com/MilaNLProc/contextualized-topic-models\n",
    "2. Latent Dirichlet Allocation (LDA): https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "3. BERTopic: https://github.com/MaartenGr/BERTopic"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Evaluation measures \n",
    "\n",
    "1. **Normalized Point-wise Mutual Information (NPMI) (Lau et al.,\n",
    "2014)**\n",
    "\n",
    "It measures how much the top-10 words of a topic are related to each other, considering the empirical frequency of the words computed on the\n",
    "original corpus. τ is a symbolic metric and relies on co-occurrence.\n",
    "\n",
    "2. **External Word Embeddings Topic Coherence**\n",
    "\n",
    "As Ding et al. (2018) pointed out, though, topic\n",
    "coherence computed on the same data is inherently\n",
    "limited. Coherence computed on an external corpus, on the other hand, correlates much more to\n",
    "human judgment, but it may be expensive to estimate. Thus, our second metric is an external\n",
    "word embeddings topic coherence metric, which we compute by adopting a strategy similar to that\n",
    "described in Ding et al. (2018). First, we compute\n",
    "the average pairwise cosine similarity of the word\n",
    "embeddings of the top-10 words in a topic using (Mikolov et al., 2013) embeddings. Then, we\n",
    "compute the overall average of those values for all\n",
    "the topics (α).\n",
    "\n",
    "3. **rank-\n",
    "biased overlap (RBO) (Webber et al., 2010)**\n",
    "\n",
    "To evaluate how diverse the topics\n",
    "generated by a single model are, we use the rank-\n",
    "biased overlap (RBO) (Webber et al., 2010). RBO\n",
    "compares two topics of the same model. The key\n",
    "qualities of this measure are twofold: it allows\n",
    "disjointedness between the lists of topics (i.e., two\n",
    "topics can have different words in them) and it is\n",
    "weighted on the ranking (i.e., two lists that share\n",
    "some of the same words, albeit at different rankings,\n",
    "are penalized less than two lists that share the same\n",
    "words at the highest ranks). We deﬁne ρ as the rank-\n",
    "biased overlap diversity, that we interpret as the\n",
    "reciprocal of the standard RBO. ρ is 0 for identical\n",
    "topics and 1 for completely different topics. Both\n",
    "metrics are computed on the top-k ranked lists.\n",
    "Following the state-of-the-art, we consider k = 10."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# global variable used throughout te notebook to update preprocessing steps\n",
    "UPDATE = False\n",
    "DIR_REPORTS = 'C:/Users/Pieter-Jan/Documents/Work/Candriam/nlp/ESG/reports/Sectors/'"
   ]
  },
  {
   "source": [
    "## Download reports and perform some text processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "C:\\Users\\Pieter-Jan\\Anaconda3\\envs\\marketDb\\lib\\site-packages\\xlrd\\xlsx.py:266: DeprecationWarning: This method will be removed in future versions.  Use 'tree.iter()' or 'list(tree.iter())' instead.\n",
      "  for elem in self.tree.iter() if Element_has_iter else self.tree.getiterator():\n",
      "C:\\Users\\Pieter-Jan\\Anaconda3\\envs\\marketDb\\lib\\site-packages\\xlrd\\xlsx.py:312: DeprecationWarning: This method will be removed in future versions.  Use 'tree.iter()' or 'list(tree.iter())' instead.\n",
      "  for elem in self.tree.iter() if Element_has_iter else self.tree.getiterator():\n",
      "C:\\Users\\Pieter-Jan\\Anaconda3\\envs\\marketDb\\lib\\site-packages\\xlrd\\xlsx.py:266: DeprecationWarning: This method will be removed in future versions.  Use 'tree.iter()' or 'list(tree.iter())' instead.\n",
      "  for elem in self.tree.iter() if Element_has_iter else self.tree.getiterator():\n",
      "2020-11-18 22:27:12.588 INFO    numexpr.utils: NumExpr defaulting to 4 threads.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           company                 industry     sector  \\\n",
       "0  Brookfield Property Partners LP  Real Estate Development  Financial   \n",
       "1           PrairieSky Royalty Ltd  Real Estate Development  Financial   \n",
       "2           PrairieSky Royalty Ltd  Real Estate Development  Financial   \n",
       "3           PrairieSky Royalty Ltd  Real Estate Development  Financial   \n",
       "4     Summit Hotel Properties Inc.  Real Estate Development  Financial   \n",
       "\n",
       "     year                                                url  \n",
       "0  2019.0  https://www.brookfield.com/sites/default/files...  \n",
       "1  2017.0  https://www.prairiesky.com/wp-content/uploads/...  \n",
       "2  2018.0  https://www.prairiesky.com/wp-content/uploads/...  \n",
       "3  2019.0  https://www.prairiesky.com/wp-content/uploads/...  \n",
       "4  2018.0  https://www.responsibilityreports.com/HostedDa...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>company</th>\n      <th>industry</th>\n      <th>sector</th>\n      <th>year</th>\n      <th>url</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Brookfield Property Partners LP</td>\n      <td>Real Estate Development</td>\n      <td>Financial</td>\n      <td>2019.0</td>\n      <td>https://www.brookfield.com/sites/default/files...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>PrairieSky Royalty Ltd</td>\n      <td>Real Estate Development</td>\n      <td>Financial</td>\n      <td>2017.0</td>\n      <td>https://www.prairiesky.com/wp-content/uploads/...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PrairieSky Royalty Ltd</td>\n      <td>Real Estate Development</td>\n      <td>Financial</td>\n      <td>2018.0</td>\n      <td>https://www.prairiesky.com/wp-content/uploads/...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>PrairieSky Royalty Ltd</td>\n      <td>Real Estate Development</td>\n      <td>Financial</td>\n      <td>2019.0</td>\n      <td>https://www.prairiesky.com/wp-content/uploads/...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Summit Hotel Properties Inc.</td>\n      <td>Real Estate Development</td>\n      <td>Financial</td>\n      <td>2018.0</td>\n      <td>https://www.responsibilityreports.com/HostedDa...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df = pd.read_excel(\"data//Industry_CSR_Datasets.xlsx\").dropna().reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "100%|██████████| 168/168 [05:32<00:00,  1.98s/it]\n"
     ]
    }
   ],
   "source": [
    "# download reports and store\n",
    "df_download, could_not_download = preprocess.download_reports(df=df, directory=DIR_REPORTS, update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(168, 6)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "df_download.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# onlyfiles = [f for f in os.listdir(DIR_REPORTS) if os.path.isfile(os.path.join(DIR_REPORTS, f))]\n",
    "# df = df[~df['company'].isin(could_not_download)]\n",
    "# df[\"filename\"] = onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO    pdfminer.pdfinterp: Processing xobj: <PDFStream(121): raw=28316, {'BitsPerComponent': 8, 'ColorSpace': <PDFObjRef:200>, 'Filter': /'DCTDecode', 'Height': 343, 'Intent': /'RelativeColorimetric', 'Length': 28314, 'Metadata': <PDFObjRef:120>, 'Name': /'X', 'Subtype': /'Image', 'Type': /'XObject', 'Width': 259}>\n",
      "2020-11-18 23:06:39.424 INFO    pdfminer.pdfpage: Page: {'ArtBox': [0.0, 0.0, 612.0, 792.0], 'BleedBox': [0.0, 0.0, 612.0, 792.0], 'Contents': <PDFObjRef:123>, 'CropBox': [0.0, 0.0, 612.0, 792.0], 'MediaBox': [0.0, 0.0, 612.0, 792.0], 'Parent': <PDFObjRef:192>, 'PieceInfo': {'InDesign': {'DocumentID': b'\\xfe\\xff\\x00x\\x00m\\x00p\\x00.\\x00d\\x00i\\x00d\\x00:\\x006\\x009\\x000\\x00c\\x00a\\x00d\\x002\\x004\\x00-\\x001\\x00e\\x00d\\x002\\x00-\\x006\\x00b\\x004\\x00f\\x00-\\x00a\\x005\\x008\\x00d\\x00-\\x003\\x00e\\x004\\x001\\x00f\\x000\\x00c\\x004\\x00f\\x004\\x00d\\x001', 'LastModified': b'\\xfe\\xff\\x00D\\x00:\\x002\\x000\\x002\\x000\\x000\\x005\\x002\\x007\\x001\\x007\\x000\\x002\\x002\\x008\\x00Z', 'NumberofPages': 1, 'OriginalDocumentID': b'\\xfe\\xff\\x00x\\x00m\\x00p\\x00.\\x00d\\x00i\\x00d\\x00:\\x005\\x00e\\x00e\\x00e\\x005\\x00a\\x000\\x00d\\x00-\\x008\\x00f\\x008\\x00f\\x00-\\x004\\x000\\x004\\x008\\x00-\\x00a\\x00d\\x00f\\x006\\x00-\\x002\\x009\\x00f\\x004\\x001\\x00d\\x001\\x006\\x00e\\x000\\x009\\x001', 'PageUIDList': {'0': 7962}, 'PageWidthList': {'0': 612.0}}}, 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}, 'Font': {'T1_0': <PDFObjRef:199>, 'T1_1': <PDFObjRef:163>, 'T1_2': <PDFObjRef:208>, 'T1_3': <PDFObjRef:152>, 'T1_4': <PDFObjRef:207>}, 'ProcSet': [/'PDF', /'Text', /'ImageC'], 'XObject': {'Fm0': <PDFObjRef:218>, 'Im0': <PDFObjRef:125>, 'Im1': <PDFObjRef:127>, 'Im2': <PDFObjRef:129>, 'Im3': <PDFObjRef:131>}}, 'Rotate': 0, 'Tabs': /'W', 'Thumb': <PDFObjRef:184>, 'TrimBox': [0.0, 0.0, 612.0, 792.0], 'Type': /'Page'}\n",
      "2020-11-18 23:06:39.426 INFO    pdfminer.pdfinterp: Processing page: <PDFPage: Resources={'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}, 'Font': {'T1_0': <PDFObjRef:199>, 'T1_1': <PDFObjRef:163>, 'T1_2': <PDFObjRef:208>, 'T1_3': <PDFObjRef:152>, 'T1_4': <PDFObjRef:207>}, 'ProcSet': [/'PDF', /'Text', /'ImageC'], 'XObject': {'Fm0': <PDFObjRef:218>, 'Im0': <PDFObjRef:125>, 'Im1': <PDFObjRef:127>, 'Im2': <PDFObjRef:129>, 'Im3': <PDFObjRef:131>}}, MediaBox=[0.0, 0.0, 612.0, 792.0]>\n",
      "2020-11-18 23:06:39.430 INFO    pdfminer.pdfinterp: render_contents: resources={'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}, 'Font': {'T1_0': <PDFObjRef:199>, 'T1_1': <PDFObjRef:163>, 'T1_2': <PDFObjRef:208>, 'T1_3': <PDFObjRef:152>, 'T1_4': <PDFObjRef:207>}, 'ProcSet': [/'PDF', /'Text', /'ImageC'], 'XObject': {'Fm0': <PDFObjRef:218>, 'Im0': <PDFObjRef:125>, 'Im1': <PDFObjRef:127>, 'Im2': <PDFObjRef:129>, 'Im3': <PDFObjRef:131>}}, streams=[<PDFStream(123): raw=3497, {'Filter': /'FlateDecode', 'Length': 3495}>], ctm=(1, 0, 0, 1, -0.0, -0.0)\n",
      "2020-11-18 23:06:39.433 INFO    pdfminer.pdfinterp: Processing xobj: <PDFStream(218): len=337, {'BBox': [0.0, 792.0, 612.0, 0.0], 'Filter': /'FlateDecode', 'Length': 220, 'Matrix': [1.0, 0.0, 0.0, 1.0, 0.0, 0.0], 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}, 'Font': {'T1_0': <PDFObjRef:224>}, 'ProcSet': [/'PDF', /'Text']}, 'Subtype': /'Form'}>\n",
      "2020-11-18 23:06:39.436 INFO    pdfminer.pdfinterp: render_contents: resources={'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}, 'Font': {'T1_0': <PDFObjRef:224>}, 'ProcSet': [/'PDF', /'Text']}, streams=[<PDFStream(218): len=337, {'BBox': [0.0, 792.0, 612.0, 0.0], 'Filter': /'FlateDecode', 'Length': 220, 'Matrix': [1.0, 0.0, 0.0, 1.0, 0.0, 0.0], 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}, 'Font': {'T1_0': <PDFObjRef:224>}, 'ProcSet': [/'PDF', /'Text']}, 'Subtype': /'Form'}>], ctm=(1.0, 0.0, 0.0, 1.0, 0.0, 0.0)\n",
      "2020-11-18 23:06:39.512 INFO    pdfminer.pdfinterp: Processing xobj: <PDFStream(125): raw=22463, {'BitsPerComponent': 8, 'ColorSpace': <PDFObjRef:200>, 'Filter': /'DCTDecode', 'Height': 290, 'Intent': /'RelativeColorimetric', 'Length': 22461, 'Metadata': <PDFObjRef:124>, 'Name': /'X', 'Subtype': /'Image', 'Type': /'XObject', 'Width': 375}>\n",
      "2020-11-18 23:06:39.517 INFO    pdfminer.pdfinterp: Processing xobj: <PDFStream(127): raw=21060, {'BitsPerComponent': 8, 'ColorSpace': <PDFObjRef:200>, 'Filter': /'DCTDecode', 'Height': 280, 'Intent': /'RelativeColorimetric', 'Length': 21058, 'Metadata': <PDFObjRef:126>, 'Name': /'X', 'Subtype': /'Image', 'Type': /'XObject', 'Width': 375}>\n",
      "2020-11-18 23:06:39.524 INFO    pdfminer.pdfinterp: Processing xobj: <PDFStream(129): raw=27347, {'BitsPerComponent': 8, 'ColorSpace': <PDFObjRef:200>, 'Filter': /'DCTDecode', 'Height': 288, 'Intent': /'RelativeColorimetric', 'Length': 27345, 'Metadata': <PDFObjRef:128>, 'Name': /'X', 'Subtype': /'Image', 'Type': /'XObject', 'Width': 388}>\n",
      "2020-11-18 23:06:39.539 INFO    pdfminer.pdfinterp: Processing xobj: <PDFStream(131): raw=30795, {'BitsPerComponent': 8, 'ColorSpace': <PDFObjRef:200>, 'Filter': /'DCTDecode', 'Height': 283, 'Intent': /'RelativeColorimetric', 'Length': 30793, 'Metadata': <PDFObjRef:130>, 'Name': /'X', 'Subtype': /'Image', 'Type': /'XObject', 'Width': 387}>\n",
      "2020-11-18 23:06:39.582 INFO    pdfminer.pdfpage: Page: {'ArtBox': [0.0, 0.0, 612.0, 792.0], 'BleedBox': [0.0, 0.0, 612.0, 792.0], 'Contents': <PDFObjRef:133>, 'CropBox': [0.0, 0.0, 612.0, 792.0], 'Group': <PDFObjRef:138>, 'MediaBox': [0.0, 0.0, 612.0, 792.0], 'Parent': <PDFObjRef:192>, 'PieceInfo': {'InDesign': {'DocumentID': b'\\xfe\\xff\\x00x\\x00m\\x00p\\x00.\\x00d\\x00i\\x00d\\x00:\\x006\\x009\\x000\\x00c\\x00a\\x00d\\x002\\x004\\x00-\\x001\\x00e\\x00d\\x002\\x00-\\x006\\x00b\\x004\\x00f\\x00-\\x00a\\x005\\x008\\x00d\\x00-\\x003\\x00e\\x004\\x001\\x00f\\x000\\x00c\\x004\\x00f\\x004\\x00d\\x001', 'LastModified': b'\\xfe\\xff\\x00D\\x00:\\x002\\x000\\x002\\x000\\x000\\x005\\x002\\x007\\x001\\x007\\x000\\x002\\x003\\x006\\x00Z', 'NumberofPages': 1, 'OriginalDocumentID': b'\\xfe\\xff\\x00x\\x00m\\x00p\\x00.\\x00d\\x00i\\x00d\\x00:\\x005\\x00e\\x00e\\x00e\\x005\\x00a\\x000\\x00d\\x00-\\x008\\x00f\\x008\\x00f\\x00-\\x004\\x000\\x004\\x008\\x00-\\x00a\\x00d\\x00f\\x006\\x00-\\x002\\x009\\x00f\\x004\\x001\\x00d\\x001\\x006\\x00e\\x000\\x009\\x001', 'PageUIDList': {'0': 24740}, 'PageWidthList': {'0': 612.0}}}, 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>, 'GS1': <PDFObjRef:217>}, 'Font': {'T1_0': <PDFObjRef:199>, 'T1_1': <PDFObjRef:209>}, 'ProcSet': [/'PDF', /'Text', /'ImageC'], 'XObject': {'Fm0': <PDFObjRef:218>, 'Fm1': <PDFObjRef:135>, 'Im0': <PDFObjRef:137>}}, 'Rotate': 0, 'Tabs': /'W', 'Thumb': <PDFObjRef:185>, 'TrimBox': [0.0, 0.0, 612.0, 792.0], 'Type': /'Page'}\n",
      "2020-11-18 23:06:39.584 INFO    pdfminer.pdfinterp: Processing page: <PDFPage: Resources={'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>, 'GS1': <PDFObjRef:217>}, 'Font': {'T1_0': <PDFObjRef:199>, 'T1_1': <PDFObjRef:209>}, 'ProcSet': [/'PDF', /'Text', /'ImageC'], 'XObject': {'Fm0': <PDFObjRef:218>, 'Fm1': <PDFObjRef:135>, 'Im0': <PDFObjRef:137>}}, MediaBox=[0.0, 0.0, 612.0, 792.0]>\n",
      "2020-11-18 23:06:39.589 INFO    pdfminer.pdfinterp: render_contents: resources={'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>, 'GS1': <PDFObjRef:217>}, 'Font': {'T1_0': <PDFObjRef:199>, 'T1_1': <PDFObjRef:209>}, 'ProcSet': [/'PDF', /'Text', /'ImageC'], 'XObject': {'Fm0': <PDFObjRef:218>, 'Fm1': <PDFObjRef:135>, 'Im0': <PDFObjRef:137>}}, streams=[<PDFStream(133): raw=2169, {'Filter': /'FlateDecode', 'Length': 2167}>], ctm=(1, 0, 0, 1, -0.0, -0.0)\n",
      "2020-11-18 23:06:39.593 INFO    pdfminer.pdfinterp: Processing xobj: <PDFStream(218): len=337, {'BBox': [0.0, 792.0, 612.0, 0.0], 'Filter': /'FlateDecode', 'Length': 220, 'Matrix': [1.0, 0.0, 0.0, 1.0, 0.0, 0.0], 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}, 'Font': {'T1_0': <PDFObjRef:224>}, 'ProcSet': [/'PDF', /'Text']}, 'Subtype': /'Form'}>\n",
      "2020-11-18 23:06:39.595 INFO    pdfminer.pdfinterp: render_contents: resources={'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}, 'Font': {'T1_0': <PDFObjRef:224>}, 'ProcSet': [/'PDF', /'Text']}, streams=[<PDFStream(218): len=337, {'BBox': [0.0, 792.0, 612.0, 0.0], 'Filter': /'FlateDecode', 'Length': 220, 'Matrix': [1.0, 0.0, 0.0, 1.0, 0.0, 0.0], 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}, 'Font': {'T1_0': <PDFObjRef:224>}, 'ProcSet': [/'PDF', /'Text']}, 'Subtype': /'Form'}>], ctm=(1.0, 0.0, 0.0, 1.0, 0.0, 0.0)\n",
      "2020-11-18 23:06:39.618 INFO    pdfminer.pdfinterp: Processing xobj: <PDFStream(137): raw=163233, {'BitsPerComponent': 8, 'ColorSpace': <PDFObjRef:200>, 'Filter': /'DCTDecode', 'Height': 1526, 'Intent': /'RelativeColorimetric', 'Length': 163231, 'Metadata': <PDFObjRef:136>, 'Name': /'X', 'Subtype': /'Image', 'Type': /'XObject', 'Width': 1180}>\n",
      "2020-11-18 23:06:39.623 INFO    pdfminer.pdfinterp: Processing xobj: <PDFStream(135): raw=68, {'BBox': [221.4, 808.0, 630.0, -19.0], 'Filter': /'FlateDecode', 'Group': <PDFObjRef:134>, 'Length': 66, 'Matrix': [1.0, 0.0, 0.0, 1.0, 0.0, 0.0], 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}}, 'Subtype': /'Form'}>\n",
      "2020-11-18 23:06:39.625 INFO    pdfminer.pdfinterp: render_contents: resources={'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}}, streams=[<PDFStream(135): raw=68, {'BBox': [221.4, 808.0, 630.0, -19.0], 'Filter': /'FlateDecode', 'Group': <PDFObjRef:134>, 'Length': 66, 'Matrix': [1.0, 0.0, 0.0, 1.0, 0.0, 0.0], 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}}, 'Subtype': /'Form'}>], ctm=(1.0, 0.0, 0.0, 1.0, 0.0, 0.0)\n",
      "2020-11-18 23:06:39.650 INFO    pdfminer.pdfpage: Page: {'ArtBox': [0.0, 0.0, 612.0, 792.0], 'BleedBox': [0.0, 0.0, 612.0, 792.0], 'Contents': <PDFObjRef:140>, 'CropBox': [0.0, 0.0, 612.0, 792.0], 'MediaBox': [0.0, 0.0, 612.0, 792.0], 'Parent': <PDFObjRef:192>, 'PieceInfo': {'InDesign': {'DocumentID': b'\\xfe\\xff\\x00x\\x00m\\x00p\\x00.\\x00d\\x00i\\x00d\\x00:\\x006\\x009\\x000\\x00c\\x00a\\x00d\\x002\\x004\\x00-\\x001\\x00e\\x00d\\x002\\x00-\\x006\\x00b\\x004\\x00f\\x00-\\x00a\\x005\\x008\\x00d\\x00-\\x003\\x00e\\x004\\x001\\x00f\\x000\\x00c\\x004\\x00f\\x004\\x00d\\x001', 'LastModified': b'\\xfe\\xff\\x00D\\x00:\\x002\\x000\\x002\\x000\\x000\\x005\\x002\\x007\\x001\\x007\\x000\\x002\\x003\\x009\\x00Z', 'NumberofPages': 1, 'OriginalDocumentID': b'\\xfe\\xff\\x00x\\x00m\\x00p\\x00.\\x00d\\x00i\\x00d\\x00:\\x005\\x00e\\x00e\\x00e\\x005\\x00a\\x000\\x00d\\x00-\\x008\\x00f\\x008\\x00f\\x00-\\x004\\x000\\x004\\x008\\x00-\\x00a\\x00d\\x00f\\x006\\x00-\\x002\\x009\\x00f\\x004\\x001\\x00d\\x001\\x006\\x00e\\x000\\x009\\x001', 'PageUIDList': {'0': 24877}, 'PageWidthList': {'0': 612.0}}}, 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}, 'Font': {'T1_0': <PDFObjRef:199>, 'T1_1': <PDFObjRef:163>, 'T1_2': <PDFObjRef:208>, 'T1_3': <PDFObjRef:209>}, 'ProcSet': [/'PDF', /'Text'], 'XObject': {'Fm0': <PDFObjRef:218>}}, 'Rotate': 0, 'Tabs': /'W', 'Thumb': <PDFObjRef:186>, 'TrimBox': [0.0, 0.0, 612.0, 792.0], 'Type': /'Page'}\n",
      "2020-11-18 23:06:39.652 INFO    pdfminer.pdfinterp: Processing page: <PDFPage: Resources={'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}, 'Font': {'T1_0': <PDFObjRef:199>, 'T1_1': <PDFObjRef:163>, 'T1_2': <PDFObjRef:208>, 'T1_3': <PDFObjRef:209>}, 'ProcSet': [/'PDF', /'Text'], 'XObject': {'Fm0': <PDFObjRef:218>}}, MediaBox=[0.0, 0.0, 612.0, 792.0]>\n",
      "2020-11-18 23:06:39.655 INFO    pdfminer.pdfinterp: render_contents: resources={'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}, 'Font': {'T1_0': <PDFObjRef:199>, 'T1_1': <PDFObjRef:163>, 'T1_2': <PDFObjRef:208>, 'T1_3': <PDFObjRef:209>}, 'ProcSet': [/'PDF', /'Text'], 'XObject': {'Fm0': <PDFObjRef:218>}}, streams=[<PDFStream(140): raw=2161, {'Filter': /'FlateDecode', 'Length': 2159}>], ctm=(1, 0, 0, 1, -0.0, -0.0)\n",
      "2020-11-18 23:06:39.657 INFO    pdfminer.pdfinterp: Processing xobj: <PDFStream(218): len=337, {'BBox': [0.0, 792.0, 612.0, 0.0], 'Filter': /'FlateDecode', 'Length': 220, 'Matrix': [1.0, 0.0, 0.0, 1.0, 0.0, 0.0], 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}, 'Font': {'T1_0': <PDFObjRef:224>}, 'ProcSet': [/'PDF', /'Text']}, 'Subtype': /'Form'}>\n",
      "2020-11-18 23:06:39.659 INFO    pdfminer.pdfinterp: render_contents: resources={'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}, 'Font': {'T1_0': <PDFObjRef:224>}, 'ProcSet': [/'PDF', /'Text']}, streams=[<PDFStream(218): len=337, {'BBox': [0.0, 792.0, 612.0, 0.0], 'Filter': /'FlateDecode', 'Length': 220, 'Matrix': [1.0, 0.0, 0.0, 1.0, 0.0, 0.0], 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}, 'Font': {'T1_0': <PDFObjRef:224>}, 'ProcSet': [/'PDF', /'Text']}, 'Subtype': /'Form'}>], ctm=(1.0, 0.0, 0.0, 1.0, 0.0, 0.0)\n",
      "2020-11-18 23:06:39.758 INFO    pdfminer.pdfpage: Page: {'ArtBox': [0.0, 0.0, 612.0, 792.0], 'BleedBox': [0.0, 0.0, 612.0, 792.0], 'Contents': <PDFObjRef:142>, 'CropBox': [0.0, 0.0, 612.0, 792.0], 'Group': <PDFObjRef:147>, 'MediaBox': [0.0, 0.0, 612.0, 792.0], 'Parent': <PDFObjRef:192>, 'PieceInfo': {'InDesign': {'DocumentID': b'\\xfe\\xff\\x00x\\x00m\\x00p\\x00.\\x00d\\x00i\\x00d\\x00:\\x006\\x009\\x000\\x00c\\x00a\\x00d\\x002\\x004\\x00-\\x001\\x00e\\x00d\\x002\\x00-\\x006\\x00b\\x004\\x00f\\x00-\\x00a\\x005\\x008\\x00d\\x00-\\x003\\x00e\\x004\\x001\\x00f\\x000\\x00c\\x004\\x00f\\x004\\x00d\\x001', 'LastModified': b'\\xfe\\xff\\x00D\\x00:\\x002\\x000\\x002\\x000\\x000\\x005\\x002\\x007\\x001\\x007\\x000\\x002\\x003\\x009\\x00Z', 'NumberofPages': 1, 'OriginalDocumentID': b'\\xfe\\xff\\x00x\\x00m\\x00p\\x00.\\x00d\\x00i\\x00d\\x00:\\x005\\x00e\\x00e\\x00e\\x005\\x00a\\x000\\x00d\\x00-\\x008\\x00f\\x008\\x00f\\x00-\\x004\\x000\\x004\\x008\\x00-\\x00a\\x00d\\x00f\\x006\\x00-\\x002\\x009\\x00f\\x004\\x001\\x00d\\x001\\x006\\x00e\\x000\\x009\\x001', 'PageUIDList': {'0': 14438}, 'PageWidthList': {'0': 612.0}}}, 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}, 'Font': {'T1_0': <PDFObjRef:199>, 'T1_1': <PDFObjRef:163>, 'T1_2': <PDFObjRef:208>}, 'ProcSet': [/'PDF', /'Text', /'ImageC'], 'XObject': {'Fm0': <PDFObjRef:218>, 'Im0': <PDFObjRef:144>, 'Im1': <PDFObjRef:146>}}, 'Rotate': 0, 'Tabs': /'W', 'Thumb': <PDFObjRef:187>, 'TrimBox': [0.0, 0.0, 612.0, 792.0], 'Type': /'Page'}\n",
      "2020-11-18 23:06:39.760 INFO    pdfminer.pdfinterp: Processing page: <PDFPage: Resources={'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}, 'Font': {'T1_0': <PDFObjRef:199>, 'T1_1': <PDFObjRef:163>, 'T1_2': <PDFObjRef:208>}, 'ProcSet': [/'PDF', /'Text', /'ImageC'], 'XObject': {'Fm0': <PDFObjRef:218>, 'Im0': <PDFObjRef:144>, 'Im1': <PDFObjRef:146>}}, MediaBox=[0.0, 0.0, 612.0, 792.0]>\n",
      "2020-11-18 23:06:39.763 INFO    pdfminer.pdfinterp: render_contents: resources={'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}, 'Font': {'T1_0': <PDFObjRef:199>, 'T1_1': <PDFObjRef:163>, 'T1_2': <PDFObjRef:208>}, 'ProcSet': [/'PDF', /'Text', /'ImageC'], 'XObject': {'Fm0': <PDFObjRef:218>, 'Im0': <PDFObjRef:144>, 'Im1': <PDFObjRef:146>}}, streams=[<PDFStream(142): raw=1893, {'Filter': /'FlateDecode', 'Length': 1891}>], ctm=(1, 0, 0, 1, -0.0, -0.0)\n",
      "2020-11-18 23:06:39.765 INFO    pdfminer.pdfinterp: Processing xobj: <PDFStream(218): len=337, {'BBox': [0.0, 792.0, 612.0, 0.0], 'Filter': /'FlateDecode', 'Length': 220, 'Matrix': [1.0, 0.0, 0.0, 1.0, 0.0, 0.0], 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}, 'Font': {'T1_0': <PDFObjRef:224>}, 'ProcSet': [/'PDF', /'Text']}, 'Subtype': /'Form'}>\n",
      "2020-11-18 23:06:39.766 INFO    pdfminer.pdfinterp: render_contents: resources={'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}, 'Font': {'T1_0': <PDFObjRef:224>}, 'ProcSet': [/'PDF', /'Text']}, streams=[<PDFStream(218): len=337, {'BBox': [0.0, 792.0, 612.0, 0.0], 'Filter': /'FlateDecode', 'Length': 220, 'Matrix': [1.0, 0.0, 0.0, 1.0, 0.0, 0.0], 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}, 'Font': {'T1_0': <PDFObjRef:224>}, 'ProcSet': [/'PDF', /'Text']}, 'Subtype': /'Form'}>], ctm=(1.0, 0.0, 0.0, 1.0, 0.0, 0.0)\n",
      "2020-11-18 23:06:39.809 INFO    pdfminer.pdfinterp: Processing xobj: <PDFStream(144): raw=55493, {'BitsPerComponent': 8, 'ColorSpace': <PDFObjRef:200>, 'Filter': /'DCTDecode', 'Height': 464, 'Intent': /'RelativeColorimetric', 'Length': 55491, 'Metadata': <PDFObjRef:143>, 'Name': /'X', 'Subtype': /'Image', 'Type': /'XObject', 'Width': 770}>\n",
      "2020-11-18 23:06:39.812 INFO    pdfminer.pdfinterp: Processing xobj: <PDFStream(146): raw=11677, {'BitsPerComponent': 8, 'ColorSpace': <PDFObjRef:200>, 'Filter': /'DCTDecode', 'Height': 117, 'Intent': /'RelativeColorimetric', 'Length': 11675, 'Name': /'X', 'SMask': <PDFObjRef:145>, 'Subtype': /'Image', 'Type': /'XObject', 'Width': 330}>\n",
      "2020-11-18 23:06:39.848 INFO    pdfminer.pdfpage: Page: {'ArtBox': [0.0, 0.0, 612.0, 792.0], 'BleedBox': [0.0, 0.0, 612.0, 792.0], 'Contents': <PDFObjRef:149>, 'CropBox': [0.0, 0.0, 612.0, 792.0], 'MediaBox': [0.0, 0.0, 612.0, 792.0], 'Parent': <PDFObjRef:192>, 'PieceInfo': {'InDesign': {'DocumentID': b'\\xfe\\xff\\x00x\\x00m\\x00p\\x00.\\x00d\\x00i\\x00d\\x00:\\x006\\x009\\x000\\x00c\\x00a\\x00d\\x002\\x004\\x00-\\x001\\x00e\\x00d\\x002\\x00-\\x006\\x00b\\x004\\x00f\\x00-\\x00a\\x005\\x008\\x00d\\x00-\\x003\\x00e\\x004\\x001\\x00f\\x000\\x00c\\x004\\x00f\\x004\\x00d\\x001', 'LastModified': b'\\xfe\\xff\\x00D\\x00:\\x002\\x000\\x002\\x000\\x000\\x005\\x002\\x007\\x001\\x007\\x000\\x002\\x004\\x003\\x00Z', 'NumberofPages': 1, 'OriginalDocumentID': b'\\xfe\\xff\\x00x\\x00m\\x00p\\x00.\\x00d\\x00i\\x00d\\x00:\\x005\\x00e\\x00e\\x00e\\x005\\x00a\\x000\\x00d\\x00-\\x008\\x00f\\x008\\x00f\\x00-\\x004\\x000\\x004\\x008\\x00-\\x00a\\x00d\\x00f\\x006\\x00-\\x002\\x009\\x00f\\x004\\x001\\x00d\\x001\\x006\\x00e\\x000\\x009\\x001', 'PageUIDList': {'0': 17974}, 'PageWidthList': {'0': 612.0}}}, 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}, 'Font': {'T1_0': <PDFObjRef:199>}, 'ProcSet': [/'PDF', /'Text'], 'XObject': {'Fm0': <PDFObjRef:218>}}, 'Rotate': 0, 'Tabs': /'W', 'Thumb': <PDFObjRef:188>, 'TrimBox': [0.0, 0.0, 612.0, 792.0], 'Type': /'Page'}\n",
      "2020-11-18 23:06:39.851 INFO    pdfminer.pdfinterp: Processing page: <PDFPage: Resources={'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}, 'Font': {'T1_0': <PDFObjRef:199>}, 'ProcSet': [/'PDF', /'Text'], 'XObject': {'Fm0': <PDFObjRef:218>}}, MediaBox=[0.0, 0.0, 612.0, 792.0]>\n",
      "2020-11-18 23:06:39.856 INFO    pdfminer.pdfinterp: render_contents: resources={'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}, 'Font': {'T1_0': <PDFObjRef:199>}, 'ProcSet': [/'PDF', /'Text'], 'XObject': {'Fm0': <PDFObjRef:218>}}, streams=[<PDFStream(149): raw=4678, {'Filter': /'FlateDecode', 'Length': 4676}>], ctm=(1, 0, 0, 1, -0.0, -0.0)\n",
      "2020-11-18 23:06:39.859 INFO    pdfminer.pdfinterp: Processing xobj: <PDFStream(218): len=337, {'BBox': [0.0, 792.0, 612.0, 0.0], 'Filter': /'FlateDecode', 'Length': 220, 'Matrix': [1.0, 0.0, 0.0, 1.0, 0.0, 0.0], 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}, 'Font': {'T1_0': <PDFObjRef:224>}, 'ProcSet': [/'PDF', /'Text']}, 'Subtype': /'Form'}>\n",
      "2020-11-18 23:06:39.860 INFO    pdfminer.pdfinterp: render_contents: resources={'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}, 'Font': {'T1_0': <PDFObjRef:224>}, 'ProcSet': [/'PDF', /'Text']}, streams=[<PDFStream(218): len=337, {'BBox': [0.0, 792.0, 612.0, 0.0], 'Filter': /'FlateDecode', 'Length': 220, 'Matrix': [1.0, 0.0, 0.0, 1.0, 0.0, 0.0], 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:200>}, 'ExtGState': {'GS0': <PDFObjRef:201>}, 'Font': {'T1_0': <PDFObjRef:224>}, 'ProcSet': [/'PDF', /'Text']}, 'Subtype': /'Form'}>], ctm=(1.0, 0.0, 0.0, 1.0, 0.0, 0.0)\n",
      "100%|██████████| 168/168 [33:52<00:00, 12.10s/it]\n",
      "100%|██████████| 162/162 [00:18<00:00,  8.82it/s]\n"
     ]
    }
   ],
   "source": [
    "df_processed = preprocess.load_processed_text(\n",
    "    df_download,\n",
    "    dir_read_pdf=DIR_REPORTS, \n",
    "    columns_to_keep = [\"company\",\"industry\", \"sector\", \"year\", \"url\", \"filename\"],\n",
    "    file_processed_text=\"output/CRS_processed_pdfMiner.txt\",\n",
    "    n_min_word_paragraph=50, \n",
    "    n_max_word_paragraph=125,  \n",
    "    update=True,\n",
    "    method_extract_content = \"pdfMiner\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "(20532, 7)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           company                 industry     sector  year  \\\n",
       "0  Brookfield Property Partners LP  Real Estate Development  Financial  2019   \n",
       "1  Brookfield Property Partners LP  Real Estate Development  Financial  2019   \n",
       "2  Brookfield Property Partners LP  Real Estate Development  Financial  2019   \n",
       "3  Brookfield Property Partners LP  Real Estate Development  Financial  2019   \n",
       "4  Brookfield Property Partners LP  Real Estate Development  Financial  2019   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.brookfield.com/sites/default/files...   \n",
       "1  https://www.brookfield.com/sites/default/files...   \n",
       "2  https://www.brookfield.com/sites/default/files...   \n",
       "3  https://www.brookfield.com/sites/default/files...   \n",
       "4  https://www.brookfield.com/sites/default/files...   \n",
       "\n",
       "                                filename  \\\n",
       "0  BrookfieldPropertyPartnersLP-2019.pdf   \n",
       "1  BrookfieldPropertyPartnersLP-2019.pdf   \n",
       "2  BrookfieldPropertyPartnersLP-2019.pdf   \n",
       "3  BrookfieldPropertyPartnersLP-2019.pdf   \n",
       "4  BrookfieldPropertyPartnersLP-2019.pdf   \n",
       "\n",
       "                                           paragraph  \n",
       "0  ESG Report ESG REPORT Contents OUR EOPLE Human...  \n",
       "1  ESG Integration into Our Investment Process Sy...  \n",
       "2  private wealth investors For more information ...  \n",
       "3  ESTAT INF RASTRUCTURE RENEWABLE POWER PRIVATE ...  \n",
       "4  Today our capacity is approximately MW of elec...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>company</th>\n      <th>industry</th>\n      <th>sector</th>\n      <th>year</th>\n      <th>url</th>\n      <th>filename</th>\n      <th>paragraph</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Brookfield Property Partners LP</td>\n      <td>Real Estate Development</td>\n      <td>Financial</td>\n      <td>2019</td>\n      <td>https://www.brookfield.com/sites/default/files...</td>\n      <td>BrookfieldPropertyPartnersLP-2019.pdf</td>\n      <td>ESG Report ESG REPORT Contents OUR EOPLE Human...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Brookfield Property Partners LP</td>\n      <td>Real Estate Development</td>\n      <td>Financial</td>\n      <td>2019</td>\n      <td>https://www.brookfield.com/sites/default/files...</td>\n      <td>BrookfieldPropertyPartnersLP-2019.pdf</td>\n      <td>ESG Integration into Our Investment Process Sy...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Brookfield Property Partners LP</td>\n      <td>Real Estate Development</td>\n      <td>Financial</td>\n      <td>2019</td>\n      <td>https://www.brookfield.com/sites/default/files...</td>\n      <td>BrookfieldPropertyPartnersLP-2019.pdf</td>\n      <td>private wealth investors For more information ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Brookfield Property Partners LP</td>\n      <td>Real Estate Development</td>\n      <td>Financial</td>\n      <td>2019</td>\n      <td>https://www.brookfield.com/sites/default/files...</td>\n      <td>BrookfieldPropertyPartnersLP-2019.pdf</td>\n      <td>ESTAT INF RASTRUCTURE RENEWABLE POWER PRIVATE ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Brookfield Property Partners LP</td>\n      <td>Real Estate Development</td>\n      <td>Financial</td>\n      <td>2019</td>\n      <td>https://www.brookfield.com/sites/default/files...</td>\n      <td>BrookfieldPropertyPartnersLP-2019.pdf</td>\n      <td>Today our capacity is approximately MW of elec...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "print(df_processed.shape)\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# save only the paragraph to a text file\n",
    "df_processed[\"paragraph\"].to_csv('output/CSR_processed_raw_pdfMiner.txt', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# filter out company names\n",
    "own_stop_words = [\"prairiesky\", \"cousin\", \"use\", \"uk\", \"france\", \"landlord\", \"abs\", \"london\"]\n",
    "fsi_stop_words = df_processed[\"company\"].unique().tolist() \n",
    "\n",
    "# our list contains all english stop words + companies names + specific keywords\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(fsi_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "✔ Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# load spacy model to lematize text\n",
    "nlp = preprocess.load_spacy_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "100%|██████████| 20532/20532 [04:58<00:00, 68.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# lematize text and remove stopwords\n",
    "# use method 1\n",
    "\n",
    "df_processed = preprocess.load_lemmatize(\n",
    "    data=df_processed, \n",
    "    dir_file='output/CSR_processed_cleaned_pdfMiner.txt', \n",
    "    stop_words=stop_words, \n",
    "    nlp=nlp, \n",
    "    method=1, \n",
    "    update=True\n",
    ")"
   ]
  },
  {
   "source": [
    "## 1. Contextualized Topic Modelling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1.1 Prepare data for model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "2020-11-18 23:12:42.102 INFO    gensim.corpora.dictionary: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-18 23:12:43.124 INFO    gensim.corpora.dictionary: adding document #10000 to Dictionary(11876 unique tokens: ['asset', 'association', 'audit', 'board', 'brookfield']...)\n",
      "2020-11-18 23:12:43.977 INFO    gensim.corpora.dictionary: adding document #20000 to Dictionary(18772 unique tokens: ['asset', 'association', 'audit', 'board', 'brookfield']...)\n",
      "2020-11-18 23:12:44.024 INFO    gensim.corpora.dictionary: built Dictionary(19085 unique tokens: ['asset', 'association', 'audit', 'board', 'brookfield']...) from 20532 documents (total 1132200 corpus positions)\n",
      "2020-11-18 23:12:44.059 INFO    gensim.corpora.dictionary: discarding 14069 tokens: [('asset', 2419), ('business', 3981), ('corporate', 3866), ('datum', 2743), ('development', 3846), ('engagement', 2090), ('eople', 1), ('governance', 2359), ('management', 4078), ('ppro', 2)]...\n",
      "2020-11-18 23:12:44.061 INFO    gensim.corpora.dictionary: keeping 5016 tokens which were in no less than 10 and no more than 2053 (=10.0%) documents\n",
      "2020-11-18 23:12:44.081 INFO    gensim.corpora.dictionary: resulting dictionary: Dictionary(5016 unique tokens: ['association', 'audit', 'board', 'brookfield', 'capital']...)\n",
      "5016\n"
     ]
    }
   ],
   "source": [
    "with open('output/CSR_processed_cleaned_pdfMiner.txt',\"r\") as fr:\n",
    "    text_cleaned = [doc.split() for doc in fr.read().splitlines()] # load text for NPMI\n",
    "\n",
    "dictionary = Dictionary(text_cleaned)\n",
    "\n",
    "'''\n",
    "Remove very rare and very common words:\n",
    "\n",
    "- words appearing less than 15 times\n",
    "- words appearing in more than 10% of all documents\n",
    "'''\n",
    "\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.10, keep_n= 100000)\n",
    "print(len(dictionary.token2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in text_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(20532, 5016)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "tf_array = matutils.corpus2dense(bow_corpus, num_terms=len(dictionary.token2id)).T\n",
    "tf_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<20532x5016 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 674538 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# convert to sparse matrix\n",
    "tf_array_sparse = sparse.csr_matrix(tf_array)\n",
    "tf_array_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "2020-11-18 23:12:54.686 INFO    root: Load pretrained SentenceTransformer: distiluse-base-multilingual-cased\n",
      "2020-11-18 23:12:54.689 INFO    root: Did not find folder distiluse-base-multilingual-cased. Assume to download model from server.\n",
      "2020-11-18 23:12:54.724 INFO    root: Load SentenceTransformer from folder: C:\\Users\\Pieter-Jan/.cache\\torch\\sentence_transformers\\sbert.net_models_distiluse-base-multilingual-cased\n",
      "2020-11-18 23:12:59.160 INFO    root: Use pytorch device: cpu\n",
      "Batches: 100%|██████████| 103/103 [1:34:15<00:00, 54.91s/it]\n"
     ]
    }
   ],
   "source": [
    "# create or load bert embeddings (either use raw text or clean text)\n",
    "# we can expirment with both\n",
    "embeddings_bert = preprocess.load_bert_embeddings(\n",
    "        text_dir=\"output/CSR_processed_raw_pdfMiner.txt\", \n",
    "        model=\"distiluse-base-multilingual-cased\",\n",
    "        dir_embeddings=\"output/CRS_bertEmbeddings_pdfMiner.npy\",\n",
    "        update=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(20532, 512)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "embeddings_bert.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# ivert dictionary\n",
    "inv_token2id = {v: k for k, v in dictionary.token2id.items()}\n",
    "# create dataset\n",
    "training_dataset = CTMDataset(tf_array_sparse, embeddings_bert, inv_token2id)"
   ]
  },
  {
   "source": [
    "## 1.2 Train model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "Settings: \n",
      "                   N Components: 20\n",
      "                   Topic Prior Mean: 0.0\n",
      "                   Topic Prior Variance: 0.95\n",
      "                   Model Type: prodLDA\n",
      "                   Hidden Sizes: (100, 100)\n",
      "                   Activation: softplus\n",
      "                   Dropout: 0.2\n",
      "                   Learn Priors: True\n",
      "                   Learning Rate: 0.002\n",
      "                   Momentum: 0.99\n",
      "                   Reduce On Plateau: True\n",
      "                   Save Dir: None\n",
      "Epoch: [1/10]\tSamples: [20532/205320]\tTrain Loss: 360.49390376150643\tTime: 0:01:07.800099\n",
      "Epoch: [2/10]\tSamples: [41064/205320]\tTrain Loss: 339.0184819161857\tTime: 0:01:06.111920\n",
      "Epoch: [3/10]\tSamples: [61596/205320]\tTrain Loss: 334.02033401747883\tTime: 0:01:00.856951\n",
      "Epoch: [4/10]\tSamples: [82128/205320]\tTrain Loss: 331.64205469472955\tTime: 0:01:40.769198\n",
      "Epoch: [5/10]\tSamples: [102660/205320]\tTrain Loss: 330.4773736385581\tTime: 0:01:58.198308\n",
      "Epoch: [6/10]\tSamples: [123192/205320]\tTrain Loss: 329.7896332554062\tTime: 0:01:43.396394\n",
      "Epoch: [7/10]\tSamples: [143724/205320]\tTrain Loss: 329.49904417494645\tTime: 0:02:13.157609\n",
      "Epoch: [8/10]\tSamples: [164256/205320]\tTrain Loss: 328.8921850647769\tTime: 0:01:56.591835\n",
      "Epoch: [9/10]\tSamples: [184788/205320]\tTrain Loss: 328.70686040388176\tTime: 0:01:49.177430\n",
      "Epoch: [10/10]\tSamples: [205320/205320]\tTrain Loss: 328.40718218626716\tTime: 0:01:00.367380\n"
     ]
    }
   ],
   "source": [
    "random.seed(69)\n",
    "ctm = CTM(\n",
    "    input_size=len(dictionary.token2id), \n",
    "    bert_input_size=512, \n",
    "    n_components=20, \n",
    "    inference_type=\"combined\", \n",
    "    num_epochs=10,\n",
    "    reduce_on_plateau=True\n",
    "    )\n",
    "ctm.fit(training_dataset) # run model"
   ]
  },
  {
   "source": [
    "## 1.3 Evaluate topics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "cmt_topics_l = ctm.get_topic_lists(10)\n",
    "cmt_topics_d = {}\n",
    "for i in range(len(cmt_topics_l)):\n",
    "    cmt_topics_d[i] = cmt_topics_l[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             0              1            2           3             4  \\\n",
       "0      benefit      associate         life       offer          time   \n",
       "1        risks    curtailment         cemp      regime         recur   \n",
       "2       health         safety       survey    training       meeting   \n",
       "3        scope            use       metric      factor    greenhouse   \n",
       "4       garden        plastic       yellow      summer          host   \n",
       "5     landlord           like  electricity       tonne      coverage   \n",
       "6         leed  certification       street      square        office   \n",
       "7         real         estate        green      global         award   \n",
       "8       saving       lighting        solar      instal        system   \n",
       "9    assurance         review  information   statement  verification   \n",
       "10  efficiency           goal  sustainable        cost      prologis   \n",
       "11      target        achieve    reduction      design      progress   \n",
       "12       board      committee    executive    director        member   \n",
       "13      policy           code      conduct  compliance         ethic   \n",
       "14      centre       shopping    hammerson      retail        france   \n",
       "15        risk        climate       relate    identify      strategy   \n",
       "16      number         gender         time      female          rate   \n",
       "17        page          topic     material    boundary      approach   \n",
       "18      tanger         outlet    volunteer      center         local   \n",
       "19     culture         create       people        make       success   \n",
       "\n",
       "               5             6                7             8            9  \n",
       "0            pay      wellness          culture     diversity      medical  \n",
       "1         accrue     alternate  competitiveness     favorable   achievable  \n",
       "2   construction      supplier         investor      security        issue  \n",
       "3         source      indirect           direct     calculate      control  \n",
       "4    participant          just     preparedness       charity          say  \n",
       "5      intensity         meter            table          epra       obtain  \n",
       "6           star       certify             gold          foot    francisco  \n",
       "7       industry         index           leader   association     prologis  \n",
       "8           save     efficient     installation    efficiency        store  \n",
       "9          basis       process            level       limited   conclusion  \n",
       "10     renewable      increase      stewardship    initiative        value  \n",
       "11        breeam       measure         positive         major    recycling  \n",
       "12       officer   independent            chief  compensation    president  \n",
       "13         human      practice         supplier         right     internal  \n",
       "14          park         place         activity         local    programme  \n",
       "15      physical      response         increase     potential       assess  \n",
       "16        injury          hour             male           day   contractor  \n",
       "17    disclosure  organization         economic       content       aspect  \n",
       "18         event        school          partner       premium  partnership  \n",
       "19    commitment       deliver      environment       develop       future  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>benefit</td>\n      <td>associate</td>\n      <td>life</td>\n      <td>offer</td>\n      <td>time</td>\n      <td>pay</td>\n      <td>wellness</td>\n      <td>culture</td>\n      <td>diversity</td>\n      <td>medical</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>risks</td>\n      <td>curtailment</td>\n      <td>cemp</td>\n      <td>regime</td>\n      <td>recur</td>\n      <td>accrue</td>\n      <td>alternate</td>\n      <td>competitiveness</td>\n      <td>favorable</td>\n      <td>achievable</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>health</td>\n      <td>safety</td>\n      <td>survey</td>\n      <td>training</td>\n      <td>meeting</td>\n      <td>construction</td>\n      <td>supplier</td>\n      <td>investor</td>\n      <td>security</td>\n      <td>issue</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>scope</td>\n      <td>use</td>\n      <td>metric</td>\n      <td>factor</td>\n      <td>greenhouse</td>\n      <td>source</td>\n      <td>indirect</td>\n      <td>direct</td>\n      <td>calculate</td>\n      <td>control</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>garden</td>\n      <td>plastic</td>\n      <td>yellow</td>\n      <td>summer</td>\n      <td>host</td>\n      <td>participant</td>\n      <td>just</td>\n      <td>preparedness</td>\n      <td>charity</td>\n      <td>say</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>landlord</td>\n      <td>like</td>\n      <td>electricity</td>\n      <td>tonne</td>\n      <td>coverage</td>\n      <td>intensity</td>\n      <td>meter</td>\n      <td>table</td>\n      <td>epra</td>\n      <td>obtain</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>leed</td>\n      <td>certification</td>\n      <td>street</td>\n      <td>square</td>\n      <td>office</td>\n      <td>star</td>\n      <td>certify</td>\n      <td>gold</td>\n      <td>foot</td>\n      <td>francisco</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>real</td>\n      <td>estate</td>\n      <td>green</td>\n      <td>global</td>\n      <td>award</td>\n      <td>industry</td>\n      <td>index</td>\n      <td>leader</td>\n      <td>association</td>\n      <td>prologis</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>saving</td>\n      <td>lighting</td>\n      <td>solar</td>\n      <td>instal</td>\n      <td>system</td>\n      <td>save</td>\n      <td>efficient</td>\n      <td>installation</td>\n      <td>efficiency</td>\n      <td>store</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>assurance</td>\n      <td>review</td>\n      <td>information</td>\n      <td>statement</td>\n      <td>verification</td>\n      <td>basis</td>\n      <td>process</td>\n      <td>level</td>\n      <td>limited</td>\n      <td>conclusion</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>efficiency</td>\n      <td>goal</td>\n      <td>sustainable</td>\n      <td>cost</td>\n      <td>prologis</td>\n      <td>renewable</td>\n      <td>increase</td>\n      <td>stewardship</td>\n      <td>initiative</td>\n      <td>value</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>target</td>\n      <td>achieve</td>\n      <td>reduction</td>\n      <td>design</td>\n      <td>progress</td>\n      <td>breeam</td>\n      <td>measure</td>\n      <td>positive</td>\n      <td>major</td>\n      <td>recycling</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>board</td>\n      <td>committee</td>\n      <td>executive</td>\n      <td>director</td>\n      <td>member</td>\n      <td>officer</td>\n      <td>independent</td>\n      <td>chief</td>\n      <td>compensation</td>\n      <td>president</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>policy</td>\n      <td>code</td>\n      <td>conduct</td>\n      <td>compliance</td>\n      <td>ethic</td>\n      <td>human</td>\n      <td>practice</td>\n      <td>supplier</td>\n      <td>right</td>\n      <td>internal</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>centre</td>\n      <td>shopping</td>\n      <td>hammerson</td>\n      <td>retail</td>\n      <td>france</td>\n      <td>park</td>\n      <td>place</td>\n      <td>activity</td>\n      <td>local</td>\n      <td>programme</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>risk</td>\n      <td>climate</td>\n      <td>relate</td>\n      <td>identify</td>\n      <td>strategy</td>\n      <td>physical</td>\n      <td>response</td>\n      <td>increase</td>\n      <td>potential</td>\n      <td>assess</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>number</td>\n      <td>gender</td>\n      <td>time</td>\n      <td>female</td>\n      <td>rate</td>\n      <td>injury</td>\n      <td>hour</td>\n      <td>male</td>\n      <td>day</td>\n      <td>contractor</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>page</td>\n      <td>topic</td>\n      <td>material</td>\n      <td>boundary</td>\n      <td>approach</td>\n      <td>disclosure</td>\n      <td>organization</td>\n      <td>economic</td>\n      <td>content</td>\n      <td>aspect</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>tanger</td>\n      <td>outlet</td>\n      <td>volunteer</td>\n      <td>center</td>\n      <td>local</td>\n      <td>event</td>\n      <td>school</td>\n      <td>partner</td>\n      <td>premium</td>\n      <td>partnership</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>culture</td>\n      <td>create</td>\n      <td>people</td>\n      <td>make</td>\n      <td>success</td>\n      <td>commitment</td>\n      <td>deliver</td>\n      <td>environment</td>\n      <td>develop</td>\n      <td>future</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "(pd.DataFrame.from_dict(cmt_topics_d).T).to_csv(\"output/topics_CRS_CMT.csv\")\n",
    "pd.DataFrame.from_dict(cmt_topics_d).T"
   ]
  },
  {
   "source": [
    "### 1.3.1 Normalized Point-wise Mutual Information"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "oherence.text_analysis: 189 batches submitted to accumulate stats from 12096 documents (546473 virtual)\n",
      "2020-11-19 01:03:16.159 INFO    gensim.topic_coherence.text_analysis: 190 batches submitted to accumulate stats from 12160 documents (549800 virtual)\n",
      "2020-11-19 01:03:16.271 INFO    gensim.topic_coherence.text_analysis: 191 batches submitted to accumulate stats from 12224 documents (552668 virtual)\n",
      "2020-11-19 01:03:16.301 INFO    gensim.topic_coherence.text_analysis: 192 batches submitted to accumulate stats from 12288 documents (555693 virtual)\n",
      "2020-11-19 01:03:16.347 INFO    gensim.topic_coherence.text_analysis: 193 batches submitted to accumulate stats from 12352 documents (559313 virtual)\n",
      "2020-11-19 01:03:16.436 INFO    gensim.topic_coherence.text_analysis: 194 batches submitted to accumulate stats from 12416 documents (562405 virtual)\n",
      "2020-11-19 01:03:16.503 INFO    gensim.topic_coherence.text_analysis: 195 batches submitted to accumulate stats from 12480 documents (568909 virtual)\n",
      "2020-11-19 01:03:16.515 INFO    gensim.topic_coherence.text_analysis: 196 batches submitted to accumulate stats from 12544 documents (572159 virtual)\n",
      "2020-11-19 01:03:16.553 INFO    gensim.topic_coherence.text_analysis: 197 batches submitted to accumulate stats from 12608 documents (575459 virtual)\n",
      "2020-11-19 01:03:16.647 INFO    gensim.topic_coherence.text_analysis: 198 batches submitted to accumulate stats from 12672 documents (578799 virtual)\n",
      "2020-11-19 01:03:16.657 INFO    gensim.topic_coherence.text_analysis: 199 batches submitted to accumulate stats from 12736 documents (582069 virtual)\n",
      "2020-11-19 01:03:16.678 INFO    gensim.topic_coherence.text_analysis: 200 batches submitted to accumulate stats from 12800 documents (585926 virtual)\n",
      "2020-11-19 01:03:16.781 INFO    gensim.topic_coherence.text_analysis: 201 batches submitted to accumulate stats from 12864 documents (589720 virtual)\n",
      "2020-11-19 01:03:16.804 INFO    gensim.topic_coherence.text_analysis: 202 batches submitted to accumulate stats from 12928 documents (593581 virtual)\n",
      "2020-11-19 01:03:16.890 INFO    gensim.topic_coherence.text_analysis: 203 batches submitted to accumulate stats from 12992 documents (597481 virtual)\n",
      "2020-11-19 01:03:16.917 INFO    gensim.topic_coherence.text_analysis: 204 batches submitted to accumulate stats from 13056 documents (601473 virtual)\n",
      "2020-11-19 01:03:16.922 INFO    gensim.topic_coherence.text_analysis: 205 batches submitted to accumulate stats from 13120 documents (604299 virtual)\n",
      "2020-11-19 01:03:17.034 INFO    gensim.topic_coherence.text_analysis: 206 batches submitted to accumulate stats from 13184 documents (607287 virtual)\n",
      "2020-11-19 01:03:17.065 INFO    gensim.topic_coherence.text_analysis: 207 batches submitted to accumulate stats from 13248 documents (609997 virtual)\n",
      "2020-11-19 01:03:17.082 INFO    gensim.topic_coherence.text_analysis: 208 batches submitted to accumulate stats from 13312 documents (612921 virtual)\n",
      "2020-11-19 01:03:17.188 INFO    gensim.topic_coherence.text_analysis: 209 batches submitted to accumulate stats from 13376 documents (615775 virtual)\n",
      "2020-11-19 01:03:17.193 INFO    gensim.topic_coherence.text_analysis: 210 batches submitted to accumulate stats from 13440 documents (618452 virtual)\n",
      "2020-11-19 01:03:17.213 INFO    gensim.topic_coherence.text_analysis: 211 batches submitted to accumulate stats from 13504 documents (621093 virtual)\n",
      "2020-11-19 01:03:17.312 INFO    gensim.topic_coherence.text_analysis: 212 batches submitted to accumulate stats from 13568 documents (624057 virtual)\n",
      "2020-11-19 01:03:17.318 INFO    gensim.topic_coherence.text_analysis: 213 batches submitted to accumulate stats from 13632 documents (626868 virtual)\n",
      "2020-11-19 01:03:17.359 INFO    gensim.topic_coherence.text_analysis: 214 batches submitted to accumulate stats from 13696 documents (630027 virtual)\n",
      "2020-11-19 01:03:17.476 INFO    gensim.topic_coherence.text_analysis: 215 batches submitted to accumulate stats from 13760 documents (632571 virtual)\n",
      "2020-11-19 01:03:17.479 INFO    gensim.topic_coherence.text_analysis: 216 batches submitted to accumulate stats from 13824 documents (635371 virtual)\n",
      "2020-11-19 01:03:17.510 INFO    gensim.topic_coherence.text_analysis: 217 batches submitted to accumulate stats from 13888 documents (638992 virtual)\n",
      "2020-11-19 01:03:17.636 INFO    gensim.topic_coherence.text_analysis: 218 batches submitted to accumulate stats from 13952 documents (641554 virtual)\n",
      "2020-11-19 01:03:17.643 INFO    gensim.topic_coherence.text_analysis: 219 batches submitted to accumulate stats from 14016 documents (643962 virtual)\n",
      "2020-11-19 01:03:17.717 INFO    gensim.topic_coherence.text_analysis: 220 batches submitted to accumulate stats from 14080 documents (646719 virtual)\n",
      "2020-11-19 01:03:17.777 INFO    gensim.topic_coherence.text_analysis: 221 batches submitted to accumulate stats from 14144 documents (649838 virtual)\n",
      "2020-11-19 01:03:17.823 INFO    gensim.topic_coherence.text_analysis: 222 batches submitted to accumulate stats from 14208 documents (653116 virtual)\n",
      "2020-11-19 01:03:17.895 INFO    gensim.topic_coherence.text_analysis: 223 batches submitted to accumulate stats from 14272 documents (656147 virtual)\n",
      "2020-11-19 01:03:17.914 INFO    gensim.topic_coherence.text_analysis: 224 batches submitted to accumulate stats from 14336 documents (658508 virtual)\n",
      "2020-11-19 01:03:17.949 INFO    gensim.topic_coherence.text_analysis: 225 batches submitted to accumulate stats from 14400 documents (661077 virtual)\n",
      "2020-11-19 01:03:18.025 INFO    gensim.topic_coherence.text_analysis: 226 batches submitted to accumulate stats from 14464 documents (663614 virtual)\n",
      "2020-11-19 01:03:18.044 INFO    gensim.topic_coherence.text_analysis: 227 batches submitted to accumulate stats from 14528 documents (666326 virtual)\n",
      "2020-11-19 01:03:18.085 INFO    gensim.topic_coherence.text_analysis: 228 batches submitted to accumulate stats from 14592 documents (669390 virtual)\n",
      "2020-11-19 01:03:18.147 INFO    gensim.topic_coherence.text_analysis: 229 batches submitted to accumulate stats from 14656 documents (672012 virtual)\n",
      "2020-11-19 01:03:18.163 INFO    gensim.topic_coherence.text_analysis: 230 batches submitted to accumulate stats from 14720 documents (674719 virtual)\n",
      "2020-11-19 01:03:18.194 INFO    gensim.topic_coherence.text_analysis: 231 batches submitted to accumulate stats from 14784 documents (677993 virtual)\n",
      "2020-11-19 01:03:18.273 INFO    gensim.topic_coherence.text_analysis: 232 batches submitted to accumulate stats from 14848 documents (681258 virtual)\n",
      "2020-11-19 01:03:18.280 INFO    gensim.topic_coherence.text_analysis: 233 batches submitted to accumulate stats from 14912 documents (684644 virtual)\n",
      "2020-11-19 01:03:18.314 INFO    gensim.topic_coherence.text_analysis: 234 batches submitted to accumulate stats from 14976 documents (687856 virtual)\n",
      "2020-11-19 01:03:18.386 INFO    gensim.topic_coherence.text_analysis: 235 batches submitted to accumulate stats from 15040 documents (691171 virtual)\n",
      "2020-11-19 01:03:18.390 INFO    gensim.topic_coherence.text_analysis: 236 batches submitted to accumulate stats from 15104 documents (694267 virtual)\n",
      "2020-11-19 01:03:18.443 INFO    gensim.topic_coherence.text_analysis: 237 batches submitted to accumulate stats from 15168 documents (696971 virtual)\n",
      "2020-11-19 01:03:18.515 INFO    gensim.topic_coherence.text_analysis: 238 batches submitted to accumulate stats from 15232 documents (699628 virtual)\n",
      "2020-11-19 01:03:18.532 INFO    gensim.topic_coherence.text_analysis: 239 batches submitted to accumulate stats from 15296 documents (702632 virtual)\n",
      "2020-11-19 01:03:18.594 INFO    gensim.topic_coherence.text_analysis: 240 batches submitted to accumulate stats from 15360 documents (705213 virtual)\n",
      "2020-11-19 01:03:18.676 INFO    gensim.topic_coherence.text_analysis: 241 batches submitted to accumulate stats from 15424 documents (707815 virtual)\n",
      "2020-11-19 01:03:18.687 INFO    gensim.topic_coherence.text_analysis: 242 batches submitted to accumulate stats from 15488 documents (710790 virtual)\n",
      "2020-11-19 01:03:18.723 INFO    gensim.topic_coherence.text_analysis: 243 batches submitted to accumulate stats from 15552 documents (713334 virtual)\n",
      "2020-11-19 01:03:18.775 INFO    gensim.topic_coherence.text_analysis: 244 batches submitted to accumulate stats from 15616 documents (716378 virtual)\n",
      "2020-11-19 01:03:18.811 INFO    gensim.topic_coherence.text_analysis: 245 batches submitted to accumulate stats from 15680 documents (718890 virtual)\n",
      "2020-11-19 01:03:18.827 INFO    gensim.topic_coherence.text_analysis: 246 batches submitted to accumulate stats from 15744 documents (722092 virtual)\n",
      "2020-11-19 01:03:18.891 INFO    gensim.topic_coherence.text_analysis: 247 batches submitted to accumulate stats from 15808 documents (725041 virtual)\n",
      "2020-11-19 01:03:18.923 INFO    gensim.topic_coherence.text_analysis: 248 batches submitted to accumulate stats from 15872 documents (728451 virtual)\n",
      "2020-11-19 01:03:18.930 INFO    gensim.topic_coherence.text_analysis: 249 batches submitted to accumulate stats from 15936 documents (730896 virtual)\n",
      "2020-11-19 01:03:19.022 INFO    gensim.topic_coherence.text_analysis: 250 batches submitted to accumulate stats from 16000 documents (733892 virtual)\n",
      "2020-11-19 01:03:19.030 INFO    gensim.topic_coherence.text_analysis: 251 batches submitted to accumulate stats from 16064 documents (736876 virtual)\n",
      "2020-11-19 01:03:19.104 INFO    gensim.topic_coherence.text_analysis: 252 batches submitted to accumulate stats from 16128 documents (739417 virtual)\n",
      "2020-11-19 01:03:19.175 INFO    gensim.topic_coherence.text_analysis: 253 batches submitted to accumulate stats from 16192 documents (742869 virtual)\n",
      "2020-11-19 01:03:19.254 INFO    gensim.topic_coherence.text_analysis: 254 batches submitted to accumulate stats from 16256 documents (745244 virtual)\n",
      "2020-11-19 01:03:19.270 INFO    gensim.topic_coherence.text_analysis: 255 batches submitted to accumulate stats from 16320 documents (748139 virtual)\n",
      "2020-11-19 01:03:19.346 INFO    gensim.topic_coherence.text_analysis: 256 batches submitted to accumulate stats from 16384 documents (751441 virtual)\n",
      "2020-11-19 01:03:19.405 INFO    gensim.topic_coherence.text_analysis: 257 batches submitted to accumulate stats from 16448 documents (754053 virtual)\n",
      "2020-11-19 01:03:19.420 INFO    gensim.topic_coherence.text_analysis: 258 batches submitted to accumulate stats from 16512 documents (757367 virtual)\n",
      "2020-11-19 01:03:19.502 INFO    gensim.topic_coherence.text_analysis: 259 batches submitted to accumulate stats from 16576 documents (760816 virtual)\n",
      "2020-11-19 01:03:19.518 INFO    gensim.topic_coherence.text_analysis: 260 batches submitted to accumulate stats from 16640 documents (764388 virtual)\n",
      "2020-11-19 01:03:19.553 INFO    gensim.topic_coherence.text_analysis: 261 batches submitted to accumulate stats from 16704 documents (767661 virtual)\n",
      "2020-11-19 01:03:19.636 INFO    gensim.topic_coherence.text_analysis: 262 batches submitted to accumulate stats from 16768 documents (770824 virtual)\n",
      "2020-11-19 01:03:19.684 INFO    gensim.topic_coherence.text_analysis: 263 batches submitted to accumulate stats from 16832 documents (773639 virtual)\n",
      "2020-11-19 01:03:19.721 INFO    gensim.topic_coherence.text_analysis: 264 batches submitted to accumulate stats from 16896 documents (776450 virtual)\n",
      "2020-11-19 01:03:19.810 INFO    gensim.topic_coherence.text_analysis: 265 batches submitted to accumulate stats from 16960 documents (779310 virtual)\n",
      "2020-11-19 01:03:19.825 INFO    gensim.topic_coherence.text_analysis: 266 batches submitted to accumulate stats from 17024 documents (782053 virtual)\n",
      "2020-11-19 01:03:19.842 INFO    gensim.topic_coherence.text_analysis: 267 batches submitted to accumulate stats from 17088 documents (785458 virtual)\n",
      "2020-11-19 01:03:19.930 INFO    gensim.topic_coherence.text_analysis: 268 batches submitted to accumulate stats from 17152 documents (788204 virtual)\n",
      "2020-11-19 01:03:19.938 INFO    gensim.topic_coherence.text_analysis: 269 batches submitted to accumulate stats from 17216 documents (791146 virtual)\n",
      "2020-11-19 01:03:19.961 INFO    gensim.topic_coherence.text_analysis: 270 batches submitted to accumulate stats from 17280 documents (794007 virtual)\n",
      "2020-11-19 01:03:20.041 INFO    gensim.topic_coherence.text_analysis: 271 batches submitted to accumulate stats from 17344 documents (797054 virtual)\n",
      "2020-11-19 01:03:20.067 INFO    gensim.topic_coherence.text_analysis: 272 batches submitted to accumulate stats from 17408 documents (800219 virtual)\n",
      "2020-11-19 01:03:20.125 INFO    gensim.topic_coherence.text_analysis: 273 batches submitted to accumulate stats from 17472 documents (803269 virtual)\n",
      "2020-11-19 01:03:20.178 INFO    gensim.topic_coherence.text_analysis: 274 batches submitted to accumulate stats from 17536 documents (805801 virtual)\n",
      "2020-11-19 01:03:20.222 INFO    gensim.topic_coherence.text_analysis: 275 batches submitted to accumulate stats from 17600 documents (808642 virtual)\n",
      "2020-11-19 01:03:20.248 INFO    gensim.topic_coherence.text_analysis: 276 batches submitted to accumulate stats from 17664 documents (811945 virtual)\n",
      "2020-11-19 01:03:20.336 INFO    gensim.topic_coherence.text_analysis: 277 batches submitted to accumulate stats from 17728 documents (814720 virtual)\n",
      "2020-11-19 01:03:20.374 INFO    gensim.topic_coherence.text_analysis: 278 batches submitted to accumulate stats from 17792 documents (818048 virtual)\n",
      "2020-11-19 01:03:20.430 INFO    gensim.topic_coherence.text_analysis: 279 batches submitted to accumulate stats from 17856 documents (820925 virtual)\n",
      "2020-11-19 01:03:20.469 INFO    gensim.topic_coherence.text_analysis: 280 batches submitted to accumulate stats from 17920 documents (824268 virtual)\n",
      "2020-11-19 01:03:20.509 INFO    gensim.topic_coherence.text_analysis: 281 batches submitted to accumulate stats from 17984 documents (827157 virtual)\n",
      "2020-11-19 01:03:20.577 INFO    gensim.topic_coherence.text_analysis: 282 batches submitted to accumulate stats from 18048 documents (830758 virtual)\n",
      "2020-11-19 01:03:20.594 INFO    gensim.topic_coherence.text_analysis: 283 batches submitted to accumulate stats from 18112 documents (833538 virtual)\n",
      "2020-11-19 01:03:20.649 INFO    gensim.topic_coherence.text_analysis: 284 batches submitted to accumulate stats from 18176 documents (836962 virtual)\n",
      "2020-11-19 01:03:20.694 INFO    gensim.topic_coherence.text_analysis: 285 batches submitted to accumulate stats from 18240 documents (839976 virtual)\n",
      "2020-11-19 01:03:20.759 INFO    gensim.topic_coherence.text_analysis: 286 batches submitted to accumulate stats from 18304 documents (843187 virtual)\n",
      "2020-11-19 01:03:20.803 INFO    gensim.topic_coherence.text_analysis: 287 batches submitted to accumulate stats from 18368 documents (846109 virtual)\n",
      "2020-11-19 01:03:20.876 INFO    gensim.topic_coherence.text_analysis: 288 batches submitted to accumulate stats from 18432 documents (848910 virtual)\n",
      "2020-11-19 01:03:20.881 INFO    gensim.topic_coherence.text_analysis: 289 batches submitted to accumulate stats from 18496 documents (852411 virtual)\n",
      "2020-11-19 01:03:20.968 INFO    gensim.topic_coherence.text_analysis: 290 batches submitted to accumulate stats from 18560 documents (856278 virtual)\n",
      "2020-11-19 01:03:21.001 INFO    gensim.topic_coherence.text_analysis: 291 batches submitted to accumulate stats from 18624 documents (859637 virtual)\n",
      "2020-11-19 01:03:21.009 INFO    gensim.topic_coherence.text_analysis: 292 batches submitted to accumulate stats from 18688 documents (863052 virtual)\n",
      "2020-11-19 01:03:21.087 INFO    gensim.topic_coherence.text_analysis: 293 batches submitted to accumulate stats from 18752 documents (866355 virtual)\n",
      "2020-11-19 01:03:21.094 INFO    gensim.topic_coherence.text_analysis: 294 batches submitted to accumulate stats from 18816 documents (869866 virtual)\n",
      "2020-11-19 01:03:21.153 INFO    gensim.topic_coherence.text_analysis: 295 batches submitted to accumulate stats from 18880 documents (873037 virtual)\n",
      "2020-11-19 01:03:21.226 INFO    gensim.topic_coherence.text_analysis: 296 batches submitted to accumulate stats from 18944 documents (876188 virtual)\n",
      "2020-11-19 01:03:21.266 INFO    gensim.topic_coherence.text_analysis: 297 batches submitted to accumulate stats from 19008 documents (879311 virtual)\n",
      "2020-11-19 01:03:21.316 INFO    gensim.topic_coherence.text_analysis: 298 batches submitted to accumulate stats from 19072 documents (882353 virtual)\n",
      "2020-11-19 01:03:21.381 INFO    gensim.topic_coherence.text_analysis: 299 batches submitted to accumulate stats from 19136 documents (885264 virtual)\n",
      "2020-11-19 01:03:21.408 INFO    gensim.topic_coherence.text_analysis: 300 batches submitted to accumulate stats from 19200 documents (888364 virtual)\n",
      "2020-11-19 01:03:21.451 INFO    gensim.topic_coherence.text_analysis: 301 batches submitted to accumulate stats from 19264 documents (891569 virtual)\n",
      "2020-11-19 01:03:21.511 INFO    gensim.topic_coherence.text_analysis: 302 batches submitted to accumulate stats from 19328 documents (894608 virtual)\n",
      "2020-11-19 01:03:21.571 INFO    gensim.topic_coherence.text_analysis: 303 batches submitted to accumulate stats from 19392 documents (897807 virtual)\n",
      "2020-11-19 01:03:21.624 INFO    gensim.topic_coherence.text_analysis: 304 batches submitted to accumulate stats from 19456 documents (900785 virtual)\n",
      "2020-11-19 01:03:21.676 INFO    gensim.topic_coherence.text_analysis: 305 batches submitted to accumulate stats from 19520 documents (903969 virtual)\n",
      "2020-11-19 01:03:21.731 INFO    gensim.topic_coherence.text_analysis: 306 batches submitted to accumulate stats from 19584 documents (907369 virtual)\n",
      "2020-11-19 01:03:21.799 INFO    gensim.topic_coherence.text_analysis: 307 batches submitted to accumulate stats from 19648 documents (910259 virtual)\n",
      "2020-11-19 01:03:21.824 INFO    gensim.topic_coherence.text_analysis: 308 batches submitted to accumulate stats from 19712 documents (913023 virtual)\n",
      "2020-11-19 01:03:21.954 INFO    gensim.topic_coherence.text_analysis: 309 batches submitted to accumulate stats from 19776 documents (915819 virtual)\n",
      "2020-11-19 01:03:21.988 INFO    gensim.topic_coherence.text_analysis: 310 batches submitted to accumulate stats from 19840 documents (918629 virtual)\n",
      "2020-11-19 01:03:22.022 INFO    gensim.topic_coherence.text_analysis: 311 batches submitted to accumulate stats from 19904 documents (921567 virtual)\n",
      "2020-11-19 01:03:22.122 INFO    gensim.topic_coherence.text_analysis: 312 batches submitted to accumulate stats from 19968 documents (924288 virtual)\n",
      "2020-11-19 01:03:22.138 INFO    gensim.topic_coherence.text_analysis: 313 batches submitted to accumulate stats from 20032 documents (927325 virtual)\n",
      "2020-11-19 01:03:22.168 INFO    gensim.topic_coherence.text_analysis: 314 batches submitted to accumulate stats from 20096 documents (930205 virtual)\n",
      "2020-11-19 01:03:22.246 INFO    gensim.topic_coherence.text_analysis: 315 batches submitted to accumulate stats from 20160 documents (933648 virtual)\n",
      "2020-11-19 01:03:22.264 INFO    gensim.topic_coherence.text_analysis: 316 batches submitted to accumulate stats from 20224 documents (936554 virtual)\n",
      "2020-11-19 01:03:22.330 INFO    gensim.topic_coherence.text_analysis: 317 batches submitted to accumulate stats from 20288 documents (939874 virtual)\n",
      "2020-11-19 01:03:22.372 INFO    gensim.topic_coherence.text_analysis: 318 batches submitted to accumulate stats from 20352 documents (942476 virtual)\n",
      "2020-11-19 01:03:22.404 INFO    gensim.topic_coherence.text_analysis: 319 batches submitted to accumulate stats from 20416 documents (945688 virtual)\n",
      "2020-11-19 01:03:22.451 INFO    gensim.topic_coherence.text_analysis: 320 batches submitted to accumulate stats from 20480 documents (945991 virtual)\n",
      "2020-11-19 01:03:22.837 INFO    gensim.topic_coherence.text_analysis: 3 accumulators retrieved from output queue\n",
      "2020-11-19 01:03:22.900 INFO    gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 946290 virtual documents\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.10005078526162889"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "npmi = CoherenceNPMI(texts=text_cleaned, topics=ctm.get_topic_lists(10))\n",
    "npmi.score()"
   ]
  },
  {
   "source": [
    "### 1.3.2 External Word Embeddings Topic Coherence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "2020-11-19 01:03:23.915 INFO    gensim.models.utils_any2vec: loading projection weights from C:\\Users\\Pieter-Jan/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n",
      "2020-11-19 01:04:25.497 INFO    gensim.models.utils_any2vec: loaded (3000000, 300) matrix from C:\\Users\\Pieter-Jan/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.14553869"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "CoherenceWordEmbeddings(ctm.get_topic_lists(10)).score()"
   ]
  },
  {
   "source": [
    "### 1.3.3 Rank-Biased Overlap "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9981349341284962"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "InvertedRBO(ctm.get_topic_lists(10)).score()"
   ]
  },
  {
   "source": [
    "## 2. Latent Dirichlet Allocation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.1 Train model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " #14 (0.050): 0.035*\"hall\" + 0.034*\"office\" + 0.033*\"charter\" + 0.030*\"square\" + 0.021*\"foot\" + 0.020*\"street\" + 0.016*\"fund\" + 0.014*\"june\" + 0.014*\"july\" + 0.012*\"retail\"\n",
      "2020-11-19 01:06:38.447 INFO    gensim.models.ldamodel: topic diff=0.184392, rho=0.227826\n",
      "2020-11-19 01:06:38.449 INFO    gensim.models.ldamulticore: PROGRESS: pass 8, dispatched chunk #7 = documents up to #16000/20532, outstanding queue size 6\n",
      "2020-11-19 01:06:40.352 INFO    gensim.models.ldamulticore: PROGRESS: pass 8, dispatched chunk #8 = documents up to #18000/20532, outstanding queue size 6\n",
      "2020-11-19 01:06:40.764 INFO    gensim.models.ldamodel: merging changes from 4000 documents into a model of 20532 documents\n",
      "2020-11-19 01:06:40.777 INFO    gensim.models.ldamodel: topic #0 (0.050): 0.037*\"electricity\" + 0.022*\"natural\" + 0.021*\"use\" + 0.020*\"fuel\" + 0.017*\"factor\" + 0.017*\"source\" + 0.017*\"renewable\" + 0.015*\"travel\" + 0.014*\"office\" + 0.014*\"landlord\"\n",
      "2020-11-19 01:06:40.780 INFO    gensim.models.ldamodel: topic #19 (0.050): 0.020*\"local\" + 0.017*\"volunteer\" + 0.013*\"partner\" + 0.013*\"school\" + 0.012*\"time\" + 0.012*\"organization\" + 0.011*\"event\" + 0.011*\"help\" + 0.011*\"charity\" + 0.010*\"city\"\n",
      "2020-11-19 01:06:40.783 INFO    gensim.models.ldamodel: topic #9 (0.050): 0.037*\"training\" + 0.036*\"health\" + 0.034*\"safety\" + 0.011*\"manager\" + 0.010*\"skill\" + 0.010*\"professional\" + 0.010*\"associate\" + 0.009*\"education\" + 0.009*\"hour\" + 0.009*\"career\"\n",
      "2020-11-19 01:06:40.785 INFO    gensim.models.ldamodel: topic #4 (0.050): 0.114*\"scope\" + 0.080*\"like\" + 0.058*\"intensity\" + 0.031*\"mtco\" + 0.027*\"landlord\" + 0.027*\"control\" + 0.026*\"reduction\" + 0.024*\"metric\" + 0.022*\"electricity\" + 0.016*\"baseline\"\n",
      "2020-11-19 01:06:40.795 INFO    gensim.models.ldamodel: topic #7 (0.050): 0.030*\"page\" + 0.023*\"disclosure\" + 0.021*\"reporting\" + 0.019*\"approach\" + 0.018*\"material\" + 0.018*\"indicator\" + 0.014*\"organization\" + 0.014*\"topic\" + 0.013*\"information\" + 0.013*\"aspect\"\n",
      "2020-11-19 01:06:40.797 INFO    gensim.models.ldamodel: topic diff=0.175079, rho=0.227826\n",
      "2020-11-19 01:06:40.811 INFO    gensim.models.ldamulticore: PROGRESS: pass 8, dispatched chunk #9 = documents up to #20000/20532, outstanding queue size 6\n",
      "2020-11-19 01:06:42.690 INFO    gensim.models.ldamulticore: PROGRESS: pass 8, dispatched chunk #10 = documents up to #20532/20532, outstanding queue size 6\n",
      "2020-11-19 01:06:43.153 INFO    gensim.models.ldamodel: merging changes from 4000 documents into a model of 20532 documents\n",
      "2020-11-19 01:06:43.177 INFO    gensim.models.ldamodel: topic #4 (0.050): 0.116*\"scope\" + 0.075*\"like\" + 0.059*\"intensity\" + 0.030*\"mtco\" + 0.027*\"metric\" + 0.027*\"reduction\" + 0.026*\"control\" + 0.023*\"landlord\" + 0.022*\"electricity\" + 0.016*\"baseline\"\n",
      "2020-11-19 01:06:43.179 INFO    gensim.models.ldamodel: topic #8 (0.050): 0.011*\"supplier\" + 0.010*\"target\" + 0.010*\"issue\" + 0.010*\"design\" + 0.009*\"plan\" + 0.008*\"local\" + 0.008*\"develop\" + 0.008*\"review\" + 0.008*\"programme\" + 0.008*\"supply\"\n",
      "2020-11-19 01:06:43.180 INFO    gensim.models.ldamodel: topic #19 (0.050): 0.021*\"local\" + 0.018*\"volunteer\" + 0.015*\"organization\" + 0.013*\"foundation\" + 0.012*\"school\" + 0.012*\"partner\" + 0.012*\"time\" + 0.011*\"charity\" + 0.011*\"help\" + 0.011*\"event\"\n",
      "2020-11-19 01:06:43.182 INFO    gensim.models.ldamodel: topic #5 (0.050): 0.055*\"center\" + 0.014*\"wellness\" + 0.013*\"simon\" + 0.013*\"percent\" + 0.011*\"healthy\" + 0.009*\"health\" + 0.009*\"tree\" + 0.008*\"mall\" + 0.007*\"station\" + 0.007*\"say\"\n",
      "2020-11-19 01:06:43.184 INFO    gensim.models.ldamodel: topic #0 (0.050): 0.039*\"electricity\" + 0.021*\"use\" + 0.019*\"natural\" + 0.018*\"fuel\" + 0.017*\"renewable\" + 0.017*\"source\" + 0.017*\"factor\" + 0.016*\"travel\" + 0.015*\"office\" + 0.013*\"indirect\"\n",
      "2020-11-19 01:06:43.186 INFO    gensim.models.ldamodel: topic diff=0.134445, rho=0.227826\n",
      "2020-11-19 01:06:43.735 INFO    gensim.models.ldamodel: -7.704 per-word bound, 208.5 perplexity estimate based on a held-out corpus of 532 documents with 23103 words\n",
      "2020-11-19 01:06:44.813 INFO    gensim.models.ldamodel: merging changes from 4000 documents into a model of 20532 documents\n",
      "2020-11-19 01:06:44.823 INFO    gensim.models.ldamodel: topic #6 (0.050): 0.038*\"cost\" + 0.019*\"increase\" + 0.013*\"value\" + 0.013*\"outlet\" + 0.010*\"benefit\" + 0.010*\"result\" + 0.010*\"saving\" + 0.010*\"believe\" + 0.009*\"long\" + 0.009*\"charge\"\n",
      "2020-11-19 01:06:44.826 INFO    gensim.models.ldamodel: topic #10 (0.050): 0.024*\"apartment\" + 0.023*\"hotel\" + 0.019*\"francisco\" + 0.016*\"california\" + 0.016*\"park\" + 0.016*\"like\" + 0.015*\"avenue\" + 0.015*\"york\" + 0.015*\"home\" + 0.014*\"drive\"\n",
      "2020-11-19 01:06:44.828 INFO    gensim.models.ldamodel: topic #8 (0.050): 0.011*\"supplier\" + 0.010*\"issue\" + 0.010*\"target\" + 0.010*\"design\" + 0.009*\"plan\" + 0.008*\"local\" + 0.008*\"develop\" + 0.008*\"supply\" + 0.008*\"strategy\" + 0.008*\"review\"\n",
      "2020-11-19 01:06:44.830 INFO    gensim.models.ldamodel: topic #18 (0.050): 0.066*\"risk\" + 0.039*\"climate\" + 0.013*\"relate\" + 0.012*\"investment\" + 0.011*\"strategy\" + 0.011*\"process\" + 0.011*\"identify\" + 0.009*\"increase\" + 0.009*\"financial\" + 0.008*\"response\"\n",
      "2020-11-19 01:06:44.832 INFO    gensim.models.ldamodel: topic #12 (0.050): 0.065*\"leed\" + 0.050*\"certification\" + 0.025*\"certify\" + 0.025*\"star\" + 0.025*\"gold\" + 0.023*\"green\" + 0.022*\"exist\" + 0.015*\"design\" + 0.014*\"process\" + 0.013*\"construction\"\n",
      "2020-11-19 01:06:44.834 INFO    gensim.models.ldamodel: topic diff=0.141256, rho=0.227826\n",
      "2020-11-19 01:06:45.645 INFO    gensim.models.ldamodel: -7.741 per-word bound, 213.9 perplexity estimate based on a held-out corpus of 532 documents with 23103 words\n",
      "2020-11-19 01:06:46.875 INFO    gensim.models.ldamodel: merging changes from 4000 documents into a model of 20532 documents\n",
      "2020-11-19 01:06:46.885 INFO    gensim.models.ldamodel: topic #7 (0.050): 0.025*\"page\" + 0.020*\"organization\" + 0.020*\"disclosure\" + 0.018*\"material\" + 0.018*\"reporting\" + 0.017*\"approach\" + 0.015*\"topic\" + 0.015*\"aspect\" + 0.014*\"indicator\" + 0.014*\"information\"\n",
      "2020-11-19 01:06:46.890 INFO    gensim.models.ldamodel: topic #3 (0.050): 0.049*\"goal\" + 0.033*\"reduction\" + 0.030*\"achieve\" + 0.029*\"efficiency\" + 0.028*\"target\" + 0.021*\"progress\" + 0.017*\"sustainable\" + 0.015*\"commitment\" + 0.012*\"initiative\" + 0.011*\"track\"\n",
      "2020-11-19 01:06:46.892 INFO    gensim.models.ldamodel: topic #5 (0.050): 0.059*\"center\" + 0.034*\"simon\" + 0.011*\"shopping\" + 0.011*\"wellness\" + 0.010*\"mall\" + 0.010*\"healthy\" + 0.010*\"station\" + 0.009*\"percent\" + 0.008*\"health\" + 0.007*\"improve\"\n",
      "2020-11-19 01:06:46.894 INFO    gensim.models.ldamodel: topic #2 (0.050): 0.044*\"centre\" + 0.036*\"hammerson\" + 0.032*\"number\" + 0.031*\"retail\" + 0.031*\"shopping\" + 0.026*\"park\" + 0.022*\"france\" + 0.018*\"female\" + 0.018*\"positive\" + 0.018*\"employment\"\n",
      "2020-11-19 01:06:46.896 INFO    gensim.models.ldamodel: topic #10 (0.050): 0.037*\"apartment\" + 0.018*\"home\" + 0.018*\"california\" + 0.018*\"francisco\" + 0.017*\"hotel\" + 0.016*\"york\" + 0.015*\"seattle\" + 0.014*\"like\" + 0.014*\"park\" + 0.012*\"avenue\"\n",
      "2020-11-19 01:06:46.898 INFO    gensim.models.ldamodel: topic diff=0.137491, rho=0.227826\n",
      "2020-11-19 01:06:47.301 INFO    gensim.models.ldamodel: -7.759 per-word bound, 216.6 perplexity estimate based on a held-out corpus of 532 documents with 23103 words\n",
      "2020-11-19 01:06:47.311 INFO    gensim.models.ldamodel: merging changes from 532 documents into a model of 20532 documents\n",
      "2020-11-19 01:06:47.320 INFO    gensim.models.ldamodel: topic #15 (0.050): 0.041*\"board\" + 0.025*\"committee\" + 0.023*\"director\" + 0.020*\"policy\" + 0.019*\"executive\" + 0.014*\"conduct\" + 0.014*\"ethic\" + 0.014*\"code\" + 0.012*\"officer\" + 0.011*\"member\"\n",
      "2020-11-19 01:06:47.322 INFO    gensim.models.ldamodel: topic #0 (0.050): 0.036*\"electricity\" + 0.023*\"use\" + 0.021*\"fuel\" + 0.021*\"natural\" + 0.019*\"source\" + 0.016*\"factor\" + 0.016*\"greenhouse\" + 0.015*\"travel\" + 0.015*\"renewable\" + 0.015*\"office\"\n",
      "2020-11-19 01:06:47.323 INFO    gensim.models.ldamodel: topic #10 (0.050): 0.029*\"apartment\" + 0.015*\"houston\" + 0.015*\"like\" + 0.015*\"california\" + 0.014*\"home\" + 0.013*\"york\" + 0.013*\"hotel\" + 0.013*\"francisco\" + 0.013*\"office\" + 0.012*\"street\"\n",
      "2020-11-19 01:06:47.325 INFO    gensim.models.ldamodel: topic #9 (0.050): 0.033*\"training\" + 0.030*\"health\" + 0.029*\"safety\" + 0.013*\"associate\" + 0.011*\"manager\" + 0.010*\"leadership\" + 0.010*\"time\" + 0.010*\"professional\" + 0.010*\"hour\" + 0.010*\"education\"\n",
      "2020-11-19 01:06:47.327 INFO    gensim.models.ldamodel: topic #12 (0.050): 0.063*\"leed\" + 0.044*\"certification\" + 0.027*\"certify\" + 0.026*\"gold\" + 0.024*\"star\" + 0.021*\"green\" + 0.018*\"exist\" + 0.015*\"design\" + 0.015*\"process\" + 0.013*\"silver\"\n",
      "2020-11-19 01:06:47.330 INFO    gensim.models.ldamodel: topic diff=0.166390, rho=0.227826\n",
      "2020-11-19 01:06:47.669 INFO    gensim.models.ldamodel: -7.589 per-word bound, 192.5 perplexity estimate based on a held-out corpus of 532 documents with 23103 words\n",
      "2020-11-19 01:06:47.671 INFO    gensim.models.ldamulticore: PROGRESS: pass 9, dispatched chunk #0 = documents up to #2000/20532, outstanding queue size 1\n",
      "2020-11-19 01:06:47.693 INFO    gensim.models.ldamulticore: PROGRESS: pass 9, dispatched chunk #1 = documents up to #4000/20532, outstanding queue size 2\n",
      "2020-11-19 01:06:47.726 INFO    gensim.models.ldamulticore: PROGRESS: pass 9, dispatched chunk #2 = documents up to #6000/20532, outstanding queue size 3\n",
      "2020-11-19 01:06:47.728 INFO    gensim.models.ldamulticore: PROGRESS: pass 9, dispatched chunk #3 = documents up to #8000/20532, outstanding queue size 4\n",
      "2020-11-19 01:06:47.730 INFO    gensim.models.ldamulticore: PROGRESS: pass 9, dispatched chunk #4 = documents up to #10000/20532, outstanding queue size 5\n",
      "2020-11-19 01:06:47.766 INFO    gensim.models.ldamulticore: PROGRESS: pass 9, dispatched chunk #5 = documents up to #12000/20532, outstanding queue size 6\n",
      "2020-11-19 01:06:49.855 INFO    gensim.models.ldamulticore: PROGRESS: pass 9, dispatched chunk #6 = documents up to #14000/20532, outstanding queue size 7\n",
      "2020-11-19 01:06:50.026 INFO    gensim.models.ldamodel: merging changes from 4000 documents into a model of 20532 documents\n",
      "2020-11-19 01:06:50.036 INFO    gensim.models.ldamodel: topic #10 (0.050): 0.026*\"apartment\" + 0.016*\"like\" + 0.014*\"office\" + 0.014*\"street\" + 0.014*\"california\" + 0.014*\"home\" + 0.013*\"hotel\" + 0.013*\"houston\" + 0.012*\"york\" + 0.011*\"utility\"\n",
      "2020-11-19 01:06:50.039 INFO    gensim.models.ldamodel: topic #4 (0.050): 0.104*\"scope\" + 0.092*\"like\" + 0.057*\"intensity\" + 0.031*\"landlord\" + 0.029*\"reduction\" + 0.026*\"control\" + 0.026*\"electricity\" + 0.024*\"metric\" + 0.016*\"mtco\" + 0.015*\"baseline\"\n",
      "2020-11-19 01:06:50.040 INFO    gensim.models.ldamodel: topic #0 (0.050): 0.034*\"electricity\" + 0.023*\"use\" + 0.018*\"natural\" + 0.018*\"source\" + 0.017*\"fuel\" + 0.016*\"travel\" + 0.016*\"office\" + 0.016*\"page\" + 0.015*\"factor\" + 0.014*\"greenhouse\"\n",
      "2020-11-19 01:06:50.046 INFO    gensim.models.ldamodel: topic #14 (0.050): 0.034*\"hall\" + 0.034*\"office\" + 0.033*\"charter\" + 0.030*\"square\" + 0.021*\"foot\" + 0.019*\"street\" + 0.016*\"fund\" + 0.014*\"june\" + 0.014*\"july\" + 0.012*\"retail\"\n",
      "2020-11-19 01:06:50.048 INFO    gensim.models.ldamodel: topic #11 (0.050): 0.045*\"recycling\" + 0.044*\"recycle\" + 0.031*\"landfill\" + 0.025*\"material\" + 0.021*\"site\" + 0.020*\"construction\" + 0.020*\"street\" + 0.014*\"use\" + 0.014*\"compost\" + 0.013*\"paper\"\n",
      "2020-11-19 01:06:50.050 INFO    gensim.models.ldamodel: topic diff=0.168335, rho=0.222134\n",
      "2020-11-19 01:06:50.051 INFO    gensim.models.ldamulticore: PROGRESS: pass 9, dispatched chunk #7 = documents up to #16000/20532, outstanding queue size 6\n",
      "2020-11-19 01:06:52.259 INFO    gensim.models.ldamulticore: PROGRESS: pass 9, dispatched chunk #8 = documents up to #18000/20532, outstanding queue size 6\n",
      "2020-11-19 01:06:52.477 INFO    gensim.models.ldamodel: merging changes from 4000 documents into a model of 20532 documents\n",
      "2020-11-19 01:06:52.514 INFO    gensim.models.ldamodel: topic #17 (0.050): 0.040*\"assurance\" + 0.022*\"information\" + 0.019*\"verification\" + 0.019*\"statement\" + 0.017*\"review\" + 0.015*\"meter\" + 0.013*\"level\" + 0.012*\"deloitte\" + 0.011*\"limited\" + 0.010*\"perform\"\n",
      "2020-11-19 01:06:52.531 INFO    gensim.models.ldamodel: topic #15 (0.050): 0.042*\"board\" + 0.027*\"committee\" + 0.023*\"director\" + 0.021*\"policy\" + 0.021*\"executive\" + 0.014*\"conduct\" + 0.014*\"code\" + 0.012*\"ethic\" + 0.012*\"human\" + 0.011*\"member\"\n",
      "2020-11-19 01:06:52.534 INFO    gensim.models.ldamodel: topic #3 (0.050): 0.044*\"goal\" + 0.035*\"target\" + 0.035*\"reduction\" + 0.032*\"achieve\" + 0.025*\"efficiency\" + 0.023*\"progress\" + 0.017*\"sustainable\" + 0.017*\"commitment\" + 0.010*\"baseline\" + 0.010*\"track\"\n",
      "2020-11-19 01:06:52.538 INFO    gensim.models.ldamodel: topic #13 (0.050): 0.038*\"real\" + 0.034*\"estate\" + 0.028*\"green\" + 0.022*\"realty\" + 0.018*\"award\" + 0.015*\"industry\" + 0.013*\"global\" + 0.013*\"good\" + 0.012*\"sustainable\" + 0.011*\"practice\"\n",
      "2020-11-19 01:06:52.545 INFO    gensim.models.ldamodel: topic #10 (0.050): 0.024*\"apartment\" + 0.016*\"like\" + 0.014*\"office\" + 0.014*\"street\" + 0.014*\"york\" + 0.014*\"california\" + 0.013*\"home\" + 0.012*\"hotel\" + 0.012*\"francisco\" + 0.012*\"houston\"\n",
      "2020-11-19 01:06:52.558 INFO    gensim.models.ldamodel: topic diff=0.159896, rho=0.222134\n",
      "2020-11-19 01:06:52.564 INFO    gensim.models.ldamulticore: PROGRESS: pass 9, dispatched chunk #9 = documents up to #20000/20532, outstanding queue size 6\n",
      "2020-11-19 01:06:54.974 INFO    gensim.models.ldamulticore: PROGRESS: pass 9, dispatched chunk #10 = documents up to #20532/20532, outstanding queue size 6\n",
      "2020-11-19 01:06:55.435 INFO    gensim.models.ldamodel: merging changes from 4000 documents into a model of 20532 documents\n",
      "2020-11-19 01:06:55.474 INFO    gensim.models.ldamodel: topic #2 (0.050): 0.047*\"centre\" + 0.044*\"hammerson\" + 0.035*\"retail\" + 0.033*\"shopping\" + 0.030*\"number\" + 0.029*\"park\" + 0.027*\"france\" + 0.018*\"positive\" + 0.016*\"tonne\" + 0.014*\"employment\"\n",
      "2020-11-19 01:06:55.481 INFO    gensim.models.ldamodel: topic #5 (0.050): 0.057*\"center\" + 0.015*\"simon\" + 0.014*\"wellness\" + 0.013*\"percent\" + 0.010*\"healthy\" + 0.009*\"health\" + 0.009*\"tree\" + 0.008*\"mall\" + 0.008*\"design\" + 0.008*\"say\"\n",
      "2020-11-19 01:06:55.483 INFO    gensim.models.ldamodel: topic #11 (0.050): 0.045*\"recycle\" + 0.039*\"recycling\" + 0.035*\"landfill\" + 0.028*\"material\" + 0.025*\"site\" + 0.024*\"construction\" + 0.016*\"use\" + 0.013*\"street\" + 0.012*\"hazardous\" + 0.012*\"divert\"\n",
      "2020-11-19 01:06:55.485 INFO    gensim.models.ldamodel: topic #10 (0.050): 0.035*\"hotel\" + 0.018*\"apartment\" + 0.016*\"park\" + 0.014*\"california\" + 0.013*\"francisco\" + 0.013*\"resort\" + 0.013*\"office\" + 0.012*\"like\" + 0.012*\"york\" + 0.012*\"home\"\n",
      "2020-11-19 01:06:55.536 INFO    gensim.models.ldamodel: topic #9 (0.050): 0.037*\"health\" + 0.036*\"training\" + 0.034*\"safety\" + 0.012*\"manager\" + 0.010*\"injury\" + 0.010*\"survey\" + 0.010*\"associate\" + 0.010*\"skill\" + 0.009*\"hour\" + 0.009*\"time\"\n",
      "2020-11-19 01:06:55.538 INFO    gensim.models.ldamodel: topic diff=0.119937, rho=0.222134\n",
      "2020-11-19 01:06:56.322 INFO    gensim.models.ldamodel: -7.696 per-word bound, 207.3 perplexity estimate based on a held-out corpus of 532 documents with 23103 words\n",
      "2020-11-19 01:06:57.685 INFO    gensim.models.ldamodel: merging changes from 4000 documents into a model of 20532 documents\n",
      "2020-11-19 01:06:57.697 INFO    gensim.models.ldamodel: topic #3 (0.050): 0.046*\"goal\" + 0.034*\"reduction\" + 0.031*\"target\" + 0.030*\"achieve\" + 0.027*\"efficiency\" + 0.021*\"progress\" + 0.018*\"sustainable\" + 0.016*\"commitment\" + 0.012*\"track\" + 0.011*\"initiative\"\n",
      "2020-11-19 01:06:57.699 INFO    gensim.models.ldamodel: topic #9 (0.050): 0.036*\"health\" + 0.036*\"training\" + 0.033*\"safety\" + 0.011*\"manager\" + 0.011*\"associate\" + 0.010*\"hour\" + 0.010*\"education\" + 0.010*\"survey\" + 0.010*\"time\" + 0.009*\"injury\"\n",
      "2020-11-19 01:06:57.701 INFO    gensim.models.ldamodel: topic #17 (0.050): 0.051*\"assurance\" + 0.026*\"information\" + 0.021*\"statement\" + 0.018*\"verification\" + 0.018*\"review\" + 0.014*\"level\" + 0.012*\"independent\" + 0.011*\"perform\" + 0.011*\"meter\" + 0.010*\"deloitte\"\n",
      "2020-11-19 01:06:57.703 INFO    gensim.models.ldamodel: topic #16 (0.050): 0.022*\"solar\" + 0.017*\"lighting\" + 0.013*\"system\" + 0.012*\"efficiency\" + 0.012*\"instal\" + 0.012*\"efficient\" + 0.012*\"saving\" + 0.012*\"power\" + 0.011*\"roof\" + 0.010*\"installation\"\n",
      "2020-11-19 01:06:57.705 INFO    gensim.models.ldamodel: topic #5 (0.050): 0.056*\"center\" + 0.015*\"wellness\" + 0.012*\"percent\" + 0.011*\"simon\" + 0.011*\"healthy\" + 0.010*\"health\" + 0.009*\"tree\" + 0.008*\"station\" + 0.008*\"design\" + 0.007*\"class\"\n",
      "2020-11-19 01:06:57.708 INFO    gensim.models.ldamodel: topic diff=0.125975, rho=0.222134\n",
      "2020-11-19 01:06:58.342 INFO    gensim.models.ldamodel: -7.732 per-word bound, 212.5 perplexity estimate based on a held-out corpus of 532 documents with 23103 words\n",
      "2020-11-19 01:06:59.387 INFO    gensim.models.ldamodel: merging changes from 4000 documents into a model of 20532 documents\n",
      "2020-11-19 01:06:59.412 INFO    gensim.models.ldamodel: topic #11 (0.050): 0.050*\"recycle\" + 0.046*\"recycling\" + 0.033*\"landfill\" + 0.029*\"material\" + 0.027*\"construction\" + 0.022*\"site\" + 0.016*\"use\" + 0.013*\"rate\" + 0.013*\"reuse\" + 0.012*\"compost\"\n",
      "2020-11-19 01:06:59.414 INFO    gensim.models.ldamodel: topic #1 (0.050): 0.027*\"regency\" + 0.019*\"million\" + 0.019*\"investment\" + 0.015*\"market\" + 0.013*\"share\" + 0.013*\"center\" + 0.011*\"stewardship\" + 0.011*\"lease\" + 0.010*\"operation\" + 0.009*\"revenue\"\n",
      "2020-11-19 01:06:59.419 INFO    gensim.models.ldamodel: topic #18 (0.050): 0.067*\"risk\" + 0.037*\"climate\" + 0.013*\"relate\" + 0.013*\"investment\" + 0.012*\"process\" + 0.011*\"identify\" + 0.011*\"strategy\" + 0.009*\"increase\" + 0.009*\"financial\" + 0.009*\"response\"\n",
      "2020-11-19 01:06:59.421 INFO    gensim.models.ldamodel: topic #15 (0.050): 0.037*\"board\" + 0.028*\"committee\" + 0.024*\"director\" + 0.021*\"policy\" + 0.020*\"executive\" + 0.016*\"conduct\" + 0.014*\"code\" + 0.013*\"ethic\" + 0.012*\"officer\" + 0.011*\"human\"\n",
      "2020-11-19 01:06:59.428 INFO    gensim.models.ldamodel: topic #19 (0.050): 0.021*\"local\" + 0.016*\"volunteer\" + 0.014*\"organization\" + 0.013*\"event\" + 0.012*\"resident\" + 0.012*\"school\" + 0.012*\"foundation\" + 0.011*\"time\" + 0.011*\"help\" + 0.011*\"partner\"\n",
      "2020-11-19 01:06:59.434 INFO    gensim.models.ldamodel: topic diff=0.123641, rho=0.222134\n",
      "2020-11-19 01:06:59.884 INFO    gensim.models.ldamodel: -7.750 per-word bound, 215.3 perplexity estimate based on a held-out corpus of 532 documents with 23103 words\n",
      "2020-11-19 01:06:59.896 INFO    gensim.models.ldamodel: merging changes from 532 documents into a model of 20532 documents\n",
      "2020-11-19 01:06:59.907 INFO    gensim.models.ldamodel: topic #11 (0.050): 0.052*\"recycle\" + 0.050*\"recycling\" + 0.032*\"landfill\" + 0.031*\"material\" + 0.023*\"construction\" + 0.020*\"site\" + 0.018*\"compost\" + 0.017*\"paper\" + 0.015*\"use\" + 0.012*\"rate\"\n",
      "2020-11-19 01:06:59.912 INFO    gensim.models.ldamodel: topic #0 (0.050): 0.037*\"electricity\" + 0.023*\"use\" + 0.021*\"natural\" + 0.021*\"fuel\" + 0.020*\"source\" + 0.016*\"travel\" + 0.016*\"factor\" + 0.016*\"renewable\" + 0.015*\"greenhouse\" + 0.014*\"office\"\n",
      "2020-11-19 01:06:59.915 INFO    gensim.models.ldamodel: topic #12 (0.050): 0.064*\"leed\" + 0.046*\"certification\" + 0.028*\"certify\" + 0.027*\"star\" + 0.026*\"gold\" + 0.025*\"green\" + 0.018*\"exist\" + 0.016*\"design\" + 0.013*\"silver\" + 0.013*\"process\"\n",
      "2020-11-19 01:06:59.919 INFO    gensim.models.ldamodel: topic #6 (0.050): 0.028*\"outlet\" + 0.026*\"cost\" + 0.018*\"carey\" + 0.015*\"increase\" + 0.013*\"centers\" + 0.012*\"value\" + 0.011*\"good\" + 0.011*\"factory\" + 0.010*\"people\" + 0.010*\"do\"\n",
      "2020-11-19 01:06:59.926 INFO    gensim.models.ldamodel: topic #2 (0.050): 0.046*\"centre\" + 0.037*\"hammerson\" + 0.034*\"retail\" + 0.032*\"shopping\" + 0.032*\"number\" + 0.026*\"park\" + 0.023*\"france\" + 0.018*\"female\" + 0.017*\"positive\" + 0.017*\"male\"\n",
      "2020-11-19 01:06:59.931 INFO    gensim.models.ldamodel: topic diff=0.153286, rho=0.222134\n",
      "2020-11-19 01:07:00.423 INFO    gensim.models.ldamodel: -7.586 per-word bound, 192.2 perplexity estimate based on a held-out corpus of 532 documents with 23103 words\n"
     ]
    }
   ],
   "source": [
    "num_topics = 20\n",
    "lda_model =  LdaMulticore(\n",
    "    corpus=bow_corpus, \n",
    "    num_topics = num_topics, \n",
    "    id2word = dictionary,                                    \n",
    "    passes = 10,\n",
    "    workers = 2\n",
    "    )"
   ]
  },
  {
   "source": [
    "## 2.2 Evaluate topics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "lda_topics_d = {}\n",
    "lda_topics_l = []\n",
    "for i in range(num_topics):\n",
    "    t = [w[0] for w in lda_model.show_topic(i)[0:10]]\n",
    "    lda_topics_d[i+1] = t\n",
    "    lda_topics_l.append(t)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              0              1             2             3             4  \\\n",
       "1   electricity            use       natural          fuel        source   \n",
       "2       regency        million    investment         share        market   \n",
       "3        centre      hammerson        retail      shopping        number   \n",
       "4          goal      reduction       achieve    efficiency        target   \n",
       "5         scope           like     intensity        metric     reduction   \n",
       "6        center          simon      wellness      shopping          mall   \n",
       "7        outlet           cost         carey      increase       centers   \n",
       "8          page       approach    disclosure  organization      material   \n",
       "9         issue       supplier        design          plan         local   \n",
       "10     training         health        safety     associate       manager   \n",
       "11    apartment     california          home       houston         hotel   \n",
       "12      recycle      recycling      landfill      material  construction   \n",
       "13         leed  certification       certify          star          gold   \n",
       "14         real         estate         green        realty         award   \n",
       "15       square           foot        office          hall        street   \n",
       "16        board      committee      director        policy     executive   \n",
       "17        solar       lighting        instal          roof        system   \n",
       "18    assurance   verification   information     statement        review   \n",
       "19         risk        climate        relate       process      identify   \n",
       "20        local      volunteer  organization          time        school   \n",
       "\n",
       "            5            6             7            8          9  \n",
       "1      travel       factor     renewable   greenhouse     office  \n",
       "2      center       income         lease  shareholder    venture  \n",
       "3        park       france        female     positive       male  \n",
       "4    progress  sustainable    commitment   greenhouse      track  \n",
       "5     control         mtco   electricity     landlord   coverage  \n",
       "6        tree      station       healthy       health        say  \n",
       "7       value         good       factory       people         do  \n",
       "8   reporting      content         topic       aspect  indicator  \n",
       "9      target      develop      strategy       review     engage  \n",
       "10       time   leadership  professional    education       hour  \n",
       "11       york       street     francisco     resident     office  \n",
       "12       site      compost         paper          use       rate  \n",
       "13      green        exist        design       silver    process  \n",
       "14   industry       global   sustainable         good   practice  \n",
       "15    million      charter       benefit         plan      value  \n",
       "16    conduct        ethic          code      officer     member  \n",
       "17      power    efficient        saving   efficiency      light  \n",
       "18   cventure        level       limited      perform      meter  \n",
       "19   strategy   investment  organization       assess   response  \n",
       "20      event      partner    foundation         help   resident  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>electricity</td>\n      <td>use</td>\n      <td>natural</td>\n      <td>fuel</td>\n      <td>source</td>\n      <td>travel</td>\n      <td>factor</td>\n      <td>renewable</td>\n      <td>greenhouse</td>\n      <td>office</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>regency</td>\n      <td>million</td>\n      <td>investment</td>\n      <td>share</td>\n      <td>market</td>\n      <td>center</td>\n      <td>income</td>\n      <td>lease</td>\n      <td>shareholder</td>\n      <td>venture</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>centre</td>\n      <td>hammerson</td>\n      <td>retail</td>\n      <td>shopping</td>\n      <td>number</td>\n      <td>park</td>\n      <td>france</td>\n      <td>female</td>\n      <td>positive</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>goal</td>\n      <td>reduction</td>\n      <td>achieve</td>\n      <td>efficiency</td>\n      <td>target</td>\n      <td>progress</td>\n      <td>sustainable</td>\n      <td>commitment</td>\n      <td>greenhouse</td>\n      <td>track</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>scope</td>\n      <td>like</td>\n      <td>intensity</td>\n      <td>metric</td>\n      <td>reduction</td>\n      <td>control</td>\n      <td>mtco</td>\n      <td>electricity</td>\n      <td>landlord</td>\n      <td>coverage</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>center</td>\n      <td>simon</td>\n      <td>wellness</td>\n      <td>shopping</td>\n      <td>mall</td>\n      <td>tree</td>\n      <td>station</td>\n      <td>healthy</td>\n      <td>health</td>\n      <td>say</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>outlet</td>\n      <td>cost</td>\n      <td>carey</td>\n      <td>increase</td>\n      <td>centers</td>\n      <td>value</td>\n      <td>good</td>\n      <td>factory</td>\n      <td>people</td>\n      <td>do</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>page</td>\n      <td>approach</td>\n      <td>disclosure</td>\n      <td>organization</td>\n      <td>material</td>\n      <td>reporting</td>\n      <td>content</td>\n      <td>topic</td>\n      <td>aspect</td>\n      <td>indicator</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>issue</td>\n      <td>supplier</td>\n      <td>design</td>\n      <td>plan</td>\n      <td>local</td>\n      <td>target</td>\n      <td>develop</td>\n      <td>strategy</td>\n      <td>review</td>\n      <td>engage</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>training</td>\n      <td>health</td>\n      <td>safety</td>\n      <td>associate</td>\n      <td>manager</td>\n      <td>time</td>\n      <td>leadership</td>\n      <td>professional</td>\n      <td>education</td>\n      <td>hour</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>apartment</td>\n      <td>california</td>\n      <td>home</td>\n      <td>houston</td>\n      <td>hotel</td>\n      <td>york</td>\n      <td>street</td>\n      <td>francisco</td>\n      <td>resident</td>\n      <td>office</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>recycle</td>\n      <td>recycling</td>\n      <td>landfill</td>\n      <td>material</td>\n      <td>construction</td>\n      <td>site</td>\n      <td>compost</td>\n      <td>paper</td>\n      <td>use</td>\n      <td>rate</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>leed</td>\n      <td>certification</td>\n      <td>certify</td>\n      <td>star</td>\n      <td>gold</td>\n      <td>green</td>\n      <td>exist</td>\n      <td>design</td>\n      <td>silver</td>\n      <td>process</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>real</td>\n      <td>estate</td>\n      <td>green</td>\n      <td>realty</td>\n      <td>award</td>\n      <td>industry</td>\n      <td>global</td>\n      <td>sustainable</td>\n      <td>good</td>\n      <td>practice</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>square</td>\n      <td>foot</td>\n      <td>office</td>\n      <td>hall</td>\n      <td>street</td>\n      <td>million</td>\n      <td>charter</td>\n      <td>benefit</td>\n      <td>plan</td>\n      <td>value</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>board</td>\n      <td>committee</td>\n      <td>director</td>\n      <td>policy</td>\n      <td>executive</td>\n      <td>conduct</td>\n      <td>ethic</td>\n      <td>code</td>\n      <td>officer</td>\n      <td>member</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>solar</td>\n      <td>lighting</td>\n      <td>instal</td>\n      <td>roof</td>\n      <td>system</td>\n      <td>power</td>\n      <td>efficient</td>\n      <td>saving</td>\n      <td>efficiency</td>\n      <td>light</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>assurance</td>\n      <td>verification</td>\n      <td>information</td>\n      <td>statement</td>\n      <td>review</td>\n      <td>cventure</td>\n      <td>level</td>\n      <td>limited</td>\n      <td>perform</td>\n      <td>meter</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>risk</td>\n      <td>climate</td>\n      <td>relate</td>\n      <td>process</td>\n      <td>identify</td>\n      <td>strategy</td>\n      <td>investment</td>\n      <td>organization</td>\n      <td>assess</td>\n      <td>response</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>local</td>\n      <td>volunteer</td>\n      <td>organization</td>\n      <td>time</td>\n      <td>school</td>\n      <td>event</td>\n      <td>partner</td>\n      <td>foundation</td>\n      <td>help</td>\n      <td>resident</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "# show topics\n",
    "# evert row is a topic\n",
    "(pd.DataFrame.from_dict(lda_topics_d).T).to_csv(\"output/topics_CRS_LDA.csv\")\n",
    "pd.DataFrame.from_dict(lda_topics_d).T"
   ]
  },
  {
   "source": [
    "### 2.2.1 Normalized Point-wise Mutual Information"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "oherence.text_analysis: 189 batches submitted to accumulate stats from 12096 documents (546687 virtual)\n",
      "2020-11-19 01:07:16.870 INFO    gensim.topic_coherence.text_analysis: 190 batches submitted to accumulate stats from 12160 documents (550014 virtual)\n",
      "2020-11-19 01:07:16.915 INFO    gensim.topic_coherence.text_analysis: 191 batches submitted to accumulate stats from 12224 documents (552882 virtual)\n",
      "2020-11-19 01:07:16.992 INFO    gensim.topic_coherence.text_analysis: 192 batches submitted to accumulate stats from 12288 documents (555907 virtual)\n",
      "2020-11-19 01:07:17.015 INFO    gensim.topic_coherence.text_analysis: 193 batches submitted to accumulate stats from 12352 documents (559527 virtual)\n",
      "2020-11-19 01:07:17.046 INFO    gensim.topic_coherence.text_analysis: 194 batches submitted to accumulate stats from 12416 documents (562619 virtual)\n",
      "2020-11-19 01:07:17.135 INFO    gensim.topic_coherence.text_analysis: 195 batches submitted to accumulate stats from 12480 documents (569123 virtual)\n",
      "2020-11-19 01:07:17.159 INFO    gensim.topic_coherence.text_analysis: 196 batches submitted to accumulate stats from 12544 documents (572373 virtual)\n",
      "2020-11-19 01:07:17.172 INFO    gensim.topic_coherence.text_analysis: 197 batches submitted to accumulate stats from 12608 documents (575673 virtual)\n",
      "2020-11-19 01:07:17.259 INFO    gensim.topic_coherence.text_analysis: 198 batches submitted to accumulate stats from 12672 documents (579013 virtual)\n",
      "2020-11-19 01:07:17.288 INFO    gensim.topic_coherence.text_analysis: 199 batches submitted to accumulate stats from 12736 documents (582283 virtual)\n",
      "2020-11-19 01:07:17.309 INFO    gensim.topic_coherence.text_analysis: 200 batches submitted to accumulate stats from 12800 documents (586140 virtual)\n",
      "2020-11-19 01:07:17.414 INFO    gensim.topic_coherence.text_analysis: 201 batches submitted to accumulate stats from 12864 documents (589934 virtual)\n",
      "2020-11-19 01:07:17.436 INFO    gensim.topic_coherence.text_analysis: 202 batches submitted to accumulate stats from 12928 documents (593795 virtual)\n",
      "2020-11-19 01:07:17.469 INFO    gensim.topic_coherence.text_analysis: 203 batches submitted to accumulate stats from 12992 documents (597695 virtual)\n",
      "2020-11-19 01:07:17.559 INFO    gensim.topic_coherence.text_analysis: 204 batches submitted to accumulate stats from 13056 documents (601687 virtual)\n",
      "2020-11-19 01:07:17.571 INFO    gensim.topic_coherence.text_analysis: 205 batches submitted to accumulate stats from 13120 documents (604513 virtual)\n",
      "2020-11-19 01:07:17.618 INFO    gensim.topic_coherence.text_analysis: 206 batches submitted to accumulate stats from 13184 documents (607501 virtual)\n",
      "2020-11-19 01:07:17.688 INFO    gensim.topic_coherence.text_analysis: 207 batches submitted to accumulate stats from 13248 documents (610211 virtual)\n",
      "2020-11-19 01:07:17.722 INFO    gensim.topic_coherence.text_analysis: 208 batches submitted to accumulate stats from 13312 documents (613135 virtual)\n",
      "2020-11-19 01:07:17.774 INFO    gensim.topic_coherence.text_analysis: 209 batches submitted to accumulate stats from 13376 documents (615989 virtual)\n",
      "2020-11-19 01:07:17.861 INFO    gensim.topic_coherence.text_analysis: 210 batches submitted to accumulate stats from 13440 documents (618666 virtual)\n",
      "2020-11-19 01:07:17.876 INFO    gensim.topic_coherence.text_analysis: 211 batches submitted to accumulate stats from 13504 documents (621307 virtual)\n",
      "2020-11-19 01:07:17.912 INFO    gensim.topic_coherence.text_analysis: 212 batches submitted to accumulate stats from 13568 documents (624271 virtual)\n",
      "2020-11-19 01:07:17.974 INFO    gensim.topic_coherence.text_analysis: 213 batches submitted to accumulate stats from 13632 documents (627153 virtual)\n",
      "2020-11-19 01:07:18.001 INFO    gensim.topic_coherence.text_analysis: 214 batches submitted to accumulate stats from 13696 documents (630279 virtual)\n",
      "2020-11-19 01:07:18.019 INFO    gensim.topic_coherence.text_analysis: 215 batches submitted to accumulate stats from 13760 documents (632758 virtual)\n",
      "2020-11-19 01:07:18.092 INFO    gensim.topic_coherence.text_analysis: 216 batches submitted to accumulate stats from 13824 documents (635544 virtual)\n",
      "2020-11-19 01:07:18.127 INFO    gensim.topic_coherence.text_analysis: 217 batches submitted to accumulate stats from 13888 documents (639139 virtual)\n",
      "2020-11-19 01:07:18.148 INFO    gensim.topic_coherence.text_analysis: 218 batches submitted to accumulate stats from 13952 documents (641819 virtual)\n",
      "2020-11-19 01:07:18.213 INFO    gensim.topic_coherence.text_analysis: 219 batches submitted to accumulate stats from 14016 documents (644126 virtual)\n",
      "2020-11-19 01:07:18.241 INFO    gensim.topic_coherence.text_analysis: 220 batches submitted to accumulate stats from 14080 documents (646638 virtual)\n",
      "2020-11-19 01:07:18.271 INFO    gensim.topic_coherence.text_analysis: 221 batches submitted to accumulate stats from 14144 documents (649933 virtual)\n",
      "2020-11-19 01:07:18.329 INFO    gensim.topic_coherence.text_analysis: 222 batches submitted to accumulate stats from 14208 documents (653236 virtual)\n",
      "2020-11-19 01:07:18.381 INFO    gensim.topic_coherence.text_analysis: 223 batches submitted to accumulate stats from 14272 documents (656358 virtual)\n",
      "2020-11-19 01:07:18.386 INFO    gensim.topic_coherence.text_analysis: 224 batches submitted to accumulate stats from 14336 documents (658725 virtual)\n",
      "2020-11-19 01:07:18.414 INFO    gensim.topic_coherence.text_analysis: 225 batches submitted to accumulate stats from 14400 documents (661245 virtual)\n",
      "2020-11-19 01:07:18.481 INFO    gensim.topic_coherence.text_analysis: 226 batches submitted to accumulate stats from 14464 documents (663745 virtual)\n",
      "2020-11-19 01:07:18.499 INFO    gensim.topic_coherence.text_analysis: 227 batches submitted to accumulate stats from 14528 documents (666409 virtual)\n",
      "2020-11-19 01:07:18.554 INFO    gensim.topic_coherence.text_analysis: 228 batches submitted to accumulate stats from 14592 documents (669476 virtual)\n",
      "2020-11-19 01:07:18.589 INFO    gensim.topic_coherence.text_analysis: 229 batches submitted to accumulate stats from 14656 documents (672117 virtual)\n",
      "2020-11-19 01:07:18.605 INFO    gensim.topic_coherence.text_analysis: 230 batches submitted to accumulate stats from 14720 documents (674788 virtual)\n",
      "2020-11-19 01:07:18.664 INFO    gensim.topic_coherence.text_analysis: 231 batches submitted to accumulate stats from 14784 documents (678040 virtual)\n",
      "2020-11-19 01:07:18.675 INFO    gensim.topic_coherence.text_analysis: 232 batches submitted to accumulate stats from 14848 documents (681344 virtual)\n",
      "2020-11-19 01:07:18.724 INFO    gensim.topic_coherence.text_analysis: 233 batches submitted to accumulate stats from 14912 documents (684527 virtual)\n",
      "2020-11-19 01:07:18.767 INFO    gensim.topic_coherence.text_analysis: 234 batches submitted to accumulate stats from 14976 documents (687830 virtual)\n",
      "2020-11-19 01:07:18.776 INFO    gensim.topic_coherence.text_analysis: 235 batches submitted to accumulate stats from 15040 documents (691220 virtual)\n",
      "2020-11-19 01:07:18.830 INFO    gensim.topic_coherence.text_analysis: 236 batches submitted to accumulate stats from 15104 documents (694346 virtual)\n",
      "2020-11-19 01:07:18.894 INFO    gensim.topic_coherence.text_analysis: 237 batches submitted to accumulate stats from 15168 documents (697091 virtual)\n",
      "2020-11-19 01:07:18.916 INFO    gensim.topic_coherence.text_analysis: 238 batches submitted to accumulate stats from 15232 documents (699799 virtual)\n",
      "2020-11-19 01:07:18.992 INFO    gensim.topic_coherence.text_analysis: 239 batches submitted to accumulate stats from 15296 documents (702736 virtual)\n",
      "2020-11-19 01:07:19.068 INFO    gensim.topic_coherence.text_analysis: 240 batches submitted to accumulate stats from 15360 documents (705348 virtual)\n",
      "2020-11-19 01:07:19.098 INFO    gensim.topic_coherence.text_analysis: 241 batches submitted to accumulate stats from 15424 documents (707835 virtual)\n",
      "2020-11-19 01:07:19.131 INFO    gensim.topic_coherence.text_analysis: 242 batches submitted to accumulate stats from 15488 documents (710923 virtual)\n",
      "2020-11-19 01:07:19.183 INFO    gensim.topic_coherence.text_analysis: 243 batches submitted to accumulate stats from 15552 documents (713430 virtual)\n",
      "2020-11-19 01:07:19.215 INFO    gensim.topic_coherence.text_analysis: 244 batches submitted to accumulate stats from 15616 documents (716469 virtual)\n",
      "2020-11-19 01:07:19.271 INFO    gensim.topic_coherence.text_analysis: 245 batches submitted to accumulate stats from 15680 documents (719004 virtual)\n",
      "2020-11-19 01:07:19.311 INFO    gensim.topic_coherence.text_analysis: 246 batches submitted to accumulate stats from 15744 documents (722205 virtual)\n",
      "2020-11-19 01:07:19.343 INFO    gensim.topic_coherence.text_analysis: 247 batches submitted to accumulate stats from 15808 documents (725150 virtual)\n",
      "2020-11-19 01:07:19.408 INFO    gensim.topic_coherence.text_analysis: 248 batches submitted to accumulate stats from 15872 documents (728415 virtual)\n",
      "2020-11-19 01:07:19.411 INFO    gensim.topic_coherence.text_analysis: 249 batches submitted to accumulate stats from 15936 documents (730994 virtual)\n",
      "2020-11-19 01:07:19.470 INFO    gensim.topic_coherence.text_analysis: 250 batches submitted to accumulate stats from 16000 documents (733926 virtual)\n",
      "2020-11-19 01:07:19.509 INFO    gensim.topic_coherence.text_analysis: 251 batches submitted to accumulate stats from 16064 documents (737124 virtual)\n",
      "2020-11-19 01:07:19.534 INFO    gensim.topic_coherence.text_analysis: 252 batches submitted to accumulate stats from 16128 documents (739498 virtual)\n",
      "2020-11-19 01:07:19.607 INFO    gensim.topic_coherence.text_analysis: 253 batches submitted to accumulate stats from 16192 documents (743000 virtual)\n",
      "2020-11-19 01:07:19.623 INFO    gensim.topic_coherence.text_analysis: 254 batches submitted to accumulate stats from 16256 documents (745379 virtual)\n",
      "2020-11-19 01:07:19.674 INFO    gensim.topic_coherence.text_analysis: 255 batches submitted to accumulate stats from 16320 documents (748140 virtual)\n",
      "2020-11-19 01:07:19.756 INFO    gensim.topic_coherence.text_analysis: 256 batches submitted to accumulate stats from 16384 documents (751696 virtual)\n",
      "2020-11-19 01:07:19.773 INFO    gensim.topic_coherence.text_analysis: 257 batches submitted to accumulate stats from 16448 documents (754321 virtual)\n",
      "2020-11-19 01:07:19.801 INFO    gensim.topic_coherence.text_analysis: 258 batches submitted to accumulate stats from 16512 documents (757594 virtual)\n",
      "2020-11-19 01:07:19.874 INFO    gensim.topic_coherence.text_analysis: 259 batches submitted to accumulate stats from 16576 documents (760987 virtual)\n",
      "2020-11-19 01:07:19.898 INFO    gensim.topic_coherence.text_analysis: 260 batches submitted to accumulate stats from 16640 documents (764553 virtual)\n",
      "2020-11-19 01:07:19.917 INFO    gensim.topic_coherence.text_analysis: 261 batches submitted to accumulate stats from 16704 documents (767868 virtual)\n",
      "2020-11-19 01:07:20.000 INFO    gensim.topic_coherence.text_analysis: 262 batches submitted to accumulate stats from 16768 documents (771044 virtual)\n",
      "2020-11-19 01:07:20.048 INFO    gensim.topic_coherence.text_analysis: 263 batches submitted to accumulate stats from 16832 documents (773856 virtual)\n",
      "2020-11-19 01:07:20.060 INFO    gensim.topic_coherence.text_analysis: 264 batches submitted to accumulate stats from 16896 documents (776667 virtual)\n",
      "2020-11-19 01:07:20.170 INFO    gensim.topic_coherence.text_analysis: 265 batches submitted to accumulate stats from 16960 documents (779563 virtual)\n",
      "2020-11-19 01:07:20.194 INFO    gensim.topic_coherence.text_analysis: 266 batches submitted to accumulate stats from 17024 documents (782309 virtual)\n",
      "2020-11-19 01:07:20.233 INFO    gensim.topic_coherence.text_analysis: 267 batches submitted to accumulate stats from 17088 documents (785752 virtual)\n",
      "2020-11-19 01:07:20.321 INFO    gensim.topic_coherence.text_analysis: 268 batches submitted to accumulate stats from 17152 documents (788543 virtual)\n",
      "2020-11-19 01:07:20.345 INFO    gensim.topic_coherence.text_analysis: 269 batches submitted to accumulate stats from 17216 documents (791473 virtual)\n",
      "2020-11-19 01:07:20.355 INFO    gensim.topic_coherence.text_analysis: 270 batches submitted to accumulate stats from 17280 documents (794367 virtual)\n",
      "2020-11-19 01:07:20.447 INFO    gensim.topic_coherence.text_analysis: 271 batches submitted to accumulate stats from 17344 documents (797406 virtual)\n",
      "2020-11-19 01:07:20.465 INFO    gensim.topic_coherence.text_analysis: 272 batches submitted to accumulate stats from 17408 documents (800574 virtual)\n",
      "2020-11-19 01:07:20.507 INFO    gensim.topic_coherence.text_analysis: 273 batches submitted to accumulate stats from 17472 documents (803624 virtual)\n",
      "2020-11-19 01:07:20.595 INFO    gensim.topic_coherence.text_analysis: 274 batches submitted to accumulate stats from 17536 documents (806156 virtual)\n",
      "2020-11-19 01:07:20.622 INFO    gensim.topic_coherence.text_analysis: 275 batches submitted to accumulate stats from 17600 documents (808997 virtual)\n",
      "2020-11-19 01:07:20.639 INFO    gensim.topic_coherence.text_analysis: 276 batches submitted to accumulate stats from 17664 documents (812300 virtual)\n",
      "2020-11-19 01:07:20.802 INFO    gensim.topic_coherence.text_analysis: 277 batches submitted to accumulate stats from 17728 documents (815075 virtual)\n",
      "2020-11-19 01:07:20.816 INFO    gensim.topic_coherence.text_analysis: 278 batches submitted to accumulate stats from 17792 documents (818466 virtual)\n",
      "2020-11-19 01:07:20.861 INFO    gensim.topic_coherence.text_analysis: 279 batches submitted to accumulate stats from 17856 documents (821345 virtual)\n",
      "2020-11-19 01:07:20.940 INFO    gensim.topic_coherence.text_analysis: 280 batches submitted to accumulate stats from 17920 documents (824653 virtual)\n",
      "2020-11-19 01:07:20.955 INFO    gensim.topic_coherence.text_analysis: 281 batches submitted to accumulate stats from 17984 documents (827532 virtual)\n",
      "2020-11-19 01:07:21.002 INFO    gensim.topic_coherence.text_analysis: 282 batches submitted to accumulate stats from 18048 documents (831137 virtual)\n",
      "2020-11-19 01:07:21.067 INFO    gensim.topic_coherence.text_analysis: 283 batches submitted to accumulate stats from 18112 documents (833917 virtual)\n",
      "2020-11-19 01:07:21.086 INFO    gensim.topic_coherence.text_analysis: 284 batches submitted to accumulate stats from 18176 documents (837351 virtual)\n",
      "2020-11-19 01:07:21.131 INFO    gensim.topic_coherence.text_analysis: 285 batches submitted to accumulate stats from 18240 documents (840365 virtual)\n",
      "2020-11-19 01:07:21.280 INFO    gensim.topic_coherence.text_analysis: 286 batches submitted to accumulate stats from 18304 documents (843576 virtual)\n",
      "2020-11-19 01:07:21.288 INFO    gensim.topic_coherence.text_analysis: 287 batches submitted to accumulate stats from 18368 documents (846498 virtual)\n",
      "2020-11-19 01:07:21.355 INFO    gensim.topic_coherence.text_analysis: 288 batches submitted to accumulate stats from 18432 documents (849299 virtual)\n",
      "2020-11-19 01:07:21.446 INFO    gensim.topic_coherence.text_analysis: 289 batches submitted to accumulate stats from 18496 documents (852800 virtual)\n",
      "2020-11-19 01:07:21.481 INFO    gensim.topic_coherence.text_analysis: 290 batches submitted to accumulate stats from 18560 documents (856667 virtual)\n",
      "2020-11-19 01:07:21.523 INFO    gensim.topic_coherence.text_analysis: 291 batches submitted to accumulate stats from 18624 documents (860026 virtual)\n",
      "2020-11-19 01:07:21.587 INFO    gensim.topic_coherence.text_analysis: 292 batches submitted to accumulate stats from 18688 documents (863441 virtual)\n",
      "2020-11-19 01:07:21.652 INFO    gensim.topic_coherence.text_analysis: 293 batches submitted to accumulate stats from 18752 documents (866744 virtual)\n",
      "2020-11-19 01:07:21.681 INFO    gensim.topic_coherence.text_analysis: 294 batches submitted to accumulate stats from 18816 documents (870255 virtual)\n",
      "2020-11-19 01:07:21.778 INFO    gensim.topic_coherence.text_analysis: 295 batches submitted to accumulate stats from 18880 documents (873426 virtual)\n",
      "2020-11-19 01:07:21.873 INFO    gensim.topic_coherence.text_analysis: 296 batches submitted to accumulate stats from 18944 documents (876577 virtual)\n",
      "2020-11-19 01:07:21.909 INFO    gensim.topic_coherence.text_analysis: 297 batches submitted to accumulate stats from 19008 documents (879688 virtual)\n",
      "2020-11-19 01:07:21.967 INFO    gensim.topic_coherence.text_analysis: 298 batches submitted to accumulate stats from 19072 documents (882720 virtual)\n",
      "2020-11-19 01:07:22.034 INFO    gensim.topic_coherence.text_analysis: 299 batches submitted to accumulate stats from 19136 documents (885632 virtual)\n",
      "2020-11-19 01:07:22.081 INFO    gensim.topic_coherence.text_analysis: 300 batches submitted to accumulate stats from 19200 documents (888723 virtual)\n",
      "2020-11-19 01:07:22.121 INFO    gensim.topic_coherence.text_analysis: 301 batches submitted to accumulate stats from 19264 documents (891954 virtual)\n",
      "2020-11-19 01:07:22.201 INFO    gensim.topic_coherence.text_analysis: 302 batches submitted to accumulate stats from 19328 documents (895003 virtual)\n",
      "2020-11-19 01:07:22.266 INFO    gensim.topic_coherence.text_analysis: 303 batches submitted to accumulate stats from 19392 documents (898177 virtual)\n",
      "2020-11-19 01:07:22.304 INFO    gensim.topic_coherence.text_analysis: 304 batches submitted to accumulate stats from 19456 documents (901159 virtual)\n",
      "2020-11-19 01:07:22.419 INFO    gensim.topic_coherence.text_analysis: 305 batches submitted to accumulate stats from 19520 documents (904348 virtual)\n",
      "2020-11-19 01:07:22.489 INFO    gensim.topic_coherence.text_analysis: 306 batches submitted to accumulate stats from 19584 documents (907752 virtual)\n",
      "2020-11-19 01:07:22.526 INFO    gensim.topic_coherence.text_analysis: 307 batches submitted to accumulate stats from 19648 documents (910615 virtual)\n",
      "2020-11-19 01:07:22.595 INFO    gensim.topic_coherence.text_analysis: 308 batches submitted to accumulate stats from 19712 documents (913382 virtual)\n",
      "2020-11-19 01:07:22.648 INFO    gensim.topic_coherence.text_analysis: 309 batches submitted to accumulate stats from 19776 documents (916192 virtual)\n",
      "2020-11-19 01:07:22.668 INFO    gensim.topic_coherence.text_analysis: 310 batches submitted to accumulate stats from 19840 documents (918991 virtual)\n",
      "2020-11-19 01:07:22.754 INFO    gensim.topic_coherence.text_analysis: 311 batches submitted to accumulate stats from 19904 documents (921932 virtual)\n",
      "2020-11-19 01:07:22.792 INFO    gensim.topic_coherence.text_analysis: 312 batches submitted to accumulate stats from 19968 documents (924635 virtual)\n",
      "2020-11-19 01:07:22.801 INFO    gensim.topic_coherence.text_analysis: 313 batches submitted to accumulate stats from 20032 documents (927748 virtual)\n",
      "2020-11-19 01:07:22.912 INFO    gensim.topic_coherence.text_analysis: 314 batches submitted to accumulate stats from 20096 documents (930634 virtual)\n",
      "2020-11-19 01:07:22.916 INFO    gensim.topic_coherence.text_analysis: 315 batches submitted to accumulate stats from 20160 documents (934030 virtual)\n",
      "2020-11-19 01:07:22.933 INFO    gensim.topic_coherence.text_analysis: 316 batches submitted to accumulate stats from 20224 documents (936971 virtual)\n",
      "2020-11-19 01:07:23.034 INFO    gensim.topic_coherence.text_analysis: 317 batches submitted to accumulate stats from 20288 documents (940260 virtual)\n",
      "2020-11-19 01:07:23.049 INFO    gensim.topic_coherence.text_analysis: 318 batches submitted to accumulate stats from 20352 documents (942869 virtual)\n",
      "2020-11-19 01:07:23.101 INFO    gensim.topic_coherence.text_analysis: 319 batches submitted to accumulate stats from 20416 documents (946032 virtual)\n",
      "2020-11-19 01:07:23.182 INFO    gensim.topic_coherence.text_analysis: 320 batches submitted to accumulate stats from 20480 documents (946342 virtual)\n",
      "2020-11-19 01:07:23.548 INFO    gensim.topic_coherence.text_analysis: 3 accumulators retrieved from output queue\n",
      "2020-11-19 01:07:23.594 INFO    gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 946650 virtual documents\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.09224300680531514"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "npmi = CoherenceNPMI(texts=text_cleaned, topics=lda_topics_l)\n",
    "npmi.score()"
   ]
  },
  {
   "source": [
    "### 2.2.2 External Word Embeddings Topic Coherence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "2020-11-19 01:07:26.279 INFO    gensim.models.utils_any2vec: loading projection weights from C:\\Users\\Pieter-Jan/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n",
      "2020-11-19 01:08:19.617 INFO    gensim.models.utils_any2vec: loaded (3000000, 300) matrix from C:\\Users\\Pieter-Jan/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.14907268"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "CoherenceWordEmbeddings(lda_topics_l).score()"
   ]
  },
  {
   "source": [
    "### 2.2.3 Rank-Biased Overlap "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9909485593956391"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "InvertedRBO(lda_topics_l).score()"
   ]
  },
  {
   "source": [
    "## 3. Bert Topic"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.1 Train model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "2020-11-19 01:09:13,354 - BERTopic - Reduced dimensionality with UMAP\n",
      "2020-11-19 01:09:13.354 INFO    BERTopic: Reduced dimensionality with UMAP\n",
      "C:\\Users\\Pieter-Jan\\Anaconda3\\envs\\marketDb\\lib\\site-packages\\joblib\\parallel.py:324: DeprecationWarning: check_pickle is deprecated in joblib 0.12 and will be removed in 0.13\n",
      "  warnings.warn('check_pickle is deprecated in joblib 0.12 and will be'\n",
      "C:\\Users\\Pieter-Jan\\Anaconda3\\envs\\marketDb\\lib\\site-packages\\joblib\\parallel.py:324: DeprecationWarning: check_pickle is deprecated in joblib 0.12 and will be removed in 0.13\n",
      "  warnings.warn('check_pickle is deprecated in joblib 0.12 and will be'\n",
      "C:\\Users\\Pieter-Jan\\Anaconda3\\envs\\marketDb\\lib\\site-packages\\joblib\\parallel.py:324: DeprecationWarning: check_pickle is deprecated in joblib 0.12 and will be removed in 0.13\n",
      "  warnings.warn('check_pickle is deprecated in joblib 0.12 and will be'\n",
      "C:\\Users\\Pieter-Jan\\Anaconda3\\envs\\marketDb\\lib\\site-packages\\joblib\\parallel.py:324: DeprecationWarning: check_pickle is deprecated in joblib 0.12 and will be removed in 0.13\n",
      "  warnings.warn('check_pickle is deprecated in joblib 0.12 and will be'\n",
      "2020-11-19 01:09:34,877 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2020-11-19 01:09:34.877 INFO    BERTopic: Clustered UMAP embeddings with HDBSCAN\n"
     ]
    }
   ],
   "source": [
    "random.seed(69)\n",
    "\n",
    "with open(\"output/CSR_processed_cleaned_pdfMiner.txt\",\"r\") as fr:\n",
    "    docs = [doc for doc in fr.read().splitlines()] \n",
    "\n",
    "model = BERTopic(verbose=True)\n",
    "topics = model.fit_transform(docs, embeddings_bert)"
   ]
  },
  {
   "source": [
    "## 3.2 Evaluate topics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "topcis_b = model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# extract words for each topic\n",
    "topics_k = {}\n",
    "for k,v in topcis_b.items():\n",
    "    t_words = []\n",
    "    for w in v:\n",
    "        t_words.append(w[0])\n",
    "    # append the first 10 words\n",
    "    topics_k[k] = t_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              0               1               2              3             4  \\\n",
       "-1     property   environmental       community          water      building   \n",
       " 0    important       relevance        material       external  organization   \n",
       " 1      rexford          infill      reposition     renovation         value   \n",
       " 2     dialogue     stakeholder           kimco        ongoing    engagement   \n",
       " 3   alexandria     alexandrias         science           life      equities   \n",
       "..          ...             ...             ...            ...           ...   \n",
       " 80     climate            risk          change         relate      scenario   \n",
       " 81        risk          carbon         climate      reduction      increase   \n",
       " 82    investor         meeting      conference         survey     quarterly   \n",
       " 83    strategy  sustainability       hammerson       business         clear   \n",
       " 84      social     stakeholder  responsibility  environmental     corporate   \n",
       "\n",
       "               5           6               7            8           9  \n",
       "-1   consumption      energy  sustainability  performance   corporate  \n",
       " 0      supplier    resident     advancement   internally     outside  \n",
       " 1         truck     benefit      industrial      million  california  \n",
       " 2        kimcos      engage     interaction        issue       topic  \n",
       " 3        profit    critical          agtech     research      campus  \n",
       "..           ...         ...             ...          ...         ...  \n",
       " 80         tcfd     weather        physical    financial     extreme  \n",
       " 81      pricing        cost        strategy       return      relate  \n",
       " 82       broker  engagement        feedback     customer        call  \n",
       " 83     positive      mirvac     stakeholder       vision      review  \n",
       " 84   initiative  governance      commitment     business       value  \n",
       "\n",
       "[86 rows x 10 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>-1</th>\n      <td>property</td>\n      <td>environmental</td>\n      <td>community</td>\n      <td>water</td>\n      <td>building</td>\n      <td>consumption</td>\n      <td>energy</td>\n      <td>sustainability</td>\n      <td>performance</td>\n      <td>corporate</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>important</td>\n      <td>relevance</td>\n      <td>material</td>\n      <td>external</td>\n      <td>organization</td>\n      <td>supplier</td>\n      <td>resident</td>\n      <td>advancement</td>\n      <td>internally</td>\n      <td>outside</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>rexford</td>\n      <td>infill</td>\n      <td>reposition</td>\n      <td>renovation</td>\n      <td>value</td>\n      <td>truck</td>\n      <td>benefit</td>\n      <td>industrial</td>\n      <td>million</td>\n      <td>california</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>dialogue</td>\n      <td>stakeholder</td>\n      <td>kimco</td>\n      <td>ongoing</td>\n      <td>engagement</td>\n      <td>kimcos</td>\n      <td>engage</td>\n      <td>interaction</td>\n      <td>issue</td>\n      <td>topic</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>alexandria</td>\n      <td>alexandrias</td>\n      <td>science</td>\n      <td>life</td>\n      <td>equities</td>\n      <td>profit</td>\n      <td>critical</td>\n      <td>agtech</td>\n      <td>research</td>\n      <td>campus</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>climate</td>\n      <td>risk</td>\n      <td>change</td>\n      <td>relate</td>\n      <td>scenario</td>\n      <td>tcfd</td>\n      <td>weather</td>\n      <td>physical</td>\n      <td>financial</td>\n      <td>extreme</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>risk</td>\n      <td>carbon</td>\n      <td>climate</td>\n      <td>reduction</td>\n      <td>increase</td>\n      <td>pricing</td>\n      <td>cost</td>\n      <td>strategy</td>\n      <td>return</td>\n      <td>relate</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>investor</td>\n      <td>meeting</td>\n      <td>conference</td>\n      <td>survey</td>\n      <td>quarterly</td>\n      <td>broker</td>\n      <td>engagement</td>\n      <td>feedback</td>\n      <td>customer</td>\n      <td>call</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>strategy</td>\n      <td>sustainability</td>\n      <td>hammerson</td>\n      <td>business</td>\n      <td>clear</td>\n      <td>positive</td>\n      <td>mirvac</td>\n      <td>stakeholder</td>\n      <td>vision</td>\n      <td>review</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>social</td>\n      <td>stakeholder</td>\n      <td>responsibility</td>\n      <td>environmental</td>\n      <td>corporate</td>\n      <td>initiative</td>\n      <td>governance</td>\n      <td>commitment</td>\n      <td>business</td>\n      <td>value</td>\n    </tr>\n  </tbody>\n</table>\n<p>86 rows × 10 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "topics_bert = list(topics_k.values())\n",
    "(pd.DataFrame.from_dict(topics_k).T).to_csv(\"output/topics_CRS_TBERT.csv\")\n",
    "pd.DataFrame.from_dict(topics_k).T"
   ]
  },
  {
   "source": [
    "### 3.2.1 Normalized Point-wise Mutual Information"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "oherence.text_analysis: 190 batches submitted to accumulate stats from 12160 documents (547104 virtual)\n",
      "2020-11-19 01:10:10.523 INFO    gensim.topic_coherence.text_analysis: 191 batches submitted to accumulate stats from 12224 documents (550321 virtual)\n",
      "2020-11-19 01:10:10.800 INFO    gensim.topic_coherence.text_analysis: 192 batches submitted to accumulate stats from 12288 documents (553430 virtual)\n",
      "2020-11-19 01:10:10.930 INFO    gensim.topic_coherence.text_analysis: 193 batches submitted to accumulate stats from 12352 documents (556217 virtual)\n",
      "2020-11-19 01:10:10.978 INFO    gensim.topic_coherence.text_analysis: 194 batches submitted to accumulate stats from 12416 documents (559960 virtual)\n",
      "2020-11-19 01:10:11.131 INFO    gensim.topic_coherence.text_analysis: 195 batches submitted to accumulate stats from 12480 documents (563025 virtual)\n",
      "2020-11-19 01:10:11.303 INFO    gensim.topic_coherence.text_analysis: 196 batches submitted to accumulate stats from 12544 documents (569556 virtual)\n",
      "2020-11-19 01:10:11.331 INFO    gensim.topic_coherence.text_analysis: 197 batches submitted to accumulate stats from 12608 documents (572804 virtual)\n",
      "2020-11-19 01:10:11.505 INFO    gensim.topic_coherence.text_analysis: 198 batches submitted to accumulate stats from 12672 documents (576088 virtual)\n",
      "2020-11-19 01:10:11.621 INFO    gensim.topic_coherence.text_analysis: 199 batches submitted to accumulate stats from 12736 documents (579335 virtual)\n",
      "2020-11-19 01:10:11.774 INFO    gensim.topic_coherence.text_analysis: 200 batches submitted to accumulate stats from 12800 documents (582649 virtual)\n",
      "2020-11-19 01:10:11.889 INFO    gensim.topic_coherence.text_analysis: 201 batches submitted to accumulate stats from 12864 documents (586387 virtual)\n",
      "2020-11-19 01:10:12.208 INFO    gensim.topic_coherence.text_analysis: 202 batches submitted to accumulate stats from 12928 documents (590267 virtual)\n",
      "2020-11-19 01:10:12.419 INFO    gensim.topic_coherence.text_analysis: 203 batches submitted to accumulate stats from 12992 documents (594091 virtual)\n",
      "2020-11-19 01:10:12.450 INFO    gensim.topic_coherence.text_analysis: 204 batches submitted to accumulate stats from 13056 documents (597900 virtual)\n",
      "2020-11-19 01:10:12.586 INFO    gensim.topic_coherence.text_analysis: 205 batches submitted to accumulate stats from 13120 documents (601918 virtual)\n",
      "2020-11-19 01:10:12.770 INFO    gensim.topic_coherence.text_analysis: 206 batches submitted to accumulate stats from 13184 documents (605005 virtual)\n",
      "2020-11-19 01:10:12.815 INFO    gensim.topic_coherence.text_analysis: 207 batches submitted to accumulate stats from 13248 documents (607898 virtual)\n",
      "2020-11-19 01:10:12.996 INFO    gensim.topic_coherence.text_analysis: 208 batches submitted to accumulate stats from 13312 documents (610697 virtual)\n",
      "2020-11-19 01:10:13.251 INFO    gensim.topic_coherence.text_analysis: 209 batches submitted to accumulate stats from 13376 documents (613569 virtual)\n",
      "2020-11-19 01:10:13.282 INFO    gensim.topic_coherence.text_analysis: 210 batches submitted to accumulate stats from 13440 documents (616387 virtual)\n",
      "2020-11-19 01:10:13.474 INFO    gensim.topic_coherence.text_analysis: 211 batches submitted to accumulate stats from 13504 documents (619014 virtual)\n",
      "2020-11-19 01:10:13.540 INFO    gensim.topic_coherence.text_analysis: 212 batches submitted to accumulate stats from 13568 documents (621770 virtual)\n",
      "2020-11-19 01:10:13.634 INFO    gensim.topic_coherence.text_analysis: 213 batches submitted to accumulate stats from 13632 documents (624641 virtual)\n",
      "2020-11-19 01:10:13.728 INFO    gensim.topic_coherence.text_analysis: 214 batches submitted to accumulate stats from 13696 documents (627592 virtual)\n",
      "2020-11-19 01:10:13.863 INFO    gensim.topic_coherence.text_analysis: 215 batches submitted to accumulate stats from 13760 documents (630655 virtual)\n",
      "2020-11-19 01:10:13.868 INFO    gensim.topic_coherence.text_analysis: 216 batches submitted to accumulate stats from 13824 documents (633190 virtual)\n",
      "2020-11-19 01:10:14.002 INFO    gensim.topic_coherence.text_analysis: 217 batches submitted to accumulate stats from 13888 documents (635900 virtual)\n",
      "2020-11-19 01:10:14.118 INFO    gensim.topic_coherence.text_analysis: 218 batches submitted to accumulate stats from 13952 documents (639375 virtual)\n",
      "2020-11-19 01:10:14.162 INFO    gensim.topic_coherence.text_analysis: 219 batches submitted to accumulate stats from 14016 documents (642287 virtual)\n",
      "2020-11-19 01:10:14.279 INFO    gensim.topic_coherence.text_analysis: 220 batches submitted to accumulate stats from 14080 documents (644627 virtual)\n",
      "2020-11-19 01:10:14.403 INFO    gensim.topic_coherence.text_analysis: 221 batches submitted to accumulate stats from 14144 documents (646841 virtual)\n",
      "2020-11-19 01:10:14.507 INFO    gensim.topic_coherence.text_analysis: 222 batches submitted to accumulate stats from 14208 documents (650147 virtual)\n",
      "2020-11-19 01:10:14.512 INFO    gensim.topic_coherence.text_analysis: 223 batches submitted to accumulate stats from 14272 documents (653480 virtual)\n",
      "2020-11-19 01:10:14.691 INFO    gensim.topic_coherence.text_analysis: 224 batches submitted to accumulate stats from 14336 documents (656678 virtual)\n",
      "2020-11-19 01:10:14.773 INFO    gensim.topic_coherence.text_analysis: 225 batches submitted to accumulate stats from 14400 documents (659236 virtual)\n",
      "2020-11-19 01:10:14.832 INFO    gensim.topic_coherence.text_analysis: 226 batches submitted to accumulate stats from 14464 documents (661591 virtual)\n",
      "2020-11-19 01:10:14.907 INFO    gensim.topic_coherence.text_analysis: 227 batches submitted to accumulate stats from 14528 documents (664098 virtual)\n",
      "2020-11-19 01:10:15.092 INFO    gensim.topic_coherence.text_analysis: 228 batches submitted to accumulate stats from 14592 documents (666845 virtual)\n",
      "2020-11-19 01:10:15.121 INFO    gensim.topic_coherence.text_analysis: 229 batches submitted to accumulate stats from 14656 documents (669744 virtual)\n",
      "2020-11-19 01:10:15.206 INFO    gensim.topic_coherence.text_analysis: 230 batches submitted to accumulate stats from 14720 documents (672649 virtual)\n",
      "2020-11-19 01:10:15.388 INFO    gensim.topic_coherence.text_analysis: 231 batches submitted to accumulate stats from 14784 documents (675067 virtual)\n",
      "2020-11-19 01:10:15.393 INFO    gensim.topic_coherence.text_analysis: 232 batches submitted to accumulate stats from 14848 documents (678339 virtual)\n",
      "2020-11-19 01:10:15.536 INFO    gensim.topic_coherence.text_analysis: 233 batches submitted to accumulate stats from 14912 documents (681588 virtual)\n",
      "2020-11-19 01:10:15.716 INFO    gensim.topic_coherence.text_analysis: 234 batches submitted to accumulate stats from 14976 documents (684638 virtual)\n",
      "2020-11-19 01:10:15.742 INFO    gensim.topic_coherence.text_analysis: 235 batches submitted to accumulate stats from 15040 documents (687952 virtual)\n",
      "2020-11-19 01:10:15.837 INFO    gensim.topic_coherence.text_analysis: 236 batches submitted to accumulate stats from 15104 documents (691283 virtual)\n",
      "2020-11-19 01:10:15.964 INFO    gensim.topic_coherence.text_analysis: 237 batches submitted to accumulate stats from 15168 documents (694632 virtual)\n",
      "2020-11-19 01:10:16.075 INFO    gensim.topic_coherence.text_analysis: 238 batches submitted to accumulate stats from 15232 documents (697485 virtual)\n",
      "2020-11-19 01:10:16.153 INFO    gensim.topic_coherence.text_analysis: 239 batches submitted to accumulate stats from 15296 documents (699961 virtual)\n",
      "2020-11-19 01:10:16.251 INFO    gensim.topic_coherence.text_analysis: 240 batches submitted to accumulate stats from 15360 documents (703051 virtual)\n",
      "2020-11-19 01:10:16.389 INFO    gensim.topic_coherence.text_analysis: 241 batches submitted to accumulate stats from 15424 documents (705557 virtual)\n",
      "2020-11-19 01:10:16.447 INFO    gensim.topic_coherence.text_analysis: 242 batches submitted to accumulate stats from 15488 documents (708123 virtual)\n",
      "2020-11-19 01:10:16.608 INFO    gensim.topic_coherence.text_analysis: 243 batches submitted to accumulate stats from 15552 documents (711051 virtual)\n",
      "2020-11-19 01:10:16.706 INFO    gensim.topic_coherence.text_analysis: 244 batches submitted to accumulate stats from 15616 documents (713587 virtual)\n",
      "2020-11-19 01:10:16.731 INFO    gensim.topic_coherence.text_analysis: 245 batches submitted to accumulate stats from 15680 documents (716881 virtual)\n",
      "2020-11-19 01:10:16.945 INFO    gensim.topic_coherence.text_analysis: 246 batches submitted to accumulate stats from 15744 documents (719384 virtual)\n",
      "2020-11-19 01:10:16.950 INFO    gensim.topic_coherence.text_analysis: 247 batches submitted to accumulate stats from 15808 documents (722489 virtual)\n",
      "2020-11-19 01:10:16.989 INFO    gensim.topic_coherence.text_analysis: 248 batches submitted to accumulate stats from 15872 documents (725391 virtual)\n",
      "2020-11-19 01:10:17.253 INFO    gensim.topic_coherence.text_analysis: 249 batches submitted to accumulate stats from 15936 documents (728598 virtual)\n",
      "2020-11-19 01:10:17.285 INFO    gensim.topic_coherence.text_analysis: 250 batches submitted to accumulate stats from 16000 documents (731513 virtual)\n",
      "2020-11-19 01:10:17.378 INFO    gensim.topic_coherence.text_analysis: 251 batches submitted to accumulate stats from 16064 documents (733889 virtual)\n",
      "2020-11-19 01:10:17.524 INFO    gensim.topic_coherence.text_analysis: 252 batches submitted to accumulate stats from 16128 documents (737501 virtual)\n",
      "2020-11-19 01:10:17.624 INFO    gensim.topic_coherence.text_analysis: 253 batches submitted to accumulate stats from 16192 documents (739787 virtual)\n",
      "2020-11-19 01:10:17.670 INFO    gensim.topic_coherence.text_analysis: 254 batches submitted to accumulate stats from 16256 documents (743074 virtual)\n",
      "2020-11-19 01:10:17.829 INFO    gensim.topic_coherence.text_analysis: 255 batches submitted to accumulate stats from 16320 documents (745551 virtual)\n",
      "2020-11-19 01:10:17.934 INFO    gensim.topic_coherence.text_analysis: 256 batches submitted to accumulate stats from 16384 documents (748230 virtual)\n",
      "2020-11-19 01:10:18.050 INFO    gensim.topic_coherence.text_analysis: 257 batches submitted to accumulate stats from 16448 documents (751745 virtual)\n",
      "2020-11-19 01:10:18.196 INFO    gensim.topic_coherence.text_analysis: 258 batches submitted to accumulate stats from 16512 documents (754319 virtual)\n",
      "2020-11-19 01:10:18.224 INFO    gensim.topic_coherence.text_analysis: 259 batches submitted to accumulate stats from 16576 documents (757254 virtual)\n",
      "2020-11-19 01:10:18.409 INFO    gensim.topic_coherence.text_analysis: 260 batches submitted to accumulate stats from 16640 documents (760890 virtual)\n",
      "2020-11-19 01:10:18.481 INFO    gensim.topic_coherence.text_analysis: 261 batches submitted to accumulate stats from 16704 documents (764351 virtual)\n",
      "2020-11-19 01:10:18.535 INFO    gensim.topic_coherence.text_analysis: 262 batches submitted to accumulate stats from 16768 documents (767483 virtual)\n",
      "2020-11-19 01:10:18.734 INFO    gensim.topic_coherence.text_analysis: 263 batches submitted to accumulate stats from 16832 documents (770654 virtual)\n",
      "2020-11-19 01:10:18.779 INFO    gensim.topic_coherence.text_analysis: 264 batches submitted to accumulate stats from 16896 documents (773778 virtual)\n",
      "2020-11-19 01:10:18.875 INFO    gensim.topic_coherence.text_analysis: 265 batches submitted to accumulate stats from 16960 documents (776620 virtual)\n",
      "2020-11-19 01:10:19.127 INFO    gensim.topic_coherence.text_analysis: 266 batches submitted to accumulate stats from 17024 documents (779528 virtual)\n",
      "2020-11-19 01:10:19.134 INFO    gensim.topic_coherence.text_analysis: 267 batches submitted to accumulate stats from 17088 documents (782213 virtual)\n",
      "2020-11-19 01:10:19.169 INFO    gensim.topic_coherence.text_analysis: 268 batches submitted to accumulate stats from 17152 documents (785540 virtual)\n",
      "2020-11-19 01:10:19.419 INFO    gensim.topic_coherence.text_analysis: 269 batches submitted to accumulate stats from 17216 documents (788416 virtual)\n",
      "2020-11-19 01:10:19.439 INFO    gensim.topic_coherence.text_analysis: 270 batches submitted to accumulate stats from 17280 documents (791247 virtual)\n",
      "2020-11-19 01:10:19.473 INFO    gensim.topic_coherence.text_analysis: 271 batches submitted to accumulate stats from 17344 documents (794148 virtual)\n",
      "2020-11-19 01:10:19.691 INFO    gensim.topic_coherence.text_analysis: 272 batches submitted to accumulate stats from 17408 documents (797154 virtual)\n",
      "2020-11-19 01:10:19.771 INFO    gensim.topic_coherence.text_analysis: 273 batches submitted to accumulate stats from 17472 documents (800353 virtual)\n",
      "2020-11-19 01:10:19.796 INFO    gensim.topic_coherence.text_analysis: 274 batches submitted to accumulate stats from 17536 documents (803377 virtual)\n",
      "2020-11-19 01:10:19.984 INFO    gensim.topic_coherence.text_analysis: 275 batches submitted to accumulate stats from 17600 documents (806144 virtual)\n",
      "2020-11-19 01:10:20.082 INFO    gensim.topic_coherence.text_analysis: 276 batches submitted to accumulate stats from 17664 documents (808758 virtual)\n",
      "2020-11-19 01:10:20.093 INFO    gensim.topic_coherence.text_analysis: 277 batches submitted to accumulate stats from 17728 documents (812052 virtual)\n",
      "2020-11-19 01:10:20.330 INFO    gensim.topic_coherence.text_analysis: 278 batches submitted to accumulate stats from 17792 documents (814929 virtual)\n",
      "2020-11-19 01:10:20.378 INFO    gensim.topic_coherence.text_analysis: 279 batches submitted to accumulate stats from 17856 documents (818237 virtual)\n",
      "2020-11-19 01:10:20.453 INFO    gensim.topic_coherence.text_analysis: 280 batches submitted to accumulate stats from 17920 documents (821112 virtual)\n",
      "2020-11-19 01:10:20.629 INFO    gensim.topic_coherence.text_analysis: 281 batches submitted to accumulate stats from 17984 documents (824406 virtual)\n",
      "2020-11-19 01:10:20.633 INFO    gensim.topic_coherence.text_analysis: 282 batches submitted to accumulate stats from 18048 documents (827211 virtual)\n",
      "2020-11-19 01:10:20.833 INFO    gensim.topic_coherence.text_analysis: 283 batches submitted to accumulate stats from 18112 documents (830663 virtual)\n",
      "2020-11-19 01:10:20.959 INFO    gensim.topic_coherence.text_analysis: 284 batches submitted to accumulate stats from 18176 documents (833607 virtual)\n",
      "2020-11-19 01:10:20.983 INFO    gensim.topic_coherence.text_analysis: 285 batches submitted to accumulate stats from 18240 documents (836792 virtual)\n",
      "2020-11-19 01:10:21.194 INFO    gensim.topic_coherence.text_analysis: 286 batches submitted to accumulate stats from 18304 documents (840139 virtual)\n",
      "2020-11-19 01:10:21.288 INFO    gensim.topic_coherence.text_analysis: 287 batches submitted to accumulate stats from 18368 documents (843098 virtual)\n",
      "2020-11-19 01:10:21.398 INFO    gensim.topic_coherence.text_analysis: 288 batches submitted to accumulate stats from 18432 documents (846433 virtual)\n",
      "2020-11-19 01:10:21.609 INFO    gensim.topic_coherence.text_analysis: 289 batches submitted to accumulate stats from 18496 documents (849038 virtual)\n",
      "2020-11-19 01:10:21.665 INFO    gensim.topic_coherence.text_analysis: 290 batches submitted to accumulate stats from 18560 documents (852121 virtual)\n",
      "2020-11-19 01:10:21.774 INFO    gensim.topic_coherence.text_analysis: 291 batches submitted to accumulate stats from 18624 documents (856101 virtual)\n",
      "2020-11-19 01:10:22.023 INFO    gensim.topic_coherence.text_analysis: 292 batches submitted to accumulate stats from 18688 documents (859709 virtual)\n",
      "2020-11-19 01:10:22.040 INFO    gensim.topic_coherence.text_analysis: 293 batches submitted to accumulate stats from 18752 documents (862925 virtual)\n",
      "2020-11-19 01:10:22.217 INFO    gensim.topic_coherence.text_analysis: 294 batches submitted to accumulate stats from 18816 documents (866419 virtual)\n",
      "2020-11-19 01:10:22.322 INFO    gensim.topic_coherence.text_analysis: 295 batches submitted to accumulate stats from 18880 documents (869777 virtual)\n",
      "2020-11-19 01:10:22.365 INFO    gensim.topic_coherence.text_analysis: 296 batches submitted to accumulate stats from 18944 documents (873087 virtual)\n",
      "2020-11-19 01:10:22.633 INFO    gensim.topic_coherence.text_analysis: 297 batches submitted to accumulate stats from 19008 documents (876267 virtual)\n",
      "2020-11-19 01:10:22.661 INFO    gensim.topic_coherence.text_analysis: 298 batches submitted to accumulate stats from 19072 documents (879458 virtual)\n",
      "2020-11-19 01:10:22.704 INFO    gensim.topic_coherence.text_analysis: 299 batches submitted to accumulate stats from 19136 documents (882316 virtual)\n",
      "2020-11-19 01:10:22.971 INFO    gensim.topic_coherence.text_analysis: 300 batches submitted to accumulate stats from 19200 documents (885412 virtual)\n",
      "2020-11-19 01:10:22.991 INFO    gensim.topic_coherence.text_analysis: 301 batches submitted to accumulate stats from 19264 documents (888243 virtual)\n",
      "2020-11-19 01:10:23.023 INFO    gensim.topic_coherence.text_analysis: 302 batches submitted to accumulate stats from 19328 documents (891561 virtual)\n",
      "2020-11-19 01:10:23.329 INFO    gensim.topic_coherence.text_analysis: 303 batches submitted to accumulate stats from 19392 documents (894713 virtual)\n",
      "2020-11-19 01:10:23.376 INFO    gensim.topic_coherence.text_analysis: 304 batches submitted to accumulate stats from 19456 documents (897885 virtual)\n",
      "2020-11-19 01:10:23.410 INFO    gensim.topic_coherence.text_analysis: 305 batches submitted to accumulate stats from 19520 documents (900845 virtual)\n",
      "2020-11-19 01:10:23.642 INFO    gensim.topic_coherence.text_analysis: 306 batches submitted to accumulate stats from 19584 documents (903886 virtual)\n",
      "2020-11-19 01:10:23.667 INFO    gensim.topic_coherence.text_analysis: 307 batches submitted to accumulate stats from 19648 documents (907216 virtual)\n",
      "2020-11-19 01:10:23.748 INFO    gensim.topic_coherence.text_analysis: 308 batches submitted to accumulate stats from 19712 documents (910660 virtual)\n",
      "2020-11-19 01:10:24.025 INFO    gensim.topic_coherence.text_analysis: 309 batches submitted to accumulate stats from 19776 documents (913251 virtual)\n",
      "2020-11-19 01:10:24.052 INFO    gensim.topic_coherence.text_analysis: 310 batches submitted to accumulate stats from 19840 documents (916001 virtual)\n",
      "2020-11-19 01:10:24.183 INFO    gensim.topic_coherence.text_analysis: 311 batches submitted to accumulate stats from 19904 documents (918894 virtual)\n",
      "2020-11-19 01:10:24.378 INFO    gensim.topic_coherence.text_analysis: 312 batches submitted to accumulate stats from 19968 documents (921662 virtual)\n",
      "2020-11-19 01:10:24.453 INFO    gensim.topic_coherence.text_analysis: 313 batches submitted to accumulate stats from 20032 documents (924574 virtual)\n",
      "2020-11-19 01:10:24.494 INFO    gensim.topic_coherence.text_analysis: 314 batches submitted to accumulate stats from 20096 documents (927381 virtual)\n",
      "2020-11-19 01:10:24.683 INFO    gensim.topic_coherence.text_analysis: 315 batches submitted to accumulate stats from 20160 documents (930313 virtual)\n",
      "2020-11-19 01:10:24.702 INFO    gensim.topic_coherence.text_analysis: 316 batches submitted to accumulate stats from 20224 documents (933287 virtual)\n",
      "2020-11-19 01:10:24.715 INFO    gensim.topic_coherence.text_analysis: 317 batches submitted to accumulate stats from 20288 documents (936865 virtual)\n",
      "2020-11-19 01:10:24.911 INFO    gensim.topic_coherence.text_analysis: 318 batches submitted to accumulate stats from 20352 documents (940020 virtual)\n",
      "2020-11-19 01:10:24.928 INFO    gensim.topic_coherence.text_analysis: 319 batches submitted to accumulate stats from 20416 documents (942681 virtual)\n",
      "2020-11-19 01:10:25.000 INFO    gensim.topic_coherence.text_analysis: 320 batches submitted to accumulate stats from 20480 documents (945507 virtual)\n",
      "2020-11-19 01:10:25.158 INFO    gensim.topic_coherence.text_analysis: 321 batches submitted to accumulate stats from 20544 documents (947471 virtual)\n",
      "2020-11-19 01:10:26.461 INFO    gensim.topic_coherence.text_analysis: 3 accumulators retrieved from output queue\n",
      "2020-11-19 01:10:26.765 INFO    gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 947987 virtual documents\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.17185044220753978"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "npmi = CoherenceNPMI(texts=text_cleaned, topics=topics_bert)\n",
    "npmi.score()"
   ]
  },
  {
   "source": [
    "### 3.2.2 External Word Embeddings Topic Coherence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "2020-11-19 01:10:29.902 INFO    gensim.models.utils_any2vec: loading projection weights from C:\\Users\\Pieter-Jan/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n",
      "2020-11-19 01:11:25.882 INFO    gensim.models.utils_any2vec: loaded (3000000, 300) matrix from C:\\Users\\Pieter-Jan/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.15327418"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "CoherenceWordEmbeddings(topics_bert).score()"
   ]
  },
  {
   "source": [
    "### 3.2.3 Rank-Biased Overlap "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9908176773662869"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "InvertedRBO(topics_bert).score()"
   ]
  }
 ]
}