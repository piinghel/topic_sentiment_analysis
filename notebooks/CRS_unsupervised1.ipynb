{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"C:\\\\Users\\\\Pieter-Jan\\\\Documents\\\\Work\\\\Candriam\\\\nlp\\\\ESG\\\\topic_sentiment_analysis\")"
   ]
  },
  {
   "source": [
    "# Libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "sorflow\\core\\framework\\types_pb2.py:177: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.EnumValueDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:181: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.EnumValueDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:185: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.EnumValueDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:189: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.EnumValueDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:193: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.EnumValueDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:197: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.EnumValueDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:201: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.EnumValueDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:205: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.EnumValueDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:209: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.EnumValueDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:213: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.EnumValueDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:217: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.EnumValueDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _DATATYPE = _descriptor.EnumDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  DESCRIPTOR = _descriptor.FileDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:46: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:76: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:83: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:90: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:97: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:104: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:111: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:69: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _RESOURCEHANDLEPROTO = _descriptor.Descriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  DESCRIPTOR = _descriptor.FileDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:47: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:54: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:61: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:68: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:75: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:82: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:89: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:96: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:103: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:110: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:117: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:124: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:131: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:138: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:145: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:152: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _TENSORPROTO = _descriptor.Descriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:183: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:190: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:197: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:176: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _VARIANTTENSORDATAPROTO = _descriptor.Descriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  DESCRIPTOR = _descriptor.FileDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:47: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:54: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:61: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:68: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:75: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:82: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:89: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _descriptor.FieldDescriptor(\nC:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n  _ATTRVALUE_LISTVALUE = _descriptor.Descriptor(\n"
     ]
    }
   ],
   "source": [
    "# data manipulations\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "# data manipulations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy import sparse\n",
    "\n",
    "# CMT\n",
    "from contextualized_topic_models.models.ctm import CTM\n",
    "from contextualized_topic_models.datasets.dataset import CTMDataset\n",
    "from contextualized_topic_models.evaluation.measures import CoherenceNPMI, InvertedRBO, CoherenceWordEmbeddings\n",
    "from contextualized_topic_models.utils.preprocessing import SimplePreprocessing\n",
    "\n",
    "# lDA\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import LdaMulticore \n",
    "import pyLDAvis.gensim\n",
    "from gensim import corpora, matutils, models, similarities\n",
    "import pyLDAvis\n",
    "\n",
    "# BerTopic\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# other\n",
    "import random\n",
    "\n",
    "# own modules\n",
    "import modules.preprocessing as preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "source": [
    "# Goal\n",
    "\n",
    "We are going to compare the performance of three **unsupervised** models for topic modelling on ESG documents.\n",
    "\n",
    "1. Contextualized Topic Modelling (CTM): https://github.com/MilaNLProc/contextualized-topic-models\n",
    "2. Latent Dirichlet Allocation (LDA): https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "3. BERTopic: https://github.com/MaartenGr/BERTopic"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Evaluation measures \n",
    "\n",
    "1. **Normalized Point-wise Mutual Information (NPMI) (Lau et al.,\n",
    "2014)**\n",
    "\n",
    "It measures how much the top-10 words of a topic are related to each other, considering the empirical frequency of the words computed on the\n",
    "original corpus. τ is a symbolic metric and relies on co-occurrence.\n",
    "\n",
    "2. **External Word Embeddings Topic Coherence**\n",
    "\n",
    "As Ding et al. (2018) pointed out, though, topic\n",
    "coherence computed on the same data is inherently\n",
    "limited. Coherence computed on an external corpus, on the other hand, correlates much more to\n",
    "human judgment, but it may be expensive to estimate. Thus, our second metric is an external\n",
    "word embeddings topic coherence metric, which we compute by adopting a strategy similar to that\n",
    "described in Ding et al. (2018). First, we compute\n",
    "the average pairwise cosine similarity of the word\n",
    "embeddings of the top-10 words in a topic using (Mikolov et al., 2013) embeddings. Then, we\n",
    "compute the overall average of those values for all\n",
    "the topics (α).\n",
    "\n",
    "3. **rank-\n",
    "biased overlap (RBO) (Webber et al., 2010)**\n",
    "\n",
    "To evaluate how diverse the topics\n",
    "generated by a single model are, we use the rank-\n",
    "biased overlap (RBO) (Webber et al., 2010). RBO\n",
    "compares two topics of the same model. The key\n",
    "qualities of this measure are twofold: it allows\n",
    "disjointedness between the lists of topics (i.e., two\n",
    "topics can have different words in them) and it is\n",
    "weighted on the ranking (i.e., two lists that share\n",
    "some of the same words, albeit at different rankings,\n",
    "are penalized less than two lists that share the same\n",
    "words at the highest ranks). We deﬁne ρ as the rank-\n",
    "biased overlap diversity, that we interpret as the\n",
    "reciprocal of the standard RBO. ρ is 0 for identical\n",
    "topics and 1 for completely different topics. Both\n",
    "metrics are computed on the top-k ranked lists.\n",
    "Following the state-of-the-art, we consider k = 10."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# global variable used throughout te notebook to update preprocessing steps\n",
    "UPDATE = False\n",
    "DIR_REPORTS = 'C:/Users/Pieter-Jan/Documents/Work/Candriam/nlp/ESG/reports/Sectors/'"
   ]
  },
  {
   "source": [
    "## Download reports and perform some text processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "C:\\Users\\Pieter-Jan\\Anaconda3\\envs\\marketDb\\lib\\site-packages\\xlrd\\xlsx.py:266: DeprecationWarning: This method will be removed in future versions.  Use 'tree.iter()' or 'list(tree.iter())' instead.\n",
      "  for elem in self.tree.iter() if Element_has_iter else self.tree.getiterator():\n",
      "C:\\Users\\Pieter-Jan\\Anaconda3\\envs\\marketDb\\lib\\site-packages\\xlrd\\xlsx.py:312: DeprecationWarning: This method will be removed in future versions.  Use 'tree.iter()' or 'list(tree.iter())' instead.\n",
      "  for elem in self.tree.iter() if Element_has_iter else self.tree.getiterator():\n",
      "C:\\Users\\Pieter-Jan\\Anaconda3\\envs\\marketDb\\lib\\site-packages\\xlrd\\xlsx.py:266: DeprecationWarning: This method will be removed in future versions.  Use 'tree.iter()' or 'list(tree.iter())' instead.\n",
      "  for elem in self.tree.iter() if Element_has_iter else self.tree.getiterator():\n",
      "2020-11-18 21:15:17.031 INFO    numexpr.utils: NumExpr defaulting to 4 threads.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           company                 industry     sector  \\\n",
       "0  Brookfield Property Partners LP  Real Estate Development  Financial   \n",
       "1           PrairieSky Royalty Ltd  Real Estate Development  Financial   \n",
       "2           PrairieSky Royalty Ltd  Real Estate Development  Financial   \n",
       "3           PrairieSky Royalty Ltd  Real Estate Development  Financial   \n",
       "4     Summit Hotel Properties Inc.  Real Estate Development  Financial   \n",
       "\n",
       "     year                                                url  \n",
       "0  2019.0  https://www.brookfield.com/sites/default/files...  \n",
       "1  2017.0  https://www.prairiesky.com/wp-content/uploads/...  \n",
       "2  2018.0  https://www.prairiesky.com/wp-content/uploads/...  \n",
       "3  2019.0  https://www.prairiesky.com/wp-content/uploads/...  \n",
       "4  2018.0  https://www.responsibilityreports.com/HostedDa...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>company</th>\n      <th>industry</th>\n      <th>sector</th>\n      <th>year</th>\n      <th>url</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Brookfield Property Partners LP</td>\n      <td>Real Estate Development</td>\n      <td>Financial</td>\n      <td>2019.0</td>\n      <td>https://www.brookfield.com/sites/default/files...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>PrairieSky Royalty Ltd</td>\n      <td>Real Estate Development</td>\n      <td>Financial</td>\n      <td>2017.0</td>\n      <td>https://www.prairiesky.com/wp-content/uploads/...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PrairieSky Royalty Ltd</td>\n      <td>Real Estate Development</td>\n      <td>Financial</td>\n      <td>2018.0</td>\n      <td>https://www.prairiesky.com/wp-content/uploads/...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>PrairieSky Royalty Ltd</td>\n      <td>Real Estate Development</td>\n      <td>Financial</td>\n      <td>2019.0</td>\n      <td>https://www.prairiesky.com/wp-content/uploads/...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Summit Hotel Properties Inc.</td>\n      <td>Real Estate Development</td>\n      <td>Financial</td>\n      <td>2018.0</td>\n      <td>https://www.responsibilityreports.com/HostedDa...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df = pd.read_excel(\"data//Industry_CSR_Datasets.xlsx\").dropna().reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "100%|██████████| 2/2 [00:03<00:00,  1.58s/it]\n"
     ]
    }
   ],
   "source": [
    "# download reports and store\n",
    "df_download, could_not_download = preprocess.download_reports(df=df, directory=DIR_REPORTS, update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(168, 6)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "df_download.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# onlyfiles = [f for f in os.listdir(DIR_REPORTS) if os.path.isfile(os.path.join(DIR_REPORTS, f))]\n",
    "# df = df[~df['company'].isin(could_not_download)]\n",
    "# df[\"filename\"] = onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "'Text']}, streams=[<PDFStream(1059): len=554, {'BBox': [0.0, 792.0, 1224.0, 0.0], 'Filter': /'FlateDecode', 'Length': 315, 'Matrix': [1.0, 0.0, 0.0, 1.0, 0.0, 0.0], 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>}, 'Font': {'T1_0': <PDFObjRef:1060>, 'T1_1': <PDFObjRef:1097>}, 'ProcSet': [/'PDF', /'Text']}, 'Subtype': /'Form'}>], ctm=(1.0, 0.0, 0.0, 1.0, 0.0, 0.0)\n",
      "2020-11-18 18:32:24.514 INFO    pdfminer.pdfinterp: Processing xobj: <PDFStream(1015): raw=36664, {'BitsPerComponent': 8, 'ColorSpace': <PDFObjRef:2419>, 'Filter': /'DCTDecode', 'Height': 318, 'Intent': /'RelativeColorimetric', 'Length': 36662, 'Metadata': <PDFObjRef:1012>, 'Name': /'X', 'SMask': <PDFObjRef:1014>, 'Subtype': /'Image', 'Type': /'XObject', 'Width': 485}>\n",
      "2020-11-18 18:32:24.533 INFO    pdfminer.pdfpage: Page: {'Annots': <PDFObjRef:1018>, 'ArtBox': [0.0, 0.0, 1224.0, 792.0], 'BleedBox': [0.0, 0.0, 1224.0, 792.0], 'Contents': <PDFObjRef:1019>, 'CropBox': [0.0, 0.0, 1224.0, 792.0], 'MediaBox': [0.0, 0.0, 1224.0, 792.0], 'Parent': <PDFObjRef:2401>, 'PieceInfo': {'InDesign': {'DocumentID': b'\\xfe\\xff\\x00x\\x00m\\x00p\\x00.\\x00d\\x00i\\x00d\\x00:\\x009\\x003\\x001\\x00f\\x002\\x000\\x007\\x00d\\x00-\\x002\\x005\\x002\\x002\\x00-\\x004\\x005\\x004\\x009\\x00-\\x008\\x006\\x009\\x00d\\x00-\\x00b\\x001\\x007\\x008\\x006\\x006\\x001\\x000\\x009\\x002\\x00b\\x006', 'LastModified': b'\\xfe\\xff\\x00D\\x00:\\x002\\x000\\x002\\x000\\x001\\x000\\x000\\x001\\x002\\x001\\x003\\x005\\x002\\x000\\x00Z', 'NumberofPages': 2, 'OriginalDocumentID': b'\\xfe\\xff\\x00x\\x00m\\x00p\\x00.\\x00d\\x00i\\x00d\\x00:\\x009\\x008\\x00c\\x00f\\x003\\x009\\x00e\\x000\\x00-\\x00c\\x004\\x001\\x00d\\x00-\\x004\\x007\\x007\\x00d\\x00-\\x00a\\x003\\x000\\x002\\x00-\\x008\\x005\\x00a\\x005\\x00d\\x001\\x000\\x006\\x00e\\x002\\x00a\\x00e', 'PageUIDList': {'0': 205079, '1': 205080}, 'PageWidthList': {'0': 612.0, '1': 612.0}}}, 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>, 'GS1': <PDFObjRef:1191>}, 'Font': {'C2_0': <PDFObjRef:2143>, 'T1_0': <PDFObjRef:2427>, 'T1_1': <PDFObjRef:2434>, 'T1_2': <PDFObjRef:2426>}, 'ProcSet': [/'PDF', /'Text'], 'XObject': {'Fm0': <PDFObjRef:1059>}}, 'Rotate': 0, 'Tabs': /'W', 'Thumb': <PDFObjRef:2384>, 'TrimBox': [0.0, 0.0, 1224.0, 792.0], 'Type': /'Page'}\n",
      "2020-11-18 18:32:24.535 INFO    pdfminer.pdfinterp: Processing page: <PDFPage: Resources={'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>, 'GS1': <PDFObjRef:1191>}, 'Font': {'C2_0': <PDFObjRef:2143>, 'T1_0': <PDFObjRef:2427>, 'T1_1': <PDFObjRef:2434>, 'T1_2': <PDFObjRef:2426>}, 'ProcSet': [/'PDF', /'Text'], 'XObject': {'Fm0': <PDFObjRef:1059>}}, MediaBox=[0.0, 0.0, 1224.0, 792.0]>\n",
      "2020-11-18 18:32:24.538 INFO    pdfminer.pdfinterp: render_contents: resources={'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>, 'GS1': <PDFObjRef:1191>}, 'Font': {'C2_0': <PDFObjRef:2143>, 'T1_0': <PDFObjRef:2427>, 'T1_1': <PDFObjRef:2434>, 'T1_2': <PDFObjRef:2426>}, 'ProcSet': [/'PDF', /'Text'], 'XObject': {'Fm0': <PDFObjRef:1059>}}, streams=[<PDFStream(1019): raw=11094, {'Filter': /'FlateDecode', 'Length': 11092}>], ctm=(1, 0, 0, 1, -0.0, -0.0)\n",
      "2020-11-18 18:32:24.540 INFO    pdfminer.pdfinterp: get_font: create: objid=2143, spec={'BaseFont': /'ARTQVT+MS-UIGothic', 'DescendantFonts': <PDFObjRef:1025>, 'Encoding': /'Identity-H', 'Subtype': /'Type0', 'ToUnicode': <PDFObjRef:1026>, 'Type': /'Font'}\n",
      "2020-11-18 18:32:24.544 INFO    pdfminer.pdfinterp: get_font: create: objid=None, spec={'BaseFont': /'ARTQVT+MS-UIGothic', 'CIDSystemInfo': <PDFObjRef:1020>, 'CIDToGIDMap': /'Identity', 'DW': 1000, 'FontDescriptor': <PDFObjRef:1023>, 'Subtype': /'CIDFontType2', 'Type': /'Font', 'Encoding': /'Identity-H', 'ToUnicode': <PDFStream(1026): raw=234, {'Filter': /'FlateDecode', 'Length': 232}>}\n",
      "2020-11-18 18:32:24.550 INFO    pdfminer.pdfinterp: Processing xobj: <PDFStream(1059): len=554, {'BBox': [0.0, 792.0, 1224.0, 0.0], 'Filter': /'FlateDecode', 'Length': 315, 'Matrix': [1.0, 0.0, 0.0, 1.0, 0.0, 0.0], 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>}, 'Font': {'T1_0': <PDFObjRef:1060>, 'T1_1': <PDFObjRef:1097>}, 'ProcSet': [/'PDF', /'Text']}, 'Subtype': /'Form'}>\n",
      "2020-11-18 18:32:24.551 INFO    pdfminer.pdfinterp: render_contents: resources={'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>}, 'Font': {'T1_0': <PDFObjRef:1060>, 'T1_1': <PDFObjRef:1097>}, 'ProcSet': [/'PDF', /'Text']}, streams=[<PDFStream(1059): len=554, {'BBox': [0.0, 792.0, 1224.0, 0.0], 'Filter': /'FlateDecode', 'Length': 315, 'Matrix': [1.0, 0.0, 0.0, 1.0, 0.0, 0.0], 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>}, 'Font': {'T1_0': <PDFObjRef:1060>, 'T1_1': <PDFObjRef:1097>}, 'ProcSet': [/'PDF', /'Text']}, 'Subtype': /'Form'}>], ctm=(1.0, 0.0, 0.0, 1.0, 0.0, 0.0)\n",
      "2020-11-18 18:32:26.618 INFO    pdfminer.pdfpage: Page: {'Annots': <PDFObjRef:1028>, 'ArtBox': [0.0, 0.0, 1224.0, 792.0], 'BleedBox': [0.0, 0.0, 1224.0, 792.0], 'Contents': <PDFObjRef:1029>, 'CropBox': [0.0, 0.0, 1224.0, 792.0], 'MediaBox': [0.0, 0.0, 1224.0, 792.0], 'Parent': <PDFObjRef:2401>, 'PieceInfo': {'InDesign': {'DocumentID': b'\\xfe\\xff\\x00x\\x00m\\x00p\\x00.\\x00d\\x00i\\x00d\\x00:\\x009\\x003\\x001\\x00f\\x002\\x000\\x007\\x00d\\x00-\\x002\\x005\\x002\\x002\\x00-\\x004\\x005\\x004\\x009\\x00-\\x008\\x006\\x009\\x00d\\x00-\\x00b\\x001\\x007\\x008\\x006\\x006\\x001\\x000\\x009\\x002\\x00b\\x006', 'LastModified': b'\\xfe\\xff\\x00D\\x00:\\x002\\x000\\x002\\x000\\x001\\x000\\x000\\x001\\x002\\x001\\x003\\x005\\x002\\x000\\x00Z', 'NumberofPages': 1, 'OriginalDocumentID': b'\\xfe\\xff\\x00x\\x00m\\x00p\\x00.\\x00d\\x00i\\x00d\\x00:\\x009\\x008\\x00c\\x00f\\x003\\x009\\x00e\\x000\\x00-\\x00c\\x004\\x001\\x00d\\x00-\\x004\\x007\\x007\\x00d\\x00-\\x00a\\x003\\x000\\x002\\x00-\\x008\\x005\\x00a\\x005\\x00d\\x001\\x000\\x006\\x00e\\x002\\x00a\\x00e', 'PageUIDList': {'0': 157520}, 'PageWidthList': {'0': 612.0}}}, 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>}, 'Font': {'T1_0': <PDFObjRef:2427>, 'T1_1': <PDFObjRef:2434>, 'T1_2': <PDFObjRef:2426>}, 'ProcSet': [/'PDF', /'Text'], 'XObject': {'Fm0': <PDFObjRef:1059>}}, 'Rotate': 0, 'Tabs': /'W', 'Thumb': <PDFObjRef:2385>, 'TrimBox': [0.0, 0.0, 1224.0, 792.0], 'Type': /'Page'}\n",
      "2020-11-18 18:32:26.622 INFO    pdfminer.pdfinterp: Processing page: <PDFPage: Resources={'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>}, 'Font': {'T1_0': <PDFObjRef:2427>, 'T1_1': <PDFObjRef:2434>, 'T1_2': <PDFObjRef:2426>}, 'ProcSet': [/'PDF', /'Text'], 'XObject': {'Fm0': <PDFObjRef:1059>}}, MediaBox=[0.0, 0.0, 1224.0, 792.0]>\n",
      "2020-11-18 18:32:26.636 INFO    pdfminer.pdfinterp: render_contents: resources={'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>}, 'Font': {'T1_0': <PDFObjRef:2427>, 'T1_1': <PDFObjRef:2434>, 'T1_2': <PDFObjRef:2426>}, 'ProcSet': [/'PDF', /'Text'], 'XObject': {'Fm0': <PDFObjRef:1059>}}, streams=[<PDFStream(1029): raw=8148, {'Filter': /'FlateDecode', 'Length': 8146}>], ctm=(1, 0, 0, 1, -0.0, -0.0)\n",
      "2020-11-18 18:32:26.639 INFO    pdfminer.pdfinterp: Processing xobj: <PDFStream(1059): len=554, {'BBox': [0.0, 792.0, 1224.0, 0.0], 'Filter': /'FlateDecode', 'Length': 315, 'Matrix': [1.0, 0.0, 0.0, 1.0, 0.0, 0.0], 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>}, 'Font': {'T1_0': <PDFObjRef:1060>, 'T1_1': <PDFObjRef:1097>}, 'ProcSet': [/'PDF', /'Text']}, 'Subtype': /'Form'}>\n",
      "2020-11-18 18:32:26.643 INFO    pdfminer.pdfinterp: render_contents: resources={'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>}, 'Font': {'T1_0': <PDFObjRef:1060>, 'T1_1': <PDFObjRef:1097>}, 'ProcSet': [/'PDF', /'Text']}, streams=[<PDFStream(1059): len=554, {'BBox': [0.0, 792.0, 1224.0, 0.0], 'Filter': /'FlateDecode', 'Length': 315, 'Matrix': [1.0, 0.0, 0.0, 1.0, 0.0, 0.0], 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>}, 'Font': {'T1_0': <PDFObjRef:1060>, 'T1_1': <PDFObjRef:1097>}, 'ProcSet': [/'PDF', /'Text']}, 'Subtype': /'Form'}>], ctm=(1.0, 0.0, 0.0, 1.0, 0.0, 0.0)\n",
      "2020-11-18 18:32:28.061 INFO    pdfminer.pdfpage: Page: {'Annots': <PDFObjRef:1031>, 'ArtBox': [0.0, 0.0, 1224.0, 792.0], 'BleedBox': [0.0, 0.0, 1224.0, 792.0], 'Contents': <PDFObjRef:1032>, 'CropBox': [0.0, 0.0, 1224.0, 792.0], 'MediaBox': [0.0, 0.0, 1224.0, 792.0], 'Parent': <PDFObjRef:2401>, 'PieceInfo': {'InDesign': {'DocumentID': b'\\xfe\\xff\\x00x\\x00m\\x00p\\x00.\\x00d\\x00i\\x00d\\x00:\\x009\\x003\\x001\\x00f\\x002\\x000\\x007\\x00d\\x00-\\x002\\x005\\x002\\x002\\x00-\\x004\\x005\\x004\\x009\\x00-\\x008\\x006\\x009\\x00d\\x00-\\x00b\\x001\\x007\\x008\\x006\\x006\\x001\\x000\\x009\\x002\\x00b\\x006', 'LastModified': b'\\xfe\\xff\\x00D\\x00:\\x002\\x000\\x002\\x000\\x001\\x000\\x000\\x001\\x002\\x001\\x003\\x005\\x002\\x001\\x00Z', 'NumberofPages': 1, 'OriginalDocumentID': b'\\xfe\\xff\\x00x\\x00m\\x00p\\x00.\\x00d\\x00i\\x00d\\x00:\\x009\\x008\\x00c\\x00f\\x003\\x009\\x00e\\x000\\x00-\\x00c\\x004\\x001\\x00d\\x00-\\x004\\x007\\x007\\x00d\\x00-\\x00a\\x003\\x000\\x002\\x00-\\x008\\x005\\x00a\\x005\\x00d\\x001\\x000\\x006\\x00e\\x002\\x00a\\x00e', 'PageUIDList': {'0': 157673}, 'PageWidthList': {'0': 612.0}}}, 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>, 'GS1': <PDFObjRef:1191>}, 'Font': {'T1_0': <PDFObjRef:2427>, 'T1_1': <PDFObjRef:2434>, 'T1_2': <PDFObjRef:2426>}, 'ProcSet': [/'PDF', /'Text'], 'XObject': {'Fm0': <PDFObjRef:1059>}}, 'Rotate': 0, 'Tabs': /'W', 'Thumb': <PDFObjRef:2386>, 'TrimBox': [0.0, 0.0, 1224.0, 792.0], 'Type': /'Page'}\n",
      "2020-11-18 18:32:28.064 INFO    pdfminer.pdfinterp: Processing page: <PDFPage: Resources={'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>, 'GS1': <PDFObjRef:1191>}, 'Font': {'T1_0': <PDFObjRef:2427>, 'T1_1': <PDFObjRef:2434>, 'T1_2': <PDFObjRef:2426>}, 'ProcSet': [/'PDF', /'Text'], 'XObject': {'Fm0': <PDFObjRef:1059>}}, MediaBox=[0.0, 0.0, 1224.0, 792.0]>\n",
      "2020-11-18 18:32:28.070 INFO    pdfminer.pdfinterp: render_contents: resources={'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>, 'GS1': <PDFObjRef:1191>}, 'Font': {'T1_0': <PDFObjRef:2427>, 'T1_1': <PDFObjRef:2434>, 'T1_2': <PDFObjRef:2426>}, 'ProcSet': [/'PDF', /'Text'], 'XObject': {'Fm0': <PDFObjRef:1059>}}, streams=[<PDFStream(1032): raw=9114, {'Filter': /'FlateDecode', 'Length': 9112}>], ctm=(1, 0, 0, 1, -0.0, -0.0)\n",
      "2020-11-18 18:32:28.072 INFO    pdfminer.pdfinterp: Processing xobj: <PDFStream(1059): len=554, {'BBox': [0.0, 792.0, 1224.0, 0.0], 'Filter': /'FlateDecode', 'Length': 315, 'Matrix': [1.0, 0.0, 0.0, 1.0, 0.0, 0.0], 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>}, 'Font': {'T1_0': <PDFObjRef:1060>, 'T1_1': <PDFObjRef:1097>}, 'ProcSet': [/'PDF', /'Text']}, 'Subtype': /'Form'}>\n",
      "2020-11-18 18:32:28.075 INFO    pdfminer.pdfinterp: render_contents: resources={'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>}, 'Font': {'T1_0': <PDFObjRef:1060>, 'T1_1': <PDFObjRef:1097>}, 'ProcSet': [/'PDF', /'Text']}, streams=[<PDFStream(1059): len=554, {'BBox': [0.0, 792.0, 1224.0, 0.0], 'Filter': /'FlateDecode', 'Length': 315, 'Matrix': [1.0, 0.0, 0.0, 1.0, 0.0, 0.0], 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>}, 'Font': {'T1_0': <PDFObjRef:1060>, 'T1_1': <PDFObjRef:1097>}, 'ProcSet': [/'PDF', /'Text']}, 'Subtype': /'Form'}>], ctm=(1.0, 0.0, 0.0, 1.0, 0.0, 0.0)\n",
      "2020-11-18 18:32:28.701 INFO    pdfminer.pdfpage: Page: {'Annots': <PDFObjRef:1034>, 'ArtBox': [0.0, 0.0, 1224.0, 792.0], 'BleedBox': [0.0, 0.0, 1224.0, 792.0], 'Contents': <PDFObjRef:1035>, 'CropBox': [0.0, 0.0, 1224.0, 792.0], 'Group': <PDFObjRef:1042>, 'MediaBox': [0.0, 0.0, 1224.0, 792.0], 'Parent': <PDFObjRef:2401>, 'PieceInfo': {'InDesign': {'DocumentID': b'\\xfe\\xff\\x00x\\x00m\\x00p\\x00.\\x00d\\x00i\\x00d\\x00:\\x009\\x003\\x001\\x00f\\x002\\x000\\x007\\x00d\\x00-\\x002\\x005\\x002\\x002\\x00-\\x004\\x005\\x004\\x009\\x00-\\x008\\x006\\x009\\x00d\\x00-\\x00b\\x001\\x007\\x008\\x006\\x006\\x001\\x000\\x009\\x002\\x00b\\x006', 'LastModified': b'\\xfe\\xff\\x00D\\x00:\\x002\\x000\\x002\\x000\\x001\\x000\\x000\\x001\\x002\\x001\\x003\\x005\\x002\\x001\\x00Z', 'NumberofPages': 1, 'OriginalDocumentID': b'\\xfe\\xff\\x00x\\x00m\\x00p\\x00.\\x00d\\x00i\\x00d\\x00:\\x009\\x008\\x00c\\x00f\\x003\\x009\\x00e\\x000\\x00-\\x00c\\x004\\x001\\x00d\\x00-\\x004\\x007\\x007\\x00d\\x00-\\x00a\\x003\\x000\\x002\\x00-\\x008\\x005\\x00a\\x005\\x00d\\x001\\x000\\x006\\x00e\\x002\\x00a\\x00e', 'PageUIDList': {'0': 118761}, 'PageWidthList': {'0': 612.0}}}, 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:1221>, 'CS1': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:1251>, 'GS1': <PDFObjRef:2418>, 'GS2': <PDFObjRef:1191>}, 'Font': {'T1_0': <PDFObjRef:2427>, 'T1_1': <PDFObjRef:2434>, 'T1_2': <PDFObjRef:2426>}, 'ProcSet': [/'PDF', /'Text', /'ImageC', /'ImageI'], 'XObject': {'Fm0': <PDFObjRef:1037>, 'Fm1': <PDFObjRef:1059>, 'Im0': <PDFObjRef:1041>}}, 'Rotate': 0, 'Tabs': /'W', 'Thumb': <PDFObjRef:2387>, 'TrimBox': [0.0, 0.0, 1224.0, 792.0], 'Type': /'Page'}\n",
      "2020-11-18 18:32:28.702 INFO    pdfminer.pdfinterp: Processing page: <PDFPage: Resources={'ColorSpace': {'CS0': <PDFObjRef:1221>, 'CS1': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:1251>, 'GS1': <PDFObjRef:2418>, 'GS2': <PDFObjRef:1191>}, 'Font': {'T1_0': <PDFObjRef:2427>, 'T1_1': <PDFObjRef:2434>, 'T1_2': <PDFObjRef:2426>}, 'ProcSet': [/'PDF', /'Text', /'ImageC', /'ImageI'], 'XObject': {'Fm0': <PDFObjRef:1037>, 'Fm1': <PDFObjRef:1059>, 'Im0': <PDFObjRef:1041>}}, MediaBox=[0.0, 0.0, 1224.0, 792.0]>\n",
      "2020-11-18 18:32:28.710 INFO    pdfminer.pdfinterp: render_contents: resources={'ColorSpace': {'CS0': <PDFObjRef:1221>, 'CS1': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:1251>, 'GS1': <PDFObjRef:2418>, 'GS2': <PDFObjRef:1191>}, 'Font': {'T1_0': <PDFObjRef:2427>, 'T1_1': <PDFObjRef:2434>, 'T1_2': <PDFObjRef:2426>}, 'ProcSet': [/'PDF', /'Text', /'ImageC', /'ImageI'], 'XObject': {'Fm0': <PDFObjRef:1037>, 'Fm1': <PDFObjRef:1059>, 'Im0': <PDFObjRef:1041>}}, streams=[<PDFStream(1035): raw=3508, {'Filter': /'FlateDecode', 'Length': 3506}>], ctm=(1, 0, 0, 1, -0.0, -0.0)\n",
      "2020-11-18 18:32:28.713 INFO    pdfminer.pdfinterp: Processing xobj: <PDFStream(1037): raw=69, {'BBox': [603.0, 811.961, 1248.84, -16.8398], 'Filter': /'FlateDecode', 'Group': <PDFObjRef:1036>, 'Length': 67, 'Matrix': [1.0, 0.0, 0.0, 1.0, 0.0, 0.0], 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>}}, 'Subtype': /'Form'}>\n",
      "2020-11-18 18:32:28.715 INFO    pdfminer.pdfinterp: render_contents: resources={'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>}}, streams=[<PDFStream(1037): raw=69, {'BBox': [603.0, 811.961, 1248.84, -16.8398], 'Filter': /'FlateDecode', 'Group': <PDFObjRef:1036>, 'Length': 67, 'Matrix': [1.0, 0.0, 0.0, 1.0, 0.0, 0.0], 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>}}, 'Subtype': /'Form'}>], ctm=(1.0, 0.0, 0.0, 1.0, 0.0, 0.0)\n",
      "2020-11-18 18:32:28.733 INFO    pdfminer.pdfinterp: Processing xobj: <PDFStream(1041): raw=249969, {'BitsPerComponent': 8, 'ColorSpace': <PDFObjRef:1243>, 'Decode': [0.0, 255.0], 'Filter': /'FlateDecode', 'Height': 624, 'Intent': /'RelativeColorimetric', 'Length': 249967, 'Metadata': <PDFObjRef:1038>, 'Name': /'X', 'SMask': <PDFObjRef:1040>, 'Subtype': /'Image', 'Type': /'XObject', 'Width': 617}>\n",
      "2020-11-18 18:32:28.735 INFO    pdfminer.pdfinterp: Processing xobj: <PDFStream(1059): len=554, {'BBox': [0.0, 792.0, 1224.0, 0.0], 'Filter': /'FlateDecode', 'Length': 315, 'Matrix': [1.0, 0.0, 0.0, 1.0, 0.0, 0.0], 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>}, 'Font': {'T1_0': <PDFObjRef:1060>, 'T1_1': <PDFObjRef:1097>}, 'ProcSet': [/'PDF', /'Text']}, 'Subtype': /'Form'}>\n",
      "2020-11-18 18:32:28.736 INFO    pdfminer.pdfinterp: render_contents: resources={'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>}, 'Font': {'T1_0': <PDFObjRef:1060>, 'T1_1': <PDFObjRef:1097>}, 'ProcSet': [/'PDF', /'Text']}, streams=[<PDFStream(1059): len=554, {'BBox': [0.0, 792.0, 1224.0, 0.0], 'Filter': /'FlateDecode', 'Length': 315, 'Matrix': [1.0, 0.0, 0.0, 1.0, 0.0, 0.0], 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>}, 'Font': {'T1_0': <PDFObjRef:1060>, 'T1_1': <PDFObjRef:1097>}, 'ProcSet': [/'PDF', /'Text']}, 'Subtype': /'Form'}>], ctm=(1.0, 0.0, 0.0, 1.0, 0.0, 0.0)\n",
      "2020-11-18 18:32:28.882 INFO    pdfminer.pdfpage: Page: {'ArtBox': [0.0, 0.0, 1224.0, 792.0], 'BleedBox': [0.0, 0.0, 1224.0, 792.0], 'Contents': <PDFObjRef:1044>, 'CropBox': [0.0, 0.0, 1224.0, 792.0], 'Group': <PDFObjRef:1051>, 'MediaBox': [0.0, 0.0, 1224.0, 792.0], 'Parent': <PDFObjRef:2401>, 'PieceInfo': {'InDesign': {'DocumentID': b'\\xfe\\xff\\x00x\\x00m\\x00p\\x00.\\x00d\\x00i\\x00d\\x00:\\x009\\x003\\x001\\x00f\\x002\\x000\\x007\\x00d\\x00-\\x002\\x005\\x002\\x002\\x00-\\x004\\x005\\x004\\x009\\x00-\\x008\\x006\\x009\\x00d\\x00-\\x00b\\x001\\x007\\x008\\x006\\x006\\x001\\x000\\x009\\x002\\x00b\\x006', 'LastModified': b'\\xfe\\xff\\x00D\\x00:\\x002\\x000\\x002\\x000\\x001\\x000\\x000\\x001\\x002\\x001\\x003\\x005\\x002\\x002\\x00Z', 'NumberofPages': 2, 'OriginalDocumentID': b'\\xfe\\xff\\x00x\\x00m\\x00p\\x00.\\x00d\\x00i\\x00d\\x00:\\x009\\x008\\x00c\\x00f\\x003\\x009\\x00e\\x000\\x00-\\x00c\\x004\\x001\\x00d\\x00-\\x004\\x007\\x007\\x00d\\x00-\\x00a\\x003\\x000\\x002\\x00-\\x008\\x005\\x00a\\x005\\x00d\\x001\\x000\\x006\\x00e\\x002\\x00a\\x00e', 'PageUIDList': {'0': 211823, '1': 267778}, 'PageWidthList': {'0': 612.0, '1': 612.0}}}, 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>, 'GS1': <PDFObjRef:1251>}, 'Font': {'T1_0': <PDFObjRef:2427>}, 'ProcSet': [/'PDF', /'Text', /'ImageB'], 'XObject': {'Fm0': <PDFObjRef:1046>, 'Im0': <PDFObjRef:1050>}}, 'Rotate': 0, 'Tabs': /'W', 'Thumb': <PDFObjRef:2388>, 'TrimBox': [0.0, 0.0, 1224.0, 792.0], 'Type': /'Page'}\n",
      "2020-11-18 18:32:28.884 INFO    pdfminer.pdfinterp: Processing page: <PDFPage: Resources={'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>, 'GS1': <PDFObjRef:1251>}, 'Font': {'T1_0': <PDFObjRef:2427>}, 'ProcSet': [/'PDF', /'Text', /'ImageB'], 'XObject': {'Fm0': <PDFObjRef:1046>, 'Im0': <PDFObjRef:1050>}}, MediaBox=[0.0, 0.0, 1224.0, 792.0]>\n",
      "2020-11-18 18:32:28.893 INFO    pdfminer.pdfinterp: render_contents: resources={'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>, 'GS1': <PDFObjRef:1251>}, 'Font': {'T1_0': <PDFObjRef:2427>}, 'ProcSet': [/'PDF', /'Text', /'ImageB'], 'XObject': {'Fm0': <PDFObjRef:1046>, 'Im0': <PDFObjRef:1050>}}, streams=[<PDFStream(1044): raw=1226, {'Filter': /'FlateDecode', 'Length': 1224}>], ctm=(1, 0, 0, 1, -0.0, -0.0)\n",
      "2020-11-18 18:32:28.898 INFO    pdfminer.pdfinterp: Processing xobj: <PDFStream(1050): raw=65435, {'BitsPerComponent': 8, 'ColorSpace': /'DeviceGray', 'Filter': /'DCTDecode', 'Height': 529, 'Intent': /'RelativeColorimetric', 'Length': 65433, 'Metadata': <PDFObjRef:1047>, 'Name': /'X', 'SMask': <PDFObjRef:1049>, 'Subtype': /'Image', 'Type': /'XObject', 'Width': 1085}>\n",
      "2020-11-18 18:32:28.901 INFO    pdfminer.pdfinterp: Processing xobj: <PDFStream(1046): raw=57, {'BBox': [-9.0, 799.0, 1233.0, -1.0], 'Filter': /'FlateDecode', 'Group': <PDFObjRef:1045>, 'Length': 55, 'Matrix': [1.0, 0.0, 0.0, 1.0, 0.0, 0.0], 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>}}, 'Subtype': /'Form'}>\n",
      "2020-11-18 18:32:28.903 INFO    pdfminer.pdfinterp: render_contents: resources={'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>}}, streams=[<PDFStream(1046): raw=57, {'BBox': [-9.0, 799.0, 1233.0, -1.0], 'Filter': /'FlateDecode', 'Group': <PDFObjRef:1045>, 'Length': 55, 'Matrix': [1.0, 0.0, 0.0, 1.0, 0.0, 0.0], 'Resources': {'ColorSpace': {'CS0': <PDFObjRef:2419>}, 'ExtGState': {'GS0': <PDFObjRef:2418>}}, 'Subtype': /'Form'}>], ctm=(1.0, 0.0, 0.0, 1.0, 0.0, 0.0)\n",
      "100%|██████████| 32/32 [11:40<00:00, 21.90s/it]\n",
      "100%|██████████| 31/31 [00:04<00:00,  7.66it/s]\n"
     ]
    }
   ],
   "source": [
    "df_processed = preprocess.load_processed_text(\n",
    "    df_download,\n",
    "    dir_read_pdf=DIR_REPORTS, \n",
    "    columns_to_keep = [\"company\",\"industry\", \"sector\", \"year\", \"url\", \"filename\"],\n",
    "    file_processed_text=\"output/CRS_processed_pdfMiner.txt\",\n",
    "    n_min_word_paragraph=50, \n",
    "    n_max_word_paragraph=125,  \n",
    "    update=True,\n",
    "    method_extract_content = \"pdfMiner\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "(6283, 7)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           company                 industry     sector  year  \\\n",
       "0  Brookfield Property Partners LP  Real Estate Development  Financial  2019   \n",
       "1  Brookfield Property Partners LP  Real Estate Development  Financial  2019   \n",
       "2  Brookfield Property Partners LP  Real Estate Development  Financial  2019   \n",
       "3  Brookfield Property Partners LP  Real Estate Development  Financial  2019   \n",
       "4  Brookfield Property Partners LP  Real Estate Development  Financial  2019   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.brookfield.com/sites/default/files...   \n",
       "1  https://www.brookfield.com/sites/default/files...   \n",
       "2  https://www.brookfield.com/sites/default/files...   \n",
       "3  https://www.brookfield.com/sites/default/files...   \n",
       "4  https://www.brookfield.com/sites/default/files...   \n",
       "\n",
       "                              filename  \\\n",
       "0  ArmadaHofflerPropertiesInc-2019.pdf   \n",
       "1  ArmadaHofflerPropertiesInc-2019.pdf   \n",
       "2  ArmadaHofflerPropertiesInc-2019.pdf   \n",
       "3  ArmadaHofflerPropertiesInc-2019.pdf   \n",
       "4  ArmadaHofflerPropertiesInc-2019.pdf   \n",
       "\n",
       "                                           paragraph  \n",
       "0   Armada Hoffler Throughout Armada Hofflers yea...  \n",
       "1  We recognize the importance of sustainability ...  \n",
       "2  As we explore new projects we approach the pla...  \n",
       "3  As forty year old company our focus has always...  \n",
       "4  employer match Employer paid life and AD insur...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>company</th>\n      <th>industry</th>\n      <th>sector</th>\n      <th>year</th>\n      <th>url</th>\n      <th>filename</th>\n      <th>paragraph</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Brookfield Property Partners LP</td>\n      <td>Real Estate Development</td>\n      <td>Financial</td>\n      <td>2019</td>\n      <td>https://www.brookfield.com/sites/default/files...</td>\n      <td>ArmadaHofflerPropertiesInc-2019.pdf</td>\n      <td>Armada Hoffler Throughout Armada Hofflers yea...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Brookfield Property Partners LP</td>\n      <td>Real Estate Development</td>\n      <td>Financial</td>\n      <td>2019</td>\n      <td>https://www.brookfield.com/sites/default/files...</td>\n      <td>ArmadaHofflerPropertiesInc-2019.pdf</td>\n      <td>We recognize the importance of sustainability ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Brookfield Property Partners LP</td>\n      <td>Real Estate Development</td>\n      <td>Financial</td>\n      <td>2019</td>\n      <td>https://www.brookfield.com/sites/default/files...</td>\n      <td>ArmadaHofflerPropertiesInc-2019.pdf</td>\n      <td>As we explore new projects we approach the pla...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Brookfield Property Partners LP</td>\n      <td>Real Estate Development</td>\n      <td>Financial</td>\n      <td>2019</td>\n      <td>https://www.brookfield.com/sites/default/files...</td>\n      <td>ArmadaHofflerPropertiesInc-2019.pdf</td>\n      <td>As forty year old company our focus has always...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Brookfield Property Partners LP</td>\n      <td>Real Estate Development</td>\n      <td>Financial</td>\n      <td>2019</td>\n      <td>https://www.brookfield.com/sites/default/files...</td>\n      <td>ArmadaHofflerPropertiesInc-2019.pdf</td>\n      <td>employer match Employer paid life and AD insur...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "print(df_processed.shape)\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# save only the paragraph to a text file\n",
    "df_processed[\"paragraph\"].to_csv('output/CSR_processed_raw_pdfMiner.txt', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# filter out company names\n",
    "own_stop_words = [\"prairiesky\", \"cousin\", \"use\", \"uk\", \"france\", \"landlord\", \"abs\", \"london\"]\n",
    "fsi_stop_words = df_processed[\"company\"].unique().tolist() \n",
    "\n",
    "# our list contains all english stop words + companies names + specific keywords\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(fsi_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "✔ Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# load spacy model to lematize text\n",
    "nlp = preprocess.load_spacy_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "100%|██████████| 6283/6283 [02:02<00:00, 51.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# lematize text and remove stopwords\n",
    "# use method 1\n",
    "\n",
    "df_processed = preprocess.load_lemmatize(\n",
    "    data=df_processed, \n",
    "    dir_file='output/CSR_processed_cleaned_pdfMiner.txt', \n",
    "    stop_words=stop_words, \n",
    "    nlp=nlp, \n",
    "    method=1, \n",
    "    update=True\n",
    ")"
   ]
  },
  {
   "source": [
    "## 1. Contextualized Topic Modelling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1.1 Prepare data for model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "2020-11-18 18:34:51.979 INFO    gensim.corpora.dictionary: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-18 18:34:53.003 INFO    gensim.corpora.dictionary: built Dictionary(8372 unique tokens: ['armada', 'central', 'citizenship', 'come', 'corporate']...) from 6283 documents (total 331204 corpus positions)\n",
      "2020-11-18 18:34:53.028 INFO    gensim.corpora.dictionary: discarding 5992 tokens: [('corporate', 870), ('environmental', 915), ('good', 656), ('hofflers', 1), ('intentional', 1), ('paramount', 5), ('profitability', 3), ('tenet', 1), ('thoughtfully', 1), ('year', 1327)]...\n",
      "2020-11-18 18:34:53.031 INFO    gensim.corpora.dictionary: keeping 2380 tokens which were in no less than 10 and no more than 628 (=10.0%) documents\n",
      "2020-11-18 18:34:53.042 INFO    gensim.corpora.dictionary: resulting dictionary: Dictionary(2380 unique tokens: ['armada', 'central', 'citizenship', 'come', 'effort']...)\n",
      "2380\n"
     ]
    }
   ],
   "source": [
    "with open('output/CSR_processed_cleaned_pdfMiner.txt',\"r\") as fr:\n",
    "    text_cleaned = [doc.split() for doc in fr.read().splitlines()] # load text for NPMI\n",
    "\n",
    "dictionary = Dictionary(text_cleaned)\n",
    "\n",
    "'''\n",
    "Remove very rare and very common words:\n",
    "\n",
    "- words appearing less than 15 times\n",
    "- words appearing in more than 10% of all documents\n",
    "'''\n",
    "\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.10, keep_n= 100000)\n",
    "print(len(dictionary.token2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in text_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6283, 2380)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "tf_array = matutils.corpus2dense(bow_corpus, num_terms=len(dictionary.token2id)).T\n",
    "tf_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<6283x2380 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 166814 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# convert to sparse matrix\n",
    "tf_array_sparse = sparse.csr_matrix(tf_array)\n",
    "tf_array_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "2020-11-18 18:52:34.793 INFO    root: Load pretrained SentenceTransformer: distiluse-base-multilingual-cased\n",
      "2020-11-18 18:52:34.795 INFO    root: Did not find folder distiluse-base-multilingual-cased. Assume to download model from server.\n",
      "2020-11-18 18:52:34.817 INFO    root: Load SentenceTransformer from folder: C:\\Users\\Pieter-Jan/.cache\\torch\\sentence_transformers\\sbert.net_models_distiluse-base-multilingual-cased\n",
      "2020-11-18 18:52:40.308 INFO    root: Use pytorch device: cpu\n",
      "Batches: 100%|██████████| 32/32 [24:50<00:00, 46.56s/it]\n"
     ]
    }
   ],
   "source": [
    "# create or load bert embeddings (either use raw text or clean text)\n",
    "# we can expirment with both\n",
    "embeddings_bert = preprocess.load_bert_embeddings(\n",
    "        text_dir=\"output/CSR_processed_raw_pdfMiner.txt\", \n",
    "        model=\"distiluse-base-multilingual-cased\",\n",
    "        dir_embeddings=\"output/CRS_bertEmbeddings_pdfMiner.npy\",\n",
    "        update=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6283, 512)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "embeddings_bert.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# ivert dictionary\n",
    "inv_token2id = {v: k for k, v in dictionary.token2id.items()}\n",
    "# create dataset\n",
    "training_dataset = CTMDataset(tf_array_sparse, embeddings_bert, inv_token2id)"
   ]
  },
  {
   "source": [
    "## 1.2 Train model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "Settings: \n",
      "                   N Components: 20\n",
      "                   Topic Prior Mean: 0.0\n",
      "                   Topic Prior Variance: 0.95\n",
      "                   Model Type: prodLDA\n",
      "                   Hidden Sizes: (100, 100)\n",
      "                   Activation: softplus\n",
      "                   Dropout: 0.2\n",
      "                   Learn Priors: True\n",
      "                   Learning Rate: 0.002\n",
      "                   Momentum: 0.99\n",
      "                   Reduce On Plateau: True\n",
      "                   Save Dir: None\n",
      "Epoch: [1/10]\tSamples: [6283/62830]\tTrain Loss: 291.5454813817419\tTime: 0:00:16.991350\n",
      "Epoch: [2/10]\tSamples: [12566/62830]\tTrain Loss: 277.4917287891806\tTime: 0:00:16.828569\n",
      "Epoch: [3/10]\tSamples: [18849/62830]\tTrain Loss: 267.1053031400456\tTime: 0:00:17.689808\n",
      "Epoch: [4/10]\tSamples: [25132/62830]\tTrain Loss: 261.10492657362875\tTime: 0:00:17.956293\n",
      "Epoch: [5/10]\tSamples: [31415/62830]\tTrain Loss: 257.3761465633331\tTime: 0:00:14.616640\n",
      "Epoch: [6/10]\tSamples: [37698/62830]\tTrain Loss: 255.10832810572677\tTime: 0:00:15.613788\n",
      "Epoch: [7/10]\tSamples: [43981/62830]\tTrain Loss: 253.52848840932376\tTime: 0:00:16.565305\n",
      "Epoch: [8/10]\tSamples: [50264/62830]\tTrain Loss: 252.14685060536615\tTime: 0:00:16.006052\n",
      "Epoch: [9/10]\tSamples: [56547/62830]\tTrain Loss: 251.04312316904245\tTime: 0:00:12.936597\n",
      "Epoch: [10/10]\tSamples: [62830/62830]\tTrain Loss: 250.40712751515747\tTime: 0:00:14.898309\n"
     ]
    }
   ],
   "source": [
    "random.seed(69)\n",
    "ctm = CTM(\n",
    "    input_size=len(dictionary.token2id), \n",
    "    bert_input_size=512, \n",
    "    n_components=20, \n",
    "    inference_type=\"combined\", \n",
    "    num_epochs=10,\n",
    "    reduce_on_plateau=True\n",
    "    )\n",
    "ctm.fit(training_dataset) # run model"
   ]
  },
  {
   "source": [
    "## 1.3 Evaluate topics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "cmt_topics_l = ctm.get_topic_lists(10)\n",
    "cmt_topics_d = {}\n",
    "for i in range(len(cmt_topics_l)):\n",
    "    cmt_topics_d[i] = cmt_topics_l[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             0             1           2             3            4  \\\n",
       "0     training         focus        need     programme      deliver   \n",
       "1     economic          hall     charter     financial     material   \n",
       "2       senior    disclosure   assurance  organisation    indicator   \n",
       "3      recycle      landfill   recycling      disposal        reuse   \n",
       "4   prairiesky       royalty    operator      industry  prairieskys   \n",
       "5    charlotte         white      collar       factory       brunel   \n",
       "6        board      director  governance     committee       policy   \n",
       "7        local        people    activity         skill   employment   \n",
       "8         park        bullre        data       reading       italie   \n",
       "9        scope     intensity        like          mtco      derwent   \n",
       "10      design        review     measure        breeam     internal   \n",
       "11      street         build        site        rating      average   \n",
       "12       chain      supplier      supply    efficiency        issue   \n",
       "13      record        direct   intensity      indirect        scope   \n",
       "14   challenge          look        help     programme         role   \n",
       "15       baker       section       atkin       summary    subscribe   \n",
       "16    landlord   electricity   intensity          like      natural   \n",
       "17      factor        travel    estimate          cost       french   \n",
       "18      desire  continuously       baker     subscribe     underway   \n",
       "19      health          risk     climate        safety       relate   \n",
       "\n",
       "             5              6             7               8               9  \n",
       "0       survey          local   opportunity           staff            wide  \n",
       "1    indicator    significant    investment          social         climate  \n",
       "2    statement         female         index            real          review  \n",
       "3     quantity          tonne     hazardous            fuel          divert  \n",
       "4         long          share         lease         company  responsibility  \n",
       "5        angel         oxford        street          embody         stephen  \n",
       "6       social      executive          high  responsibility          health  \n",
       "7      partner        charity  contribution       volunteer       wellbeing  \n",
       "8     westquay           sell    birmingham       highcross           brent  \n",
       "9        indir     greenhouse     assurance            kgco        location  \n",
       "10  assessment    sustainable  construction        supplier        complete  \n",
       "11        cpof  refurbishment          hall          design           major  \n",
       "12      expect          exist   legislation          action            need  \n",
       "13      figure           cost        source     electricity          factor  \n",
       "14        make      volunteer         learn       colleague           great  \n",
       "15     analyse         fiscal      underway            pool      estimation  \n",
       "16      obtain        heating     renewable            elec           meter  \n",
       "17   calculate          defra          data         vehicle         average  \n",
       "18   recycling     federation        making        delegate        reliance  \n",
       "19    incident         injury     potential        fatality      regulation  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>training</td>\n      <td>focus</td>\n      <td>need</td>\n      <td>programme</td>\n      <td>deliver</td>\n      <td>survey</td>\n      <td>local</td>\n      <td>opportunity</td>\n      <td>staff</td>\n      <td>wide</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>economic</td>\n      <td>hall</td>\n      <td>charter</td>\n      <td>financial</td>\n      <td>material</td>\n      <td>indicator</td>\n      <td>significant</td>\n      <td>investment</td>\n      <td>social</td>\n      <td>climate</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>senior</td>\n      <td>disclosure</td>\n      <td>assurance</td>\n      <td>organisation</td>\n      <td>indicator</td>\n      <td>statement</td>\n      <td>female</td>\n      <td>index</td>\n      <td>real</td>\n      <td>review</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>recycle</td>\n      <td>landfill</td>\n      <td>recycling</td>\n      <td>disposal</td>\n      <td>reuse</td>\n      <td>quantity</td>\n      <td>tonne</td>\n      <td>hazardous</td>\n      <td>fuel</td>\n      <td>divert</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>prairiesky</td>\n      <td>royalty</td>\n      <td>operator</td>\n      <td>industry</td>\n      <td>prairieskys</td>\n      <td>long</td>\n      <td>share</td>\n      <td>lease</td>\n      <td>company</td>\n      <td>responsibility</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>charlotte</td>\n      <td>white</td>\n      <td>collar</td>\n      <td>factory</td>\n      <td>brunel</td>\n      <td>angel</td>\n      <td>oxford</td>\n      <td>street</td>\n      <td>embody</td>\n      <td>stephen</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>board</td>\n      <td>director</td>\n      <td>governance</td>\n      <td>committee</td>\n      <td>policy</td>\n      <td>social</td>\n      <td>executive</td>\n      <td>high</td>\n      <td>responsibility</td>\n      <td>health</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>local</td>\n      <td>people</td>\n      <td>activity</td>\n      <td>skill</td>\n      <td>employment</td>\n      <td>partner</td>\n      <td>charity</td>\n      <td>contribution</td>\n      <td>volunteer</td>\n      <td>wellbeing</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>park</td>\n      <td>bullre</td>\n      <td>data</td>\n      <td>reading</td>\n      <td>italie</td>\n      <td>westquay</td>\n      <td>sell</td>\n      <td>birmingham</td>\n      <td>highcross</td>\n      <td>brent</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>scope</td>\n      <td>intensity</td>\n      <td>like</td>\n      <td>mtco</td>\n      <td>derwent</td>\n      <td>indir</td>\n      <td>greenhouse</td>\n      <td>assurance</td>\n      <td>kgco</td>\n      <td>location</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>design</td>\n      <td>review</td>\n      <td>measure</td>\n      <td>breeam</td>\n      <td>internal</td>\n      <td>assessment</td>\n      <td>sustainable</td>\n      <td>construction</td>\n      <td>supplier</td>\n      <td>complete</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>street</td>\n      <td>build</td>\n      <td>site</td>\n      <td>rating</td>\n      <td>average</td>\n      <td>cpof</td>\n      <td>refurbishment</td>\n      <td>hall</td>\n      <td>design</td>\n      <td>major</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>chain</td>\n      <td>supplier</td>\n      <td>supply</td>\n      <td>efficiency</td>\n      <td>issue</td>\n      <td>expect</td>\n      <td>exist</td>\n      <td>legislation</td>\n      <td>action</td>\n      <td>need</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>record</td>\n      <td>direct</td>\n      <td>intensity</td>\n      <td>indirect</td>\n      <td>scope</td>\n      <td>figure</td>\n      <td>cost</td>\n      <td>source</td>\n      <td>electricity</td>\n      <td>factor</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>challenge</td>\n      <td>look</td>\n      <td>help</td>\n      <td>programme</td>\n      <td>role</td>\n      <td>make</td>\n      <td>volunteer</td>\n      <td>learn</td>\n      <td>colleague</td>\n      <td>great</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>baker</td>\n      <td>section</td>\n      <td>atkin</td>\n      <td>summary</td>\n      <td>subscribe</td>\n      <td>analyse</td>\n      <td>fiscal</td>\n      <td>underway</td>\n      <td>pool</td>\n      <td>estimation</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>landlord</td>\n      <td>electricity</td>\n      <td>intensity</td>\n      <td>like</td>\n      <td>natural</td>\n      <td>obtain</td>\n      <td>heating</td>\n      <td>renewable</td>\n      <td>elec</td>\n      <td>meter</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>factor</td>\n      <td>travel</td>\n      <td>estimate</td>\n      <td>cost</td>\n      <td>french</td>\n      <td>calculate</td>\n      <td>defra</td>\n      <td>data</td>\n      <td>vehicle</td>\n      <td>average</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>desire</td>\n      <td>continuously</td>\n      <td>baker</td>\n      <td>subscribe</td>\n      <td>underway</td>\n      <td>recycling</td>\n      <td>federation</td>\n      <td>making</td>\n      <td>delegate</td>\n      <td>reliance</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>health</td>\n      <td>risk</td>\n      <td>climate</td>\n      <td>safety</td>\n      <td>relate</td>\n      <td>incident</td>\n      <td>injury</td>\n      <td>potential</td>\n      <td>fatality</td>\n      <td>regulation</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(cmt_topics_d).T"
   ]
  },
  {
   "source": [
    "### 1.3.1 Normalized Point-wise Mutual Information"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "2020-11-18 19:20:14.768 INFO    gensim.corpora.dictionary: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-18 19:20:15.230 INFO    gensim.corpora.dictionary: built Dictionary(8372 unique tokens: ['armada', 'central', 'citizenship', 'come', 'corporate']...) from 6283 documents (total 331204 corpus positions)\n",
      "2020-11-18 19:20:15.241 INFO    gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator(processes=3, batch_size=64) to estimate probabilities from sliding windows\n",
      "2020-11-18 19:20:19.356 INFO    gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (3000 virtual)\n",
      "2020-11-18 19:20:19.359 INFO    gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (6379 virtual)\n",
      "2020-11-18 19:20:19.363 INFO    gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (9391 virtual)\n",
      "2020-11-18 19:20:19.366 INFO    gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (12062 virtual)\n",
      "2020-11-18 19:20:19.369 INFO    gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (15185 virtual)\n",
      "2020-11-18 19:20:19.374 INFO    gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (18275 virtual)\n",
      "2020-11-18 19:20:19.435 INFO    gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (20912 virtual)\n",
      "2020-11-18 19:20:19.458 INFO    gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (23505 virtual)\n",
      "2020-11-18 19:20:19.471 INFO    gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (26478 virtual)\n",
      "2020-11-18 19:20:19.522 INFO    gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (29566 virtual)\n",
      "2020-11-18 19:20:19.544 INFO    gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (31990 virtual)\n",
      "2020-11-18 19:20:19.576 INFO    gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (34627 virtual)\n",
      "2020-11-18 19:20:19.614 INFO    gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (37603 virtual)\n",
      "2020-11-18 19:20:19.651 INFO    gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (39941 virtual)\n",
      "2020-11-18 19:20:19.666 INFO    gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (42700 virtual)\n",
      "2020-11-18 19:20:19.718 INFO    gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (45446 virtual)\n",
      "2020-11-18 19:20:19.723 INFO    gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (48324 virtual)\n",
      "2020-11-18 19:20:19.751 INFO    gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (50858 virtual)\n",
      "2020-11-18 19:20:19.804 INFO    gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (53332 virtual)\n",
      "2020-11-18 19:20:19.818 INFO    gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (56289 virtual)\n",
      "2020-11-18 19:20:19.853 INFO    gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (58950 virtual)\n",
      "2020-11-18 19:20:19.917 INFO    gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (61632 virtual)\n",
      "2020-11-18 19:20:19.974 INFO    gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (64663 virtual)\n",
      "2020-11-18 19:20:19.981 INFO    gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (67386 virtual)\n",
      "2020-11-18 19:20:20.024 INFO    gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (70473 virtual)\n",
      "2020-11-18 19:20:20.086 INFO    gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (73453 virtual)\n",
      "2020-11-18 19:20:20.096 INFO    gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (76819 virtual)\n",
      "2020-11-18 19:20:20.142 INFO    gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (79374 virtual)\n",
      "2020-11-18 19:20:20.203 INFO    gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (82375 virtual)\n",
      "2020-11-18 19:20:20.227 INFO    gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (85632 virtual)\n",
      "2020-11-18 19:20:20.291 INFO    gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (88540 virtual)\n",
      "2020-11-18 19:20:20.380 INFO    gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (91996 virtual)\n",
      "2020-11-18 19:20:20.384 INFO    gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (94959 virtual)\n",
      "2020-11-18 19:20:20.444 INFO    gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (98024 virtual)\n",
      "2020-11-18 19:20:20.504 INFO    gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (101058 virtual)\n",
      "2020-11-18 19:20:20.514 INFO    gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (103765 virtual)\n",
      "2020-11-18 19:20:20.575 INFO    gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (105982 virtual)\n",
      "2020-11-18 19:20:20.617 INFO    gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (108347 virtual)\n",
      "2020-11-18 19:20:20.629 INFO    gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (110988 virtual)\n",
      "2020-11-18 19:20:20.671 INFO    gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (113437 virtual)\n",
      "2020-11-18 19:20:20.697 INFO    gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (115511 virtual)\n",
      "2020-11-18 19:20:20.718 INFO    gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (118257 virtual)\n",
      "2020-11-18 19:20:20.752 INFO    gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (120838 virtual)\n",
      "2020-11-18 19:20:20.784 INFO    gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (123875 virtual)\n",
      "2020-11-18 19:20:20.801 INFO    gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (126873 virtual)\n",
      "2020-11-18 19:20:20.841 INFO    gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (129543 virtual)\n",
      "2020-11-18 19:20:20.857 INFO    gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (132120 virtual)\n",
      "2020-11-18 19:20:20.905 INFO    gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (134702 virtual)\n",
      "2020-11-18 19:20:20.935 INFO    gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (136768 virtual)\n",
      "2020-11-18 19:20:20.975 INFO    gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (138858 virtual)\n",
      "2020-11-18 19:20:21.016 INFO    gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (140771 virtual)\n",
      "2020-11-18 19:20:21.028 INFO    gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (143074 virtual)\n",
      "2020-11-18 19:20:21.074 INFO    gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (145788 virtual)\n",
      "2020-11-18 19:20:21.113 INFO    gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (148745 virtual)\n",
      "2020-11-18 19:20:21.117 INFO    gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (152358 virtual)\n",
      "2020-11-18 19:20:21.166 INFO    gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (156258 virtual)\n",
      "2020-11-18 19:20:21.186 INFO    gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (160130 virtual)\n",
      "2020-11-18 19:20:21.196 INFO    gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (163204 virtual)\n",
      "2020-11-18 19:20:21.269 INFO    gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (166041 virtual)\n",
      "2020-11-18 19:20:21.300 INFO    gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (168935 virtual)\n",
      "2020-11-18 19:20:21.332 INFO    gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (171745 virtual)\n",
      "2020-11-18 19:20:21.454 INFO    gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (174715 virtual)\n",
      "2020-11-18 19:20:21.460 INFO    gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (177395 virtual)\n",
      "2020-11-18 19:20:21.465 INFO    gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (180283 virtual)\n",
      "2020-11-18 19:20:21.546 INFO    gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (182875 virtual)\n",
      "2020-11-18 19:20:21.549 INFO    gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (185932 virtual)\n",
      "2020-11-18 19:20:21.561 INFO    gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (188601 virtual)\n",
      "2020-11-18 19:20:21.654 INFO    gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (191100 virtual)\n",
      "2020-11-18 19:20:21.668 INFO    gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (193922 virtual)\n",
      "2020-11-18 19:20:21.677 INFO    gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (196568 virtual)\n",
      "2020-11-18 19:20:21.751 INFO    gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (199480 virtual)\n",
      "2020-11-18 19:20:21.767 INFO    gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (201880 virtual)\n",
      "2020-11-18 19:20:21.782 INFO    gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (204619 virtual)\n",
      "2020-11-18 19:20:21.839 INFO    gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (207738 virtual)\n",
      "2020-11-18 19:20:21.862 INFO    gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (210668 virtual)\n",
      "2020-11-18 19:20:21.879 INFO    gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (213990 virtual)\n",
      "2020-11-18 19:20:21.937 INFO    gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (217145 virtual)\n",
      "2020-11-18 19:20:21.950 INFO    gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (219776 virtual)\n",
      "2020-11-18 19:20:21.970 INFO    gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (223050 virtual)\n",
      "2020-11-18 19:20:22.047 INFO    gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (226023 virtual)\n",
      "2020-11-18 19:20:22.075 INFO    gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (228901 virtual)\n",
      "2020-11-18 19:20:22.154 INFO    gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (232333 virtual)\n",
      "2020-11-18 19:20:22.233 INFO    gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (235739 virtual)\n",
      "2020-11-18 19:20:22.244 INFO    gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (238442 virtual)\n",
      "2020-11-18 19:20:22.300 INFO    gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (241337 virtual)\n",
      "2020-11-18 19:20:22.366 INFO    gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (243607 virtual)\n",
      "2020-11-18 19:20:22.375 INFO    gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (246013 virtual)\n",
      "2020-11-18 19:20:22.439 INFO    gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (248626 virtual)\n",
      "2020-11-18 19:20:22.473 INFO    gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (251376 virtual)\n",
      "2020-11-18 19:20:22.500 INFO    gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (254488 virtual)\n",
      "2020-11-18 19:20:22.573 INFO    gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (257101 virtual)\n",
      "2020-11-18 19:20:22.581 INFO    gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (259982 virtual)\n",
      "2020-11-18 19:20:22.597 INFO    gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (262783 virtual)\n",
      "2020-11-18 19:20:22.659 INFO    gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (265374 virtual)\n",
      "2020-11-18 19:20:22.675 INFO    gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (267835 virtual)\n",
      "2020-11-18 19:20:22.706 INFO    gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (270598 virtual)\n",
      "2020-11-18 19:20:22.747 INFO    gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (273628 virtual)\n",
      "2020-11-18 19:20:22.782 INFO    gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (274077 virtual)\n",
      "2020-11-18 19:20:22.986 INFO    gensim.topic_coherence.text_analysis: 3 accumulators retrieved from output queue\n",
      "2020-11-18 19:20:23.021 INFO    gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 274190 virtual documents\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.05363407861000221"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "npmi = CoherenceNPMI(texts=text_cleaned, topics=ctm.get_topic_lists(10))\n",
    "npmi.score()"
   ]
  },
  {
   "source": [
    "### 1.3.2 External Word Embeddings Topic Coherence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "2020-11-18 19:20:23.795 INFO    gensim.models.utils_any2vec: loading projection weights from C:\\Users\\Pieter-Jan/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n",
      "2020-11-18 19:21:25.006 INFO    gensim.models.utils_any2vec: loaded (3000000, 300) matrix from C:\\Users\\Pieter-Jan/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.13012877"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "CoherenceWordEmbeddings(ctm.get_topic_lists(10)).score()"
   ]
  },
  {
   "source": [
    "### 1.3.3 Rank-Biased Overlap "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9903040631643233"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "InvertedRBO(ctm.get_topic_lists(10)).score()"
   ]
  },
  {
   "source": [
    "## 2. Latent Dirichlet Allocation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.1 Train model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "lete\" + 0.010*\"green\" + 0.010*\"site\" + 0.010*\"recycling\" + 0.008*\"rating\" + 0.008*\"build\" + 0.008*\"refurbishment\"\n",
      "2020-11-18 19:22:00.856 INFO    gensim.models.ldamodel: topic #8 (0.050): 0.035*\"tonne\" + 0.035*\"recycle\" + 0.030*\"landfill\" + 0.025*\"hazardous\" + 0.025*\"fuel\" + 0.019*\"reuse\" + 0.019*\"material\" + 0.019*\"send\" + 0.019*\"food\" + 0.019*\"assurance\"\n",
      "2020-11-18 19:22:00.857 INFO    gensim.models.ldamodel: topic diff=0.772409, rho=0.350468\n",
      "2020-11-18 19:22:01.080 INFO    gensim.models.ldamodel: -7.018 per-word bound, 129.6 perplexity estimate based on a held-out corpus of 283 documents with 9843 words\n",
      "2020-11-18 19:22:01.082 INFO    gensim.models.ldamulticore: PROGRESS: pass 5, dispatched chunk #0 = documents up to #2000/6283, outstanding queue size 1\n",
      "2020-11-18 19:22:01.100 INFO    gensim.models.ldamulticore: PROGRESS: pass 5, dispatched chunk #1 = documents up to #4000/6283, outstanding queue size 2\n",
      "2020-11-18 19:22:01.125 INFO    gensim.models.ldamulticore: PROGRESS: pass 5, dispatched chunk #2 = documents up to #6000/6283, outstanding queue size 3\n",
      "2020-11-18 19:22:01.164 INFO    gensim.models.ldamulticore: PROGRESS: pass 5, dispatched chunk #3 = documents up to #6283/6283, outstanding queue size 4\n",
      "2020-11-18 19:22:03.109 INFO    gensim.models.ldamodel: merging changes from 4000 documents into a model of 6283 documents\n",
      "2020-11-18 19:22:03.115 INFO    gensim.models.ldamodel: topic #17 (0.050): 0.051*\"street\" + 0.018*\"rating\" + 0.016*\"build\" + 0.014*\"star\" + 0.014*\"member\" + 0.014*\"average\" + 0.012*\"site\" + 0.011*\"green\" + 0.011*\"white\" + 0.009*\"certification\"\n",
      "2020-11-18 19:22:03.117 INFO    gensim.models.ldamodel: topic #13 (0.050): 0.023*\"charter\" + 0.023*\"hall\" + 0.016*\"risk\" + 0.016*\"positive\" + 0.013*\"economic\" + 0.012*\"issue\" + 0.010*\"review\" + 0.010*\"customer\" + 0.009*\"climate\" + 0.008*\"investor\"\n",
      "2020-11-18 19:22:03.119 INFO    gensim.models.ldamodel: topic #0 (0.050): 0.041*\"scope\" + 0.031*\"like\" + 0.027*\"indicator\" + 0.015*\"greenhouse\" + 0.012*\"intensity\" + 0.011*\"indirect\" + 0.010*\"core\" + 0.010*\"source\" + 0.009*\"utility\" + 0.009*\"factor\"\n",
      "2020-11-18 19:22:03.121 INFO    gensim.models.ldamodel: topic #7 (0.050): 0.015*\"visitor\" + 0.014*\"landlord\" + 0.012*\"positive\" + 0.011*\"rainwater\" + 0.011*\"focus\" + 0.011*\"stakeholder\" + 0.010*\"service\" + 0.009*\"survey\" + 0.009*\"investor\" + 0.009*\"deliver\"\n",
      "2020-11-18 19:22:03.122 INFO    gensim.models.ldamodel: topic #14 (0.050): 0.064*\"park\" + 0.019*\"travel\" + 0.018*\"sell\" + 0.011*\"average\" + 0.010*\"house\" + 0.009*\"estate\" + 0.009*\"investment\" + 0.009*\"brent\" + 0.009*\"defra\" + 0.008*\"journey\"\n",
      "2020-11-18 19:22:03.124 INFO    gensim.models.ldamodel: topic diff=0.681843, rho=0.330743\n",
      "2020-11-18 19:22:03.462 INFO    gensim.models.ldamodel: -7.084 per-word bound, 135.7 perplexity estimate based on a held-out corpus of 283 documents with 9843 words\n",
      "2020-11-18 19:22:04.558 INFO    gensim.models.ldamodel: merging changes from 2283 documents into a model of 6283 documents\n",
      "2020-11-18 19:22:04.563 INFO    gensim.models.ldamodel: topic #8 (0.050): 0.038*\"tonne\" + 0.037*\"recycle\" + 0.033*\"landfill\" + 0.026*\"hazardous\" + 0.025*\"fuel\" + 0.021*\"material\" + 0.021*\"reuse\" + 0.020*\"food\" + 0.020*\"send\" + 0.020*\"assurance\"\n",
      "2020-11-18 19:22:04.566 INFO    gensim.models.ldamodel: topic #10 (0.050): 0.021*\"design\" + 0.016*\"breeam\" + 0.011*\"green\" + 0.010*\"site\" + 0.010*\"complete\" + 0.010*\"major\" + 0.009*\"rating\" + 0.009*\"recycling\" + 0.008*\"excellent\" + 0.008*\"build\"\n",
      "2020-11-18 19:22:04.568 INFO    gensim.models.ldamodel: topic #9 (0.050): 0.046*\"risk\" + 0.034*\"board\" + 0.025*\"governance\" + 0.021*\"committee\" + 0.020*\"climate\" + 0.017*\"policy\" + 0.015*\"director\" + 0.012*\"executive\" + 0.011*\"responsibility\" + 0.009*\"process\"\n",
      "2020-11-18 19:22:04.570 INFO    gensim.models.ldamodel: topic #6 (0.050): 0.035*\"cost\" + 0.023*\"data\" + 0.013*\"value\" + 0.012*\"intensity\" + 0.012*\"million\" + 0.012*\"electricity\" + 0.011*\"prairiesky\" + 0.010*\"charge\" + 0.009*\"investment\" + 0.009*\"note\"\n",
      "2020-11-18 19:22:04.574 INFO    gensim.models.ldamodel: topic #13 (0.050): 0.021*\"positive\" + 0.020*\"risk\" + 0.016*\"charter\" + 0.016*\"hall\" + 0.015*\"economic\" + 0.013*\"issue\" + 0.010*\"socio\" + 0.010*\"review\" + 0.010*\"climate\" + 0.009*\"stakeholder\"\n",
      "2020-11-18 19:22:04.577 INFO    gensim.models.ldamodel: topic diff=0.672973, rho=0.330743\n",
      "2020-11-18 19:22:04.791 INFO    gensim.models.ldamodel: -6.980 per-word bound, 126.2 perplexity estimate based on a held-out corpus of 283 documents with 9843 words\n",
      "2020-11-18 19:22:04.792 INFO    gensim.models.ldamulticore: PROGRESS: pass 6, dispatched chunk #0 = documents up to #2000/6283, outstanding queue size 1\n",
      "2020-11-18 19:22:04.810 INFO    gensim.models.ldamulticore: PROGRESS: pass 6, dispatched chunk #1 = documents up to #4000/6283, outstanding queue size 2\n",
      "2020-11-18 19:22:04.838 INFO    gensim.models.ldamulticore: PROGRESS: pass 6, dispatched chunk #2 = documents up to #6000/6283, outstanding queue size 3\n",
      "2020-11-18 19:22:04.863 INFO    gensim.models.ldamulticore: PROGRESS: pass 6, dispatched chunk #3 = documents up to #6283/6283, outstanding queue size 4\n",
      "2020-11-18 19:22:06.414 INFO    gensim.models.ldamodel: merging changes from 4000 documents into a model of 6283 documents\n",
      "2020-11-18 19:22:06.421 INFO    gensim.models.ldamodel: topic #13 (0.050): 0.024*\"charter\" + 0.024*\"hall\" + 0.016*\"positive\" + 0.016*\"risk\" + 0.014*\"economic\" + 0.013*\"issue\" + 0.010*\"customer\" + 0.010*\"review\" + 0.009*\"investor\" + 0.009*\"stakeholder\"\n",
      "2020-11-18 19:22:06.429 INFO    gensim.models.ldamodel: topic #9 (0.050): 0.042*\"risk\" + 0.030*\"board\" + 0.026*\"governance\" + 0.022*\"committee\" + 0.020*\"climate\" + 0.017*\"policy\" + 0.014*\"director\" + 0.013*\"executive\" + 0.011*\"responsibility\" + 0.010*\"process\"\n",
      "2020-11-18 19:22:06.434 INFO    gensim.models.ldamodel: topic #7 (0.050): 0.016*\"visitor\" + 0.012*\"positive\" + 0.012*\"landlord\" + 0.012*\"rainwater\" + 0.011*\"stakeholder\" + 0.011*\"focus\" + 0.011*\"service\" + 0.010*\"deliver\" + 0.009*\"survey\" + 0.009*\"local\"\n",
      "2020-11-18 19:22:06.439 INFO    gensim.models.ldamodel: topic #19 (0.050): 0.015*\"manager\" + 0.015*\"focus\" + 0.012*\"program\" + 0.012*\"company\" + 0.011*\"operation\" + 0.011*\"increase\" + 0.011*\"french\" + 0.010*\"level\" + 0.009*\"benefit\" + 0.008*\"improve\"\n",
      "2020-11-18 19:22:06.444 INFO    gensim.models.ldamodel: topic #17 (0.050): 0.053*\"street\" + 0.019*\"rating\" + 0.017*\"build\" + 0.017*\"star\" + 0.014*\"average\" + 0.014*\"member\" + 0.012*\"site\" + 0.012*\"white\" + 0.012*\"green\" + 0.009*\"certification\"\n",
      "2020-11-18 19:22:06.446 INFO    gensim.models.ldamodel: topic diff=0.584403, rho=0.314014\n",
      "2020-11-18 19:22:06.755 INFO    gensim.models.ldamodel: -7.043 per-word bound, 131.9 perplexity estimate based on a held-out corpus of 283 documents with 9843 words\n",
      "2020-11-18 19:22:07.419 INFO    gensim.models.ldamodel: merging changes from 2283 documents into a model of 6283 documents\n",
      "2020-11-18 19:22:07.423 INFO    gensim.models.ldamodel: topic #4 (0.050): 0.098*\"scope\" + 0.080*\"mtco\" + 0.038*\"people\" + 0.036*\"indir\" + 0.029*\"market\" + 0.029*\"location\" + 0.028*\"employment\" + 0.021*\"young\" + 0.021*\"skill\" + 0.019*\"local\"\n",
      "2020-11-18 19:22:07.424 INFO    gensim.models.ldamodel: topic #3 (0.050): 0.032*\"prairiesky\" + 0.023*\"responsibility\" + 0.017*\"operator\" + 0.014*\"value\" + 0.012*\"conduct\" + 0.012*\"royalty\" + 0.011*\"environment\" + 0.011*\"shareholder\" + 0.011*\"responsible\" + 0.011*\"company\"\n",
      "2020-11-18 19:22:07.427 INFO    gensim.models.ldamodel: topic #14 (0.050): 0.067*\"park\" + 0.021*\"travel\" + 0.019*\"sell\" + 0.013*\"average\" + 0.011*\"house\" + 0.010*\"defra\" + 0.010*\"journey\" + 0.008*\"visitor\" + 0.008*\"birmingham\" + 0.008*\"brent\"\n",
      "2020-11-18 19:22:07.430 INFO    gensim.models.ldamodel: topic #10 (0.050): 0.023*\"design\" + 0.018*\"breeam\" + 0.012*\"green\" + 0.011*\"site\" + 0.011*\"complete\" + 0.010*\"major\" + 0.010*\"excellent\" + 0.010*\"rating\" + 0.009*\"scheme\" + 0.008*\"build\"\n",
      "2020-11-18 19:22:07.432 INFO    gensim.models.ldamodel: topic #19 (0.050): 0.015*\"manager\" + 0.015*\"focus\" + 0.013*\"operation\" + 0.013*\"program\" + 0.012*\"increase\" + 0.011*\"french\" + 0.011*\"level\" + 0.011*\"company\" + 0.011*\"operational\" + 0.010*\"benefit\"\n",
      "2020-11-18 19:22:07.434 INFO    gensim.models.ldamodel: topic diff=0.568561, rho=0.314014\n",
      "2020-11-18 19:22:07.684 INFO    gensim.models.ldamodel: -6.950 per-word bound, 123.7 perplexity estimate based on a held-out corpus of 283 documents with 9843 words\n",
      "2020-11-18 19:22:07.685 INFO    gensim.models.ldamulticore: PROGRESS: pass 7, dispatched chunk #0 = documents up to #2000/6283, outstanding queue size 1\n",
      "2020-11-18 19:22:07.705 INFO    gensim.models.ldamulticore: PROGRESS: pass 7, dispatched chunk #1 = documents up to #4000/6283, outstanding queue size 2\n",
      "2020-11-18 19:22:07.727 INFO    gensim.models.ldamulticore: PROGRESS: pass 7, dispatched chunk #2 = documents up to #6000/6283, outstanding queue size 3\n",
      "2020-11-18 19:22:07.755 INFO    gensim.models.ldamulticore: PROGRESS: pass 7, dispatched chunk #3 = documents up to #6283/6283, outstanding queue size 4\n",
      "2020-11-18 19:22:09.565 INFO    gensim.models.ldamodel: merging changes from 4000 documents into a model of 6283 documents\n",
      "2020-11-18 19:22:09.571 INFO    gensim.models.ldamodel: topic #16 (0.050): 0.042*\"health\" + 0.042*\"safety\" + 0.021*\"compliance\" + 0.016*\"regulation\" + 0.016*\"prairiesky\" + 0.013*\"incident\" + 0.012*\"rate\" + 0.012*\"royalty\" + 0.011*\"injury\" + 0.010*\"site\"\n",
      "2020-11-18 19:22:09.572 INFO    gensim.models.ldamodel: topic #0 (0.050): 0.048*\"scope\" + 0.036*\"like\" + 0.028*\"indicator\" + 0.016*\"greenhouse\" + 0.015*\"intensity\" + 0.012*\"indirect\" + 0.011*\"source\" + 0.011*\"factor\" + 0.010*\"core\" + 0.010*\"utility\"\n",
      "2020-11-18 19:22:09.579 INFO    gensim.models.ldamodel: topic #8 (0.050): 0.037*\"recycle\" + 0.035*\"landfill\" + 0.035*\"tonne\" + 0.027*\"assurance\" + 0.022*\"hazardous\" + 0.021*\"fuel\" + 0.020*\"send\" + 0.019*\"material\" + 0.019*\"reuse\" + 0.018*\"like\"\n",
      "2020-11-18 19:22:09.581 INFO    gensim.models.ldamodel: topic #18 (0.050): 0.036*\"positive\" + 0.020*\"saving\" + 0.018*\"demand\" + 0.017*\"like\" + 0.016*\"retailer\" + 0.016*\"efficiency\" + 0.014*\"deliver\" + 0.013*\"cost\" + 0.012*\"focus\" + 0.011*\"site\"\n",
      "2020-11-18 19:22:09.583 INFO    gensim.models.ldamodel: topic #7 (0.050): 0.017*\"visitor\" + 0.013*\"positive\" + 0.013*\"rainwater\" + 0.012*\"stakeholder\" + 0.012*\"focus\" + 0.011*\"service\" + 0.011*\"landlord\" + 0.010*\"deliver\" + 0.010*\"relationship\" + 0.010*\"local\"\n",
      "2020-11-18 19:22:09.585 INFO    gensim.models.ldamodel: topic diff=0.490844, rho=0.299591\n",
      "2020-11-18 19:22:09.960 INFO    gensim.models.ldamodel: -7.011 per-word bound, 129.0 perplexity estimate based on a held-out corpus of 283 documents with 9843 words\n",
      "2020-11-18 19:22:11.218 INFO    gensim.models.ldamodel: merging changes from 2283 documents into a model of 6283 documents\n",
      "2020-11-18 19:22:11.225 INFO    gensim.models.ldamodel: topic #2 (0.050): 0.027*\"training\" + 0.013*\"volunteer\" + 0.013*\"local\" + 0.012*\"staff\" + 0.012*\"female\" + 0.011*\"opportunity\" + 0.011*\"programme\" + 0.011*\"diversity\" + 0.010*\"hour\" + 0.009*\"skill\"\n",
      "2020-11-18 19:22:11.227 INFO    gensim.models.ldamodel: topic #11 (0.050): 0.124*\"tonne\" + 0.057*\"health\" + 0.041*\"safety\" + 0.020*\"supplier\" + 0.018*\"training\" + 0.016*\"metric\" + 0.014*\"process\" + 0.008*\"meeting\" + 0.008*\"survey\" + 0.008*\"code\"\n",
      "2020-11-18 19:22:11.229 INFO    gensim.models.ldamodel: topic #16 (0.050): 0.040*\"health\" + 0.039*\"safety\" + 0.022*\"compliance\" + 0.019*\"prairiesky\" + 0.016*\"regulation\" + 0.015*\"royalty\" + 0.011*\"incident\" + 0.011*\"site\" + 0.010*\"policy\" + 0.010*\"injury\"\n",
      "2020-11-18 19:22:11.231 INFO    gensim.models.ldamodel: topic #14 (0.050): 0.068*\"park\" + 0.021*\"travel\" + 0.019*\"sell\" + 0.013*\"average\" + 0.012*\"house\" + 0.010*\"defra\" + 0.010*\"journey\" + 0.009*\"visitor\" + 0.009*\"saint\" + 0.009*\"birmingham\"\n",
      "2020-11-18 19:22:11.234 INFO    gensim.models.ldamodel: topic #5 (0.050): 0.100*\"landlord\" + 0.072*\"electricity\" + 0.044*\"elec\" + 0.041*\"fuels\" + 0.038*\"like\" + 0.036*\"intensity\" + 0.032*\"meter\" + 0.032*\"natural\" + 0.029*\"obtain\" + 0.023*\"renewable\"\n",
      "2020-11-18 19:22:11.236 INFO    gensim.models.ldamodel: topic diff=0.476245, rho=0.299591\n",
      "2020-11-18 19:22:11.451 INFO    gensim.models.ldamodel: -6.927 per-word bound, 121.7 perplexity estimate based on a held-out corpus of 283 documents with 9843 words\n",
      "2020-11-18 19:22:11.453 INFO    gensim.models.ldamulticore: PROGRESS: pass 8, dispatched chunk #0 = documents up to #2000/6283, outstanding queue size 1\n",
      "2020-11-18 19:22:11.478 INFO    gensim.models.ldamulticore: PROGRESS: pass 8, dispatched chunk #1 = documents up to #4000/6283, outstanding queue size 2\n",
      "2020-11-18 19:22:11.517 INFO    gensim.models.ldamulticore: PROGRESS: pass 8, dispatched chunk #2 = documents up to #6000/6283, outstanding queue size 3\n",
      "2020-11-18 19:22:11.538 INFO    gensim.models.ldamulticore: PROGRESS: pass 8, dispatched chunk #3 = documents up to #6283/6283, outstanding queue size 4\n",
      "2020-11-18 19:22:13.254 INFO    gensim.models.ldamodel: merging changes from 4000 documents into a model of 6283 documents\n",
      "2020-11-18 19:22:13.260 INFO    gensim.models.ldamodel: topic #10 (0.050): 0.024*\"design\" + 0.018*\"breeam\" + 0.014*\"green\" + 0.012*\"complete\" + 0.012*\"rating\" + 0.011*\"major\" + 0.011*\"excellent\" + 0.011*\"scheme\" + 0.010*\"assessment\" + 0.010*\"site\"\n",
      "2020-11-18 19:22:13.262 INFO    gensim.models.ldamodel: topic #6 (0.050): 0.040*\"cost\" + 0.023*\"data\" + 0.015*\"value\" + 0.014*\"million\" + 0.011*\"charge\" + 0.011*\"investment\" + 0.011*\"note\" + 0.010*\"available\" + 0.010*\"electricity\" + 0.009*\"estimate\"\n",
      "2020-11-18 19:22:13.263 INFO    gensim.models.ldamodel: topic #15 (0.050): 0.036*\"section\" + 0.031*\"disclosure\" + 0.025*\"material\" + 0.023*\"stakeholder\" + 0.020*\"boundary\" + 0.020*\"indicator\" + 0.020*\"external\" + 0.018*\"aspect\" + 0.018*\"table\" + 0.017*\"account\"\n",
      "2020-11-18 19:22:13.265 INFO    gensim.models.ldamodel: topic #1 (0.050): 0.022*\"supplier\" + 0.021*\"term\" + 0.020*\"climate\" + 0.016*\"supply\" + 0.015*\"progress\" + 0.015*\"design\" + 0.013*\"chain\" + 0.010*\"long\" + 0.009*\"construction\" + 0.009*\"review\"\n",
      "2020-11-18 19:22:13.266 INFO    gensim.models.ldamodel: topic #2 (0.050): 0.027*\"training\" + 0.015*\"staff\" + 0.013*\"local\" + 0.012*\"programme\" + 0.012*\"female\" + 0.011*\"volunteer\" + 0.011*\"opportunity\" + 0.010*\"diversity\" + 0.009*\"survey\" + 0.009*\"skill\"\n",
      "2020-11-18 19:22:13.268 INFO    gensim.models.ldamodel: topic diff=0.411517, rho=0.286988\n",
      "2020-11-18 19:22:13.776 INFO    gensim.models.ldamodel: -6.985 per-word bound, 126.7 perplexity estimate based on a held-out corpus of 283 documents with 9843 words\n",
      "2020-11-18 19:22:15.621 INFO    gensim.models.ldamodel: merging changes from 2283 documents into a model of 6283 documents\n",
      "2020-11-18 19:22:15.629 INFO    gensim.models.ldamodel: topic #9 (0.050): 0.052*\"risk\" + 0.037*\"board\" + 0.027*\"governance\" + 0.023*\"committee\" + 0.019*\"climate\" + 0.017*\"director\" + 0.017*\"policy\" + 0.015*\"executive\" + 0.012*\"responsibility\" + 0.011*\"process\"\n",
      "2020-11-18 19:22:15.631 INFO    gensim.models.ldamodel: topic #19 (0.050): 0.016*\"manager\" + 0.015*\"focus\" + 0.014*\"operation\" + 0.013*\"program\" + 0.012*\"level\" + 0.012*\"increase\" + 0.012*\"company\" + 0.012*\"french\" + 0.011*\"operational\" + 0.010*\"benefit\"\n",
      "2020-11-18 19:22:15.632 INFO    gensim.models.ldamodel: topic #8 (0.050): 0.042*\"recycle\" + 0.042*\"tonne\" + 0.039*\"landfill\" + 0.027*\"hazardous\" + 0.026*\"fuel\" + 0.024*\"material\" + 0.023*\"food\" + 0.022*\"reuse\" + 0.022*\"send\" + 0.021*\"assurance\"\n",
      "2020-11-18 19:22:15.635 INFO    gensim.models.ldamodel: topic #13 (0.050): 0.021*\"positive\" + 0.019*\"charter\" + 0.019*\"hall\" + 0.018*\"economic\" + 0.016*\"risk\" + 0.015*\"issue\" + 0.012*\"socio\" + 0.011*\"stakeholder\" + 0.011*\"investor\" + 0.010*\"review\"\n",
      "2020-11-18 19:22:15.637 INFO    gensim.models.ldamodel: topic #2 (0.050): 0.028*\"training\" + 0.013*\"staff\" + 0.013*\"volunteer\" + 0.013*\"local\" + 0.012*\"programme\" + 0.011*\"opportunity\" + 0.011*\"female\" + 0.011*\"diversity\" + 0.009*\"hour\" + 0.009*\"skill\"\n",
      "2020-11-18 19:22:15.640 INFO    gensim.models.ldamodel: topic diff=0.400188, rho=0.286988\n",
      "2020-11-18 19:22:16.130 INFO    gensim.models.ldamodel: -6.908 per-word bound, 120.1 perplexity estimate based on a held-out corpus of 283 documents with 9843 words\n",
      "2020-11-18 19:22:16.131 INFO    gensim.models.ldamulticore: PROGRESS: pass 9, dispatched chunk #0 = documents up to #2000/6283, outstanding queue size 1\n",
      "2020-11-18 19:22:16.210 INFO    gensim.models.ldamulticore: PROGRESS: pass 9, dispatched chunk #1 = documents up to #4000/6283, outstanding queue size 2\n",
      "2020-11-18 19:22:16.262 INFO    gensim.models.ldamulticore: PROGRESS: pass 9, dispatched chunk #2 = documents up to #6000/6283, outstanding queue size 3\n",
      "2020-11-18 19:22:16.300 INFO    gensim.models.ldamulticore: PROGRESS: pass 9, dispatched chunk #3 = documents up to #6283/6283, outstanding queue size 4\n",
      "2020-11-18 19:22:18.615 INFO    gensim.models.ldamodel: merging changes from 4000 documents into a model of 6283 documents\n",
      "2020-11-18 19:22:18.624 INFO    gensim.models.ldamodel: topic #10 (0.050): 0.025*\"design\" + 0.019*\"breeam\" + 0.015*\"green\" + 0.012*\"complete\" + 0.012*\"rating\" + 0.012*\"excellent\" + 0.012*\"scheme\" + 0.012*\"major\" + 0.011*\"assessment\" + 0.011*\"site\"\n",
      "2020-11-18 19:22:18.628 INFO    gensim.models.ldamodel: topic #9 (0.050): 0.048*\"risk\" + 0.033*\"board\" + 0.028*\"governance\" + 0.024*\"committee\" + 0.018*\"climate\" + 0.017*\"policy\" + 0.016*\"director\" + 0.015*\"executive\" + 0.012*\"responsibility\" + 0.011*\"process\"\n",
      "2020-11-18 19:22:18.631 INFO    gensim.models.ldamodel: topic #11 (0.050): 0.111*\"tonne\" + 0.070*\"health\" + 0.051*\"safety\" + 0.020*\"supplier\" + 0.019*\"training\" + 0.016*\"metric\" + 0.014*\"process\" + 0.008*\"meeting\" + 0.008*\"survey\" + 0.008*\"wellbeing\"\n",
      "2020-11-18 19:22:18.633 INFO    gensim.models.ldamodel: topic #8 (0.050): 0.040*\"recycle\" + 0.038*\"landfill\" + 0.037*\"tonne\" + 0.027*\"assurance\" + 0.023*\"hazardous\" + 0.022*\"fuel\" + 0.022*\"recycling\" + 0.021*\"material\" + 0.021*\"send\" + 0.020*\"reuse\"\n",
      "2020-11-18 19:22:18.638 INFO    gensim.models.ldamodel: topic #2 (0.050): 0.028*\"training\" + 0.015*\"staff\" + 0.013*\"local\" + 0.013*\"programme\" + 0.011*\"volunteer\" + 0.011*\"female\" + 0.011*\"opportunity\" + 0.010*\"diversity\" + 0.010*\"skill\" + 0.010*\"survey\"\n",
      "2020-11-18 19:22:18.643 INFO    gensim.models.ldamodel: topic diff=0.347747, rho=0.275853\n",
      "2020-11-18 19:22:19.435 INFO    gensim.models.ldamodel: -6.965 per-word bound, 124.9 perplexity estimate based on a held-out corpus of 283 documents with 9843 words\n",
      "2020-11-18 19:22:21.555 INFO    gensim.models.ldamodel: merging changes from 2283 documents into a model of 6283 documents\n",
      "2020-11-18 19:22:21.563 INFO    gensim.models.ldamodel: topic #11 (0.050): 0.124*\"tonne\" + 0.068*\"health\" + 0.051*\"safety\" + 0.022*\"supplier\" + 0.019*\"training\" + 0.015*\"metric\" + 0.014*\"process\" + 0.009*\"meeting\" + 0.008*\"survey\" + 0.008*\"code\"\n",
      "2020-11-18 19:22:21.569 INFO    gensim.models.ldamodel: topic #13 (0.050): 0.021*\"positive\" + 0.020*\"charter\" + 0.019*\"hall\" + 0.018*\"economic\" + 0.016*\"issue\" + 0.015*\"risk\" + 0.012*\"socio\" + 0.012*\"stakeholder\" + 0.012*\"investor\" + 0.010*\"review\"\n",
      "2020-11-18 19:22:21.573 INFO    gensim.models.ldamodel: topic #0 (0.050): 0.052*\"scope\" + 0.039*\"like\" + 0.025*\"indicator\" + 0.019*\"intensity\" + 0.016*\"greenhouse\" + 0.014*\"factor\" + 0.013*\"source\" + 0.012*\"indirect\" + 0.011*\"utility\" + 0.010*\"kgco\"\n",
      "2020-11-18 19:22:21.577 INFO    gensim.models.ldamodel: topic #14 (0.050): 0.071*\"park\" + 0.022*\"travel\" + 0.019*\"sell\" + 0.013*\"average\" + 0.012*\"house\" + 0.010*\"saint\" + 0.010*\"defra\" + 0.010*\"journey\" + 0.010*\"visitor\" + 0.009*\"birmingham\"\n",
      "2020-11-18 19:22:21.579 INFO    gensim.models.ldamodel: topic #18 (0.050): 0.043*\"positive\" + 0.022*\"demand\" + 0.021*\"saving\" + 0.017*\"retailer\" + 0.016*\"efficiency\" + 0.015*\"deliver\" + 0.015*\"like\" + 0.013*\"focus\" + 0.013*\"cost\" + 0.012*\"operational\"\n",
      "2020-11-18 19:22:21.585 INFO    gensim.models.ldamodel: topic diff=0.339197, rho=0.275853\n",
      "2020-11-18 19:22:22.001 INFO    gensim.models.ldamodel: -6.896 per-word bound, 119.1 perplexity estimate based on a held-out corpus of 283 documents with 9843 words\n"
     ]
    }
   ],
   "source": [
    "num_topics = 20\n",
    "lda_model =  LdaMulticore(\n",
    "    corpus=bow_corpus, \n",
    "    num_topics = num_topics, \n",
    "    id2word = dictionary,                                    \n",
    "    passes = 10,\n",
    "    workers = 2\n",
    "    )"
   ]
  },
  {
   "source": [
    "## 2.2 Evaluate topics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "lda_topics_d = {}\n",
    "lda_topics_l = []\n",
    "for i in range(num_topics):\n",
    "    t = [w[0] for w in lda_model.show_topic(i)[0:10]]\n",
    "    lda_topics_d[i+1] = t\n",
    "    lda_topics_l.append(t)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             0               1             2           3            4  \\\n",
       "1        scope            like     indicator   intensity   greenhouse   \n",
       "2      climate            term      supplier      design       supply   \n",
       "3     training           staff     volunteer       local    programme   \n",
       "4   prairiesky  responsibility      operator       value      royalty   \n",
       "5        scope            mtco        people       indir   employment   \n",
       "6     landlord     electricity          elec        like        fuels   \n",
       "7         cost            data       million       value       charge   \n",
       "8      visitor        positive     rainwater     service        focus   \n",
       "9      recycle           tonne      landfill   hazardous         fuel   \n",
       "10        risk           board    governance   committee      climate   \n",
       "11      design          breeam     excellent      scheme         site   \n",
       "12       tonne          health        safety    supplier     training   \n",
       "13      direct      investment  contribution        time        value   \n",
       "14    positive         charter          hall    economic        issue   \n",
       "15        park          travel          sell     average        house   \n",
       "16     section      disclosure      material       table  stakeholder   \n",
       "17      health          safety    compliance  prairiesky   regulation   \n",
       "18      street            star        rating      member         real   \n",
       "19    positive          demand        saving    retailer   efficiency   \n",
       "20     manager           focus     operation     program        level   \n",
       "\n",
       "              5         6            7               8             9  \n",
       "1        factor    source     indirect         utility          kgco  \n",
       "2      progress     chain         long            risk  construction  \n",
       "3   opportunity    female    diversity            hour         skill  \n",
       "4       conduct   company        right     environment   shareholder  \n",
       "5      location    market        young           skill         local  \n",
       "6     intensity     meter      natural          obtain     renewable  \n",
       "7    investment      note    available            levy       capital  \n",
       "8   stakeholder     local      deliver    relationship      landlord  \n",
       "9      material      food        reuse            send     recycling  \n",
       "10     director    policy    executive  responsibility       process  \n",
       "11        green  complete   assessment          rating         major  \n",
       "12       metric   process      meeting          survey          code  \n",
       "13     activity  contract    permanent           leave         staff  \n",
       "14         risk     socio  stakeholder        investor        review  \n",
       "15        saint     defra      journey         visitor    birmingham  \n",
       "16    indicator    aspect     external        boundary       account  \n",
       "17      royalty  incident       policy            site        injury  \n",
       "18        build    estate      average           green         white  \n",
       "19      deliver      like        focus            cost   operational  \n",
       "20      company  increase       french     operational       benefit  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>scope</td>\n      <td>like</td>\n      <td>indicator</td>\n      <td>intensity</td>\n      <td>greenhouse</td>\n      <td>factor</td>\n      <td>source</td>\n      <td>indirect</td>\n      <td>utility</td>\n      <td>kgco</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>climate</td>\n      <td>term</td>\n      <td>supplier</td>\n      <td>design</td>\n      <td>supply</td>\n      <td>progress</td>\n      <td>chain</td>\n      <td>long</td>\n      <td>risk</td>\n      <td>construction</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>training</td>\n      <td>staff</td>\n      <td>volunteer</td>\n      <td>local</td>\n      <td>programme</td>\n      <td>opportunity</td>\n      <td>female</td>\n      <td>diversity</td>\n      <td>hour</td>\n      <td>skill</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>prairiesky</td>\n      <td>responsibility</td>\n      <td>operator</td>\n      <td>value</td>\n      <td>royalty</td>\n      <td>conduct</td>\n      <td>company</td>\n      <td>right</td>\n      <td>environment</td>\n      <td>shareholder</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>scope</td>\n      <td>mtco</td>\n      <td>people</td>\n      <td>indir</td>\n      <td>employment</td>\n      <td>location</td>\n      <td>market</td>\n      <td>young</td>\n      <td>skill</td>\n      <td>local</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>landlord</td>\n      <td>electricity</td>\n      <td>elec</td>\n      <td>like</td>\n      <td>fuels</td>\n      <td>intensity</td>\n      <td>meter</td>\n      <td>natural</td>\n      <td>obtain</td>\n      <td>renewable</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>cost</td>\n      <td>data</td>\n      <td>million</td>\n      <td>value</td>\n      <td>charge</td>\n      <td>investment</td>\n      <td>note</td>\n      <td>available</td>\n      <td>levy</td>\n      <td>capital</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>visitor</td>\n      <td>positive</td>\n      <td>rainwater</td>\n      <td>service</td>\n      <td>focus</td>\n      <td>stakeholder</td>\n      <td>local</td>\n      <td>deliver</td>\n      <td>relationship</td>\n      <td>landlord</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>recycle</td>\n      <td>tonne</td>\n      <td>landfill</td>\n      <td>hazardous</td>\n      <td>fuel</td>\n      <td>material</td>\n      <td>food</td>\n      <td>reuse</td>\n      <td>send</td>\n      <td>recycling</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>risk</td>\n      <td>board</td>\n      <td>governance</td>\n      <td>committee</td>\n      <td>climate</td>\n      <td>director</td>\n      <td>policy</td>\n      <td>executive</td>\n      <td>responsibility</td>\n      <td>process</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>design</td>\n      <td>breeam</td>\n      <td>excellent</td>\n      <td>scheme</td>\n      <td>site</td>\n      <td>green</td>\n      <td>complete</td>\n      <td>assessment</td>\n      <td>rating</td>\n      <td>major</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>tonne</td>\n      <td>health</td>\n      <td>safety</td>\n      <td>supplier</td>\n      <td>training</td>\n      <td>metric</td>\n      <td>process</td>\n      <td>meeting</td>\n      <td>survey</td>\n      <td>code</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>direct</td>\n      <td>investment</td>\n      <td>contribution</td>\n      <td>time</td>\n      <td>value</td>\n      <td>activity</td>\n      <td>contract</td>\n      <td>permanent</td>\n      <td>leave</td>\n      <td>staff</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>positive</td>\n      <td>charter</td>\n      <td>hall</td>\n      <td>economic</td>\n      <td>issue</td>\n      <td>risk</td>\n      <td>socio</td>\n      <td>stakeholder</td>\n      <td>investor</td>\n      <td>review</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>park</td>\n      <td>travel</td>\n      <td>sell</td>\n      <td>average</td>\n      <td>house</td>\n      <td>saint</td>\n      <td>defra</td>\n      <td>journey</td>\n      <td>visitor</td>\n      <td>birmingham</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>section</td>\n      <td>disclosure</td>\n      <td>material</td>\n      <td>table</td>\n      <td>stakeholder</td>\n      <td>indicator</td>\n      <td>aspect</td>\n      <td>external</td>\n      <td>boundary</td>\n      <td>account</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>health</td>\n      <td>safety</td>\n      <td>compliance</td>\n      <td>prairiesky</td>\n      <td>regulation</td>\n      <td>royalty</td>\n      <td>incident</td>\n      <td>policy</td>\n      <td>site</td>\n      <td>injury</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>street</td>\n      <td>star</td>\n      <td>rating</td>\n      <td>member</td>\n      <td>real</td>\n      <td>build</td>\n      <td>estate</td>\n      <td>average</td>\n      <td>green</td>\n      <td>white</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>positive</td>\n      <td>demand</td>\n      <td>saving</td>\n      <td>retailer</td>\n      <td>efficiency</td>\n      <td>deliver</td>\n      <td>like</td>\n      <td>focus</td>\n      <td>cost</td>\n      <td>operational</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>manager</td>\n      <td>focus</td>\n      <td>operation</td>\n      <td>program</td>\n      <td>level</td>\n      <td>company</td>\n      <td>increase</td>\n      <td>french</td>\n      <td>operational</td>\n      <td>benefit</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "# show topics\n",
    "# evert row is a topic\n",
    "pd.DataFrame.from_dict(lda_topics_d).T"
   ]
  },
  {
   "source": [
    "### 2.2.1 Normalized Point-wise Mutual Information"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "2020-11-18 19:22:23.754 INFO    gensim.corpora.dictionary: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-18 19:22:24.457 INFO    gensim.corpora.dictionary: built Dictionary(8372 unique tokens: ['armada', 'central', 'citizenship', 'come', 'corporate']...) from 6283 documents (total 331204 corpus positions)\n",
      "2020-11-18 19:22:24.470 INFO    gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator(processes=3, batch_size=64) to estimate probabilities from sliding windows\n",
      "2020-11-18 19:22:28.132 INFO    gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (3000 virtual)\n",
      "2020-11-18 19:22:28.136 INFO    gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (6379 virtual)\n",
      "2020-11-18 19:22:28.140 INFO    gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (9391 virtual)\n",
      "2020-11-18 19:22:28.143 INFO    gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (12085 virtual)\n",
      "2020-11-18 19:22:28.148 INFO    gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (15212 virtual)\n",
      "2020-11-18 19:22:28.152 INFO    gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (18308 virtual)\n",
      "2020-11-18 19:22:28.227 INFO    gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (20944 virtual)\n",
      "2020-11-18 19:22:28.246 INFO    gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (23537 virtual)\n",
      "2020-11-18 19:22:28.250 INFO    gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (26510 virtual)\n",
      "2020-11-18 19:22:28.308 INFO    gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (29598 virtual)\n",
      "2020-11-18 19:22:28.334 INFO    gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (32053 virtual)\n",
      "2020-11-18 19:22:28.347 INFO    gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (34676 virtual)\n",
      "2020-11-18 19:22:28.403 INFO    gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (37754 virtual)\n",
      "2020-11-18 19:22:28.436 INFO    gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (40104 virtual)\n",
      "2020-11-18 19:22:28.442 INFO    gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (42826 virtual)\n",
      "2020-11-18 19:22:28.517 INFO    gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (45557 virtual)\n",
      "2020-11-18 19:22:28.523 INFO    gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (48450 virtual)\n",
      "2020-11-18 19:22:28.554 INFO    gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (50889 virtual)\n",
      "2020-11-18 19:22:28.618 INFO    gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (53429 virtual)\n",
      "2020-11-18 19:22:28.647 INFO    gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (56437 virtual)\n",
      "2020-11-18 19:22:28.665 INFO    gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (59033 virtual)\n",
      "2020-11-18 19:22:28.720 INFO    gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (61882 virtual)\n",
      "2020-11-18 19:22:28.746 INFO    gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (64872 virtual)\n",
      "2020-11-18 19:22:28.768 INFO    gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (67528 virtual)\n",
      "2020-11-18 19:22:28.849 INFO    gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (70655 virtual)\n",
      "2020-11-18 19:22:28.863 INFO    gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (73675 virtual)\n",
      "2020-11-18 19:22:28.894 INFO    gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (76945 virtual)\n",
      "2020-11-18 19:22:28.984 INFO    gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (79543 virtual)\n",
      "2020-11-18 19:22:29.026 INFO    gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (82634 virtual)\n",
      "2020-11-18 19:22:29.079 INFO    gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (85857 virtual)\n",
      "2020-11-18 19:22:29.154 INFO    gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (88838 virtual)\n",
      "2020-11-18 19:22:29.163 INFO    gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (92185 virtual)\n",
      "2020-11-18 19:22:29.198 INFO    gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (95191 virtual)\n",
      "2020-11-18 19:22:29.253 INFO    gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (98215 virtual)\n",
      "2020-11-18 19:22:29.285 INFO    gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (101316 virtual)\n",
      "2020-11-18 19:22:29.289 INFO    gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (103915 virtual)\n",
      "2020-11-18 19:22:29.354 INFO    gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (106165 virtual)\n",
      "2020-11-18 19:22:29.398 INFO    gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (108599 virtual)\n",
      "2020-11-18 19:22:29.402 INFO    gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (111079 virtual)\n",
      "2020-11-18 19:22:29.465 INFO    gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (113477 virtual)\n",
      "2020-11-18 19:22:29.503 INFO    gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (115733 virtual)\n",
      "2020-11-18 19:22:29.507 INFO    gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (118587 virtual)\n",
      "2020-11-18 19:22:29.552 INFO    gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (120968 virtual)\n",
      "2020-11-18 19:22:29.579 INFO    gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (124245 virtual)\n",
      "2020-11-18 19:22:29.600 INFO    gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (127131 virtual)\n",
      "2020-11-18 19:22:29.672 INFO    gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (129753 virtual)\n",
      "2020-11-18 19:22:29.708 INFO    gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (132247 virtual)\n",
      "2020-11-18 19:22:29.736 INFO    gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (134873 virtual)\n",
      "2020-11-18 19:22:29.776 INFO    gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (136926 virtual)\n",
      "2020-11-18 19:22:29.835 INFO    gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (138991 virtual)\n",
      "2020-11-18 19:22:29.850 INFO    gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (140848 virtual)\n",
      "2020-11-18 19:22:29.874 INFO    gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (143258 virtual)\n",
      "2020-11-18 19:22:29.922 INFO    gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (145977 virtual)\n",
      "2020-11-18 19:22:29.942 INFO    gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (149055 virtual)\n",
      "2020-11-18 19:22:29.952 INFO    gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (152743 virtual)\n",
      "2020-11-18 19:22:30.038 INFO    gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (156722 virtual)\n",
      "2020-11-18 19:22:30.046 INFO    gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (160409 virtual)\n",
      "2020-11-18 19:22:30.084 INFO    gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (163375 virtual)\n",
      "2020-11-18 19:22:30.158 INFO    gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (166279 virtual)\n",
      "2020-11-18 19:22:30.193 INFO    gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (169104 virtual)\n",
      "2020-11-18 19:22:30.276 INFO    gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (172066 virtual)\n",
      "2020-11-18 19:22:30.350 INFO    gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (174981 virtual)\n",
      "2020-11-18 19:22:30.367 INFO    gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (177601 virtual)\n",
      "2020-11-18 19:22:30.375 INFO    gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (180333 virtual)\n",
      "2020-11-18 19:22:30.455 INFO    gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (183105 virtual)\n",
      "2020-11-18 19:22:30.488 INFO    gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (185865 virtual)\n",
      "2020-11-18 19:22:30.498 INFO    gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (188622 virtual)\n",
      "2020-11-18 19:22:30.572 INFO    gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (191046 virtual)\n",
      "2020-11-18 19:22:30.602 INFO    gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (193883 virtual)\n",
      "2020-11-18 19:22:30.607 INFO    gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (196495 virtual)\n",
      "2020-11-18 19:22:30.685 INFO    gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (199436 virtual)\n",
      "2020-11-18 19:22:30.700 INFO    gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (201863 virtual)\n",
      "2020-11-18 19:22:30.716 INFO    gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (204582 virtual)\n",
      "2020-11-18 19:22:30.784 INFO    gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (207747 virtual)\n",
      "2020-11-18 19:22:30.807 INFO    gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (210583 virtual)\n",
      "2020-11-18 19:22:30.824 INFO    gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (213840 virtual)\n",
      "2020-11-18 19:22:30.901 INFO    gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (217025 virtual)\n",
      "2020-11-18 19:22:30.905 INFO    gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (219662 virtual)\n",
      "2020-11-18 19:22:31.000 INFO    gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (222930 virtual)\n",
      "2020-11-18 19:22:31.100 INFO    gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (225941 virtual)\n",
      "2020-11-18 19:22:31.132 INFO    gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (228777 virtual)\n",
      "2020-11-18 19:22:31.153 INFO    gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (232181 virtual)\n",
      "2020-11-18 19:22:31.231 INFO    gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (235581 virtual)\n",
      "2020-11-18 19:22:31.249 INFO    gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (238288 virtual)\n",
      "2020-11-18 19:22:31.289 INFO    gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (241205 virtual)\n",
      "2020-11-18 19:22:31.332 INFO    gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (243264 virtual)\n",
      "2020-11-18 19:22:31.385 INFO    gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (245529 virtual)\n",
      "2020-11-18 19:22:31.432 INFO    gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (247974 virtual)\n",
      "2020-11-18 19:22:31.476 INFO    gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (250650 virtual)\n",
      "2020-11-18 19:22:31.513 INFO    gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (253821 virtual)\n",
      "2020-11-18 19:22:31.550 INFO    gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (256459 virtual)\n",
      "2020-11-18 19:22:31.565 INFO    gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (259411 virtual)\n",
      "2020-11-18 19:22:31.602 INFO    gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (262128 virtual)\n",
      "2020-11-18 19:22:31.646 INFO    gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (264786 virtual)\n",
      "2020-11-18 19:22:31.670 INFO    gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (267729 virtual)\n",
      "2020-11-18 19:22:31.736 INFO    gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (269883 virtual)\n",
      "2020-11-18 19:22:31.761 INFO    gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (273070 virtual)\n",
      "2020-11-18 19:22:31.799 INFO    gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (274040 virtual)\n",
      "2020-11-18 19:22:32.045 INFO    gensim.topic_coherence.text_analysis: 3 accumulators retrieved from output queue\n",
      "2020-11-18 19:22:32.076 INFO    gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 274181 virtual documents\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0574448734166926"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "npmi = CoherenceNPMI(texts=text_cleaned, topics=lda_topics_l)\n",
    "npmi.score()"
   ]
  },
  {
   "source": [
    "### 2.2.2 External Word Embeddings Topic Coherence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "2020-11-18 19:22:32.887 INFO    gensim.models.utils_any2vec: loading projection weights from C:\\Users\\Pieter-Jan/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n",
      "2020-11-18 19:23:21.754 INFO    gensim.models.utils_any2vec: loaded (3000000, 300) matrix from C:\\Users\\Pieter-Jan/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.13026394"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "CoherenceWordEmbeddings(lda_topics_l).score()"
   ]
  },
  {
   "source": [
    "### 2.2.3 Rank-Biased Overlap "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9794654136539473"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "InvertedRBO(lda_topics_l).score()"
   ]
  },
  {
   "source": [
    "## 3. Bert Topic"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.1 Train model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "2020-11-18 19:23:46,046 - BERTopic - Reduced dimensionality with UMAP\n",
      "2020-11-18 19:23:46.046 INFO    BERTopic: Reduced dimensionality with UMAP\n",
      "2020-11-18 19:23:47,284 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2020-11-18 19:23:47.284 INFO    BERTopic: Clustered UMAP embeddings with HDBSCAN\n"
     ]
    }
   ],
   "source": [
    "random.seed(69)\n",
    "\n",
    "with open(\"output/CSR_processed_cleaned_pdfMiner.txt\",\"r\") as fr:\n",
    "    docs = [doc for doc in fr.read().splitlines()] \n",
    "\n",
    "model = BERTopic(verbose=True)\n",
    "topics = model.fit_transform(docs, embeddings_bert)"
   ]
  },
  {
   "source": [
    "## 3.2 Evaluate topics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "topcis_b = model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# extract words for each topic\n",
    "topics_k = {}\n",
    "for k,v in topcis_b.items():\n",
    "    t_words = []\n",
    "    for w in v:\n",
    "        t_words.append(w[0])\n",
    "    # append the first 10 words\n",
    "    topics_k[k] = t_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                0              1            2               3               4  \\\n",
       "-1       property  environmental       manage  sustainability     performance   \n",
       " 0           park         retail         sell           brent       excellent   \n",
       " 1     prairiesky       operator      royalty            land           lease   \n",
       " 2           page   organisation       annual      governance         account   \n",
       " 3    mzftifbludn          hyiow   gebiawvpsy            jwlm  vitwgjgayqvxze   \n",
       " 4      assurance         matter      subject       procedure         perform   \n",
       " 5       employee         number       female       permanent          gender   \n",
       " 6         oxford         street      buckley          embody       copyright   \n",
       " 7       landlord       emission      biomass           scope     electricity   \n",
       " 8          tonne       recovery        water        building          street   \n",
       " 9           core      indicator  performance            adtl      initiative   \n",
       " 10         waste          tonne    hazardous            fuel      incinerate   \n",
       " 11     recycling          waste     landfill         recycle        increase   \n",
       " 12         water       landlord       obtain       rainwater     consumption   \n",
       " 13       charter           hall         june            july         datagri   \n",
       " 14          hall        charter         cpof          street            fund   \n",
       " 15        factor         travel     emission           defra         journey   \n",
       " 16          elec          fuels      natural     electricity        landlord   \n",
       " 17          mtco          indir        scope            base        location   \n",
       " 18         asset      reporting         hold         control         include   \n",
       " 19         datum           data      utility         monthly         reading   \n",
       " 20     reporting           epra   supplement  recommendation        practice   \n",
       " 21          page    materiality    footprint         measure          carbon   \n",
       " 22  contribution           kind     leverage         charity      collection   \n",
       " 23       achieve        project      minimum          rating            leed   \n",
       " 24        carbon      reduction       reduce          target        emission   \n",
       " 25      training          staff       france       programme       awareness   \n",
       " 26        france          lease        green    biodiversity       hammerson   \n",
       " 27        really          enjoy         role            feel            work   \n",
       " 28      employee       training      session          survey     opportunity   \n",
       " 29         local      community       people           skill      employment   \n",
       " 30      strategy       business     investor  sustainability     stakeholder   \n",
       " 31          risk      committee        board       executive         climate   \n",
       " 32         board          woman    diversity          policy       workplace   \n",
       " 33        safety         health       ensure            risk    construction   \n",
       "\n",
       "                  5               6            7              8              9  \n",
       "-1      development      management        datum        project           risk  \n",
       " 0         shopping          street        saint         centre         london  \n",
       " 1          mineral          canada    liability       industry    reclamation  \n",
       " 2        statement         general  stakeholder           body          topic  \n",
       " 3        corporate         program       hwjixe        norefer         realty  \n",
       " 4       accordance    independence   conclusion      criterion        derwent  \n",
       " 5             male        contract       record         senior          leave  \n",
       " 6             site          brunel        build    pentonville      charlotte  \n",
       " 7             like        building        total         tenant      intensity  \n",
       " 8     incineration           angel         like        exclude          total  \n",
       " 9         monetary           fully     overview       indirect  environmental  \n",
       " 10       landfille         shopfit         send        compost       quantity  \n",
       " 11       diversion            rate      plastic       resource          reuse  \n",
       " 12          tenant           meter       demand        visitor        harvest  \n",
       " 13   ungcassurance      messageour     indexten     approachfy          usceo  \n",
       " 14          office            chot         june           star        average  \n",
       " 15             use     refrigerant      visitor        average      calculate  \n",
       " 16     consumption          obtain       diesel      renewable          meter  \n",
       " 17          market            kgco         part         common          total  \n",
       " 18          period           datum     directly           data           year  \n",
       " 19          manual         invoice     automate        manager          meter  \n",
       " 20          sector           cress      lasalle           lang          datum  \n",
       " 21     performance         section      summary        account           epra  \n",
       " 22          record           staff        money          value           time  \n",
       " 23          breeam          design        major  refurbishment          build  \n",
       " 24         climate          energy       change        achieve         demand  \n",
       " 25  sustainability          survey    knowledge           role        develop  \n",
       " 26     development          tenant     complete          guide       progress  \n",
       " 27         derwent      experience    interview         career      different  \n",
       " 28      leadership           skill       talent           work        develop  \n",
       " 29         support           young       school         engage        project  \n",
       " 30        positive            long        value      hammerson           term  \n",
       " 31     responsible  sustainability     business          group      corporate  \n",
       " 32         conduct          female         code     prairiesky       director  \n",
       " 33      management        incident     approach           safe         quooda  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>-1</th>\n      <td>property</td>\n      <td>environmental</td>\n      <td>manage</td>\n      <td>sustainability</td>\n      <td>performance</td>\n      <td>development</td>\n      <td>management</td>\n      <td>datum</td>\n      <td>project</td>\n      <td>risk</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>park</td>\n      <td>retail</td>\n      <td>sell</td>\n      <td>brent</td>\n      <td>excellent</td>\n      <td>shopping</td>\n      <td>street</td>\n      <td>saint</td>\n      <td>centre</td>\n      <td>london</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>prairiesky</td>\n      <td>operator</td>\n      <td>royalty</td>\n      <td>land</td>\n      <td>lease</td>\n      <td>mineral</td>\n      <td>canada</td>\n      <td>liability</td>\n      <td>industry</td>\n      <td>reclamation</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>page</td>\n      <td>organisation</td>\n      <td>annual</td>\n      <td>governance</td>\n      <td>account</td>\n      <td>statement</td>\n      <td>general</td>\n      <td>stakeholder</td>\n      <td>body</td>\n      <td>topic</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>mzftifbludn</td>\n      <td>hyiow</td>\n      <td>gebiawvpsy</td>\n      <td>jwlm</td>\n      <td>vitwgjgayqvxze</td>\n      <td>corporate</td>\n      <td>program</td>\n      <td>hwjixe</td>\n      <td>norefer</td>\n      <td>realty</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>assurance</td>\n      <td>matter</td>\n      <td>subject</td>\n      <td>procedure</td>\n      <td>perform</td>\n      <td>accordance</td>\n      <td>independence</td>\n      <td>conclusion</td>\n      <td>criterion</td>\n      <td>derwent</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>employee</td>\n      <td>number</td>\n      <td>female</td>\n      <td>permanent</td>\n      <td>gender</td>\n      <td>male</td>\n      <td>contract</td>\n      <td>record</td>\n      <td>senior</td>\n      <td>leave</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>oxford</td>\n      <td>street</td>\n      <td>buckley</td>\n      <td>embody</td>\n      <td>copyright</td>\n      <td>site</td>\n      <td>brunel</td>\n      <td>build</td>\n      <td>pentonville</td>\n      <td>charlotte</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>landlord</td>\n      <td>emission</td>\n      <td>biomass</td>\n      <td>scope</td>\n      <td>electricity</td>\n      <td>like</td>\n      <td>building</td>\n      <td>total</td>\n      <td>tenant</td>\n      <td>intensity</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>tonne</td>\n      <td>recovery</td>\n      <td>water</td>\n      <td>building</td>\n      <td>street</td>\n      <td>incineration</td>\n      <td>angel</td>\n      <td>like</td>\n      <td>exclude</td>\n      <td>total</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>core</td>\n      <td>indicator</td>\n      <td>performance</td>\n      <td>adtl</td>\n      <td>initiative</td>\n      <td>monetary</td>\n      <td>fully</td>\n      <td>overview</td>\n      <td>indirect</td>\n      <td>environmental</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>waste</td>\n      <td>tonne</td>\n      <td>hazardous</td>\n      <td>fuel</td>\n      <td>incinerate</td>\n      <td>landfille</td>\n      <td>shopfit</td>\n      <td>send</td>\n      <td>compost</td>\n      <td>quantity</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>recycling</td>\n      <td>waste</td>\n      <td>landfill</td>\n      <td>recycle</td>\n      <td>increase</td>\n      <td>diversion</td>\n      <td>rate</td>\n      <td>plastic</td>\n      <td>resource</td>\n      <td>reuse</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>water</td>\n      <td>landlord</td>\n      <td>obtain</td>\n      <td>rainwater</td>\n      <td>consumption</td>\n      <td>tenant</td>\n      <td>meter</td>\n      <td>demand</td>\n      <td>visitor</td>\n      <td>harvest</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>charter</td>\n      <td>hall</td>\n      <td>june</td>\n      <td>july</td>\n      <td>datagri</td>\n      <td>ungcassurance</td>\n      <td>messageour</td>\n      <td>indexten</td>\n      <td>approachfy</td>\n      <td>usceo</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>hall</td>\n      <td>charter</td>\n      <td>cpof</td>\n      <td>street</td>\n      <td>fund</td>\n      <td>office</td>\n      <td>chot</td>\n      <td>june</td>\n      <td>star</td>\n      <td>average</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>factor</td>\n      <td>travel</td>\n      <td>emission</td>\n      <td>defra</td>\n      <td>journey</td>\n      <td>use</td>\n      <td>refrigerant</td>\n      <td>visitor</td>\n      <td>average</td>\n      <td>calculate</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>elec</td>\n      <td>fuels</td>\n      <td>natural</td>\n      <td>electricity</td>\n      <td>landlord</td>\n      <td>consumption</td>\n      <td>obtain</td>\n      <td>diesel</td>\n      <td>renewable</td>\n      <td>meter</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>mtco</td>\n      <td>indir</td>\n      <td>scope</td>\n      <td>base</td>\n      <td>location</td>\n      <td>market</td>\n      <td>kgco</td>\n      <td>part</td>\n      <td>common</td>\n      <td>total</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>asset</td>\n      <td>reporting</td>\n      <td>hold</td>\n      <td>control</td>\n      <td>include</td>\n      <td>period</td>\n      <td>datum</td>\n      <td>directly</td>\n      <td>data</td>\n      <td>year</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>datum</td>\n      <td>data</td>\n      <td>utility</td>\n      <td>monthly</td>\n      <td>reading</td>\n      <td>manual</td>\n      <td>invoice</td>\n      <td>automate</td>\n      <td>manager</td>\n      <td>meter</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>reporting</td>\n      <td>epra</td>\n      <td>supplement</td>\n      <td>recommendation</td>\n      <td>practice</td>\n      <td>sector</td>\n      <td>cress</td>\n      <td>lasalle</td>\n      <td>lang</td>\n      <td>datum</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>page</td>\n      <td>materiality</td>\n      <td>footprint</td>\n      <td>measure</td>\n      <td>carbon</td>\n      <td>performance</td>\n      <td>section</td>\n      <td>summary</td>\n      <td>account</td>\n      <td>epra</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>contribution</td>\n      <td>kind</td>\n      <td>leverage</td>\n      <td>charity</td>\n      <td>collection</td>\n      <td>record</td>\n      <td>staff</td>\n      <td>money</td>\n      <td>value</td>\n      <td>time</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>achieve</td>\n      <td>project</td>\n      <td>minimum</td>\n      <td>rating</td>\n      <td>leed</td>\n      <td>breeam</td>\n      <td>design</td>\n      <td>major</td>\n      <td>refurbishment</td>\n      <td>build</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>carbon</td>\n      <td>reduction</td>\n      <td>reduce</td>\n      <td>target</td>\n      <td>emission</td>\n      <td>climate</td>\n      <td>energy</td>\n      <td>change</td>\n      <td>achieve</td>\n      <td>demand</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>training</td>\n      <td>staff</td>\n      <td>france</td>\n      <td>programme</td>\n      <td>awareness</td>\n      <td>sustainability</td>\n      <td>survey</td>\n      <td>knowledge</td>\n      <td>role</td>\n      <td>develop</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>france</td>\n      <td>lease</td>\n      <td>green</td>\n      <td>biodiversity</td>\n      <td>hammerson</td>\n      <td>development</td>\n      <td>tenant</td>\n      <td>complete</td>\n      <td>guide</td>\n      <td>progress</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>really</td>\n      <td>enjoy</td>\n      <td>role</td>\n      <td>feel</td>\n      <td>work</td>\n      <td>derwent</td>\n      <td>experience</td>\n      <td>interview</td>\n      <td>career</td>\n      <td>different</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>employee</td>\n      <td>training</td>\n      <td>session</td>\n      <td>survey</td>\n      <td>opportunity</td>\n      <td>leadership</td>\n      <td>skill</td>\n      <td>talent</td>\n      <td>work</td>\n      <td>develop</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>local</td>\n      <td>community</td>\n      <td>people</td>\n      <td>skill</td>\n      <td>employment</td>\n      <td>support</td>\n      <td>young</td>\n      <td>school</td>\n      <td>engage</td>\n      <td>project</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>strategy</td>\n      <td>business</td>\n      <td>investor</td>\n      <td>sustainability</td>\n      <td>stakeholder</td>\n      <td>positive</td>\n      <td>long</td>\n      <td>value</td>\n      <td>hammerson</td>\n      <td>term</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>risk</td>\n      <td>committee</td>\n      <td>board</td>\n      <td>executive</td>\n      <td>climate</td>\n      <td>responsible</td>\n      <td>sustainability</td>\n      <td>business</td>\n      <td>group</td>\n      <td>corporate</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>board</td>\n      <td>woman</td>\n      <td>diversity</td>\n      <td>policy</td>\n      <td>workplace</td>\n      <td>conduct</td>\n      <td>female</td>\n      <td>code</td>\n      <td>prairiesky</td>\n      <td>director</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>safety</td>\n      <td>health</td>\n      <td>ensure</td>\n      <td>risk</td>\n      <td>construction</td>\n      <td>management</td>\n      <td>incident</td>\n      <td>approach</td>\n      <td>safe</td>\n      <td>quooda</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "topics_bert = list(topics_k.values())\n",
    "pd.DataFrame.from_dict(topics_k).T"
   ]
  },
  {
   "source": [
    "### 3.2.1 Normalized Point-wise Mutual Information"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "2020-11-18 19:23:49.220 INFO    gensim.corpora.dictionary: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-18 19:23:49.596 INFO    gensim.corpora.dictionary: built Dictionary(8372 unique tokens: ['armada', 'central', 'citizenship', 'come', 'corporate']...) from 6283 documents (total 331204 corpus positions)\n",
      "2020-11-18 19:23:49.601 INFO    gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator(processes=3, batch_size=64) to estimate probabilities from sliding windows\n",
      "2020-11-18 19:23:52.711 INFO    gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (2974 virtual)\n",
      "2020-11-18 19:23:52.715 INFO    gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (6307 virtual)\n",
      "2020-11-18 19:23:52.719 INFO    gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (9323 virtual)\n",
      "2020-11-18 19:23:52.723 INFO    gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (12021 virtual)\n",
      "2020-11-18 19:23:52.727 INFO    gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (15139 virtual)\n",
      "2020-11-18 19:23:52.731 INFO    gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (18237 virtual)\n",
      "2020-11-18 19:23:52.830 INFO    gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (20909 virtual)\n",
      "2020-11-18 19:23:52.864 INFO    gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (23487 virtual)\n",
      "2020-11-18 19:23:52.896 INFO    gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (26446 virtual)\n",
      "2020-11-18 19:23:52.966 INFO    gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (29497 virtual)\n",
      "2020-11-18 19:23:52.985 INFO    gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (31928 virtual)\n",
      "2020-11-18 19:23:53.037 INFO    gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (34560 virtual)\n",
      "2020-11-18 19:23:53.112 INFO    gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (37556 virtual)\n",
      "2020-11-18 19:23:53.120 INFO    gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (39928 virtual)\n",
      "2020-11-18 19:23:53.229 INFO    gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (42576 virtual)\n",
      "2020-11-18 19:23:53.245 INFO    gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (45386 virtual)\n",
      "2020-11-18 19:23:53.261 INFO    gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (48285 virtual)\n",
      "2020-11-18 19:23:53.357 INFO    gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (50686 virtual)\n",
      "2020-11-18 19:23:53.371 INFO    gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (53082 virtual)\n",
      "2020-11-18 19:23:53.376 INFO    gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (56090 virtual)\n",
      "2020-11-18 19:23:53.458 INFO    gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (58828 virtual)\n",
      "2020-11-18 19:23:53.485 INFO    gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (61462 virtual)\n",
      "2020-11-18 19:23:53.503 INFO    gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (64468 virtual)\n",
      "2020-11-18 19:23:53.560 INFO    gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (67219 virtual)\n",
      "2020-11-18 19:23:53.592 INFO    gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (70245 virtual)\n",
      "2020-11-18 19:23:53.662 INFO    gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (73231 virtual)\n",
      "2020-11-18 19:23:53.732 INFO    gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (76730 virtual)\n",
      "2020-11-18 19:23:53.753 INFO    gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (79159 virtual)\n",
      "2020-11-18 19:23:53.848 INFO    gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (82138 virtual)\n",
      "2020-11-18 19:23:53.896 INFO    gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (85416 virtual)\n",
      "2020-11-18 19:23:53.943 INFO    gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (88273 virtual)\n",
      "2020-11-18 19:23:53.997 INFO    gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (91700 virtual)\n",
      "2020-11-18 19:23:54.071 INFO    gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (94809 virtual)\n",
      "2020-11-18 19:23:54.077 INFO    gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (97753 virtual)\n",
      "2020-11-18 19:23:54.153 INFO    gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (100827 virtual)\n",
      "2020-11-18 19:23:54.217 INFO    gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (103535 virtual)\n",
      "2020-11-18 19:23:54.237 INFO    gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (105727 virtual)\n",
      "2020-11-18 19:23:54.341 INFO    gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (108190 virtual)\n",
      "2020-11-18 19:23:54.385 INFO    gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (110724 virtual)\n",
      "2020-11-18 19:23:54.405 INFO    gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (113233 virtual)\n",
      "2020-11-18 19:23:54.476 INFO    gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (115267 virtual)\n",
      "2020-11-18 19:23:54.518 INFO    gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (117732 virtual)\n",
      "2020-11-18 19:23:54.534 INFO    gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (120636 virtual)\n",
      "2020-11-18 19:23:54.598 INFO    gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (123490 virtual)\n",
      "2020-11-18 19:23:54.665 INFO    gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (126421 virtual)\n",
      "2020-11-18 19:23:54.677 INFO    gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (129277 virtual)\n",
      "2020-11-18 19:23:54.722 INFO    gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (131875 virtual)\n",
      "2020-11-18 19:23:54.828 INFO    gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (134128 virtual)\n",
      "2020-11-18 19:23:54.838 INFO    gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (136597 virtual)\n",
      "2020-11-18 19:23:54.909 INFO    gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (138674 virtual)\n",
      "2020-11-18 19:23:55.001 INFO    gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (140644 virtual)\n",
      "2020-11-18 19:23:55.012 INFO    gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (142619 virtual)\n",
      "2020-11-18 19:23:55.074 INFO    gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (145239 virtual)\n",
      "2020-11-18 19:23:55.134 INFO    gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (148283 virtual)\n",
      "2020-11-18 19:23:55.160 INFO    gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (151667 virtual)\n",
      "2020-11-18 19:23:55.189 INFO    gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (155445 virtual)\n",
      "2020-11-18 19:23:55.256 INFO    gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (159544 virtual)\n",
      "2020-11-18 19:23:55.281 INFO    gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (162754 virtual)\n",
      "2020-11-18 19:23:55.350 INFO    gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (165592 virtual)\n",
      "2020-11-18 19:23:55.430 INFO    gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (168535 virtual)\n",
      "2020-11-18 19:23:55.463 INFO    gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (171157 virtual)\n",
      "2020-11-18 19:23:55.551 INFO    gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (174067 virtual)\n",
      "2020-11-18 19:23:55.602 INFO    gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (176827 virtual)\n",
      "2020-11-18 19:23:55.629 INFO    gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (179908 virtual)\n",
      "2020-11-18 19:23:55.693 INFO    gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (182296 virtual)\n",
      "2020-11-18 19:23:55.759 INFO    gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (185170 virtual)\n",
      "2020-11-18 19:23:55.764 INFO    gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (187559 virtual)\n",
      "2020-11-18 19:23:55.815 INFO    gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (190333 virtual)\n",
      "2020-11-18 19:23:55.885 INFO    gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (192944 virtual)\n",
      "2020-11-18 19:23:55.919 INFO    gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (195558 virtual)\n",
      "2020-11-18 19:23:55.950 INFO    gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (198486 virtual)\n",
      "2020-11-18 19:23:56.034 INFO    gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (200855 virtual)\n",
      "2020-11-18 19:23:56.047 INFO    gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (203434 virtual)\n",
      "2020-11-18 19:23:56.122 INFO    gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (206525 virtual)\n",
      "2020-11-18 19:23:56.246 INFO    gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (209466 virtual)\n",
      "2020-11-18 19:23:56.254 INFO    gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (212495 virtual)\n",
      "2020-11-18 19:23:56.300 INFO    gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (215807 virtual)\n",
      "2020-11-18 19:23:56.372 INFO    gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (218529 virtual)\n",
      "2020-11-18 19:23:56.385 INFO    gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (221270 virtual)\n",
      "2020-11-18 19:23:56.457 INFO    gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (224735 virtual)\n",
      "2020-11-18 19:23:56.543 INFO    gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (227574 virtual)\n",
      "2020-11-18 19:23:56.568 INFO    gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (230593 virtual)\n",
      "2020-11-18 19:23:56.634 INFO    gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (233983 virtual)\n",
      "2020-11-18 19:23:56.692 INFO    gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (237129 virtual)\n",
      "2020-11-18 19:23:56.716 INFO    gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (240041 virtual)\n",
      "2020-11-18 19:23:56.811 INFO    gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (242418 virtual)\n",
      "2020-11-18 19:23:56.823 INFO    gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (244547 virtual)\n",
      "2020-11-18 19:23:56.853 INFO    gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (246810 virtual)\n",
      "2020-11-18 19:23:57.005 INFO    gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (249335 virtual)\n",
      "2020-11-18 19:23:57.010 INFO    gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (252160 virtual)\n",
      "2020-11-18 19:23:57.018 INFO    gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (255245 virtual)\n",
      "2020-11-18 19:23:57.132 INFO    gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (257825 virtual)\n",
      "2020-11-18 19:23:57.155 INFO    gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (260819 virtual)\n",
      "2020-11-18 19:23:57.159 INFO    gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (263525 virtual)\n",
      "2020-11-18 19:23:57.267 INFO    gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (266158 virtual)\n",
      "2020-11-18 19:23:57.280 INFO    gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (268551 virtual)\n",
      "2020-11-18 19:23:57.307 INFO    gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (271426 virtual)\n",
      "2020-11-18 19:23:57.450 INFO    gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (274407 virtual)\n",
      "2020-11-18 19:23:57.470 INFO    gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (274618 virtual)\n",
      "2020-11-18 19:23:57.824 INFO    gensim.topic_coherence.text_analysis: 3 accumulators retrieved from output queue\n",
      "2020-11-18 19:23:57.878 INFO    gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 274853 virtual documents\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.1746779890206453"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "npmi = CoherenceNPMI(texts=text_cleaned, topics=topics_bert)\n",
    "npmi.score()"
   ]
  },
  {
   "source": [
    "### 3.2.2 External Word Embeddings Topic Coherence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "2020-11-18 19:23:58.692 INFO    gensim.models.utils_any2vec: loading projection weights from C:\\Users\\Pieter-Jan/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n",
      "2020-11-18 19:24:48.158 INFO    gensim.models.utils_any2vec: loaded (3000000, 300) matrix from C:\\Users\\Pieter-Jan/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.1362753"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "CoherenceWordEmbeddings(topics_bert).score()"
   ]
  },
  {
   "source": [
    "### 3.2.3 Rank-Biased Overlap "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Pieter-Jan\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9891851583628571"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "InvertedRBO(topics_bert).score()"
   ]
  }
 ]
}